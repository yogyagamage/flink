{
  "fullMethodsPaths": [
    {
      "entryPoint": "org.apache.flink.api.common.io.compression.ZStandardInputStreamFactory.create(java.io.InputStream)",
      "thirdPartyMethod": "org.apache.commons.compress.compressors.zstandard.ZstdCompressorInputStream.<init>(java.io.InputStream)",
      "directCaller": "org.apache.flink.api.common.io.compression.ZStandardInputStreamFactory.create(java.io.InputStream)",
      "path": [
        "org.apache.flink.api.common.io.compression.ZStandardInputStreamFactory.create(java.io.InputStream)",
        "org.apache.commons.compress.compressors.zstandard.ZstdCompressorInputStream.<init>(java.io.InputStream)"
      ],
      "methodSources": [
        "@Override\npublic ZstdCompressorInputStream create(InputStream in) throws IOException {\n    return new ZstdCompressorInputStream(in);\n}"
      ],
      "constructors": [
        "ZStandardInputStreamFactory() {\n}"
      ],
      "fieldDeclarations": [
        "private static final ZStandardInputStreamFactory INSTANCE = new ZStandardInputStreamFactory();"
      ],
      "setters": [],
      "imports": [
        "org.apache.commons.compress.compressors.zstandard.ZstdCompressorInputStream"
      ],
      "testTemplate": "package org.apache.flink.api.common.io.compression;\n\npublic class ZStandardInputStreamFactorycreate_ZstdCompressorInputStreammethodFikaTest {\n\n    @Test\n    public void testCreate() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.KryoUtils.copy(java.lang.Object, java.lang.Object, com.esotericsoftware.kryo.Kryo, org.apache.flink.api.common.typeutils.TypeSerializer)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.copy(java.lang.Object)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.KryoUtils.copy(java.lang.Object, java.lang.Object, com.esotericsoftware.kryo.Kryo, org.apache.flink.api.common.typeutils.TypeSerializer)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils.copy(java.lang.Object, java.lang.Object, com.esotericsoftware.kryo.Kryo, org.apache.flink.api.common.typeutils.TypeSerializer)",
        "com.esotericsoftware.kryo.Kryo.copy(java.lang.Object)"
      ],
      "methodSources": [
        "/**\n * Tries to copy the given record from using the provided Kryo instance. If this fails, then the\n * record from is copied by serializing it into a byte buffer and deserializing it from there.\n *\n * @param from\n * \t\tElement to copy\n * @param reuse\n * \t\tReuse element for the deserialization\n * @param kryo\n * \t\tKryo instance to use\n * @param serializer\n * \t\tTypeSerializer which is used in case of a Kryo failure\n * @param <T>\n * \t\tType of the element to be copied\n * @return Copied element\n */\npublic static <T> T copy(T from, T reuse, Kryo kryo, TypeSerializer<T> serializer) {\n    try {\n        // PATH: Test should invoke the next Kryo.copy(...) [step in execution path]\n        return kryo.copy(from);\n    } catch (KryoException ke) {\n        // Kryo could not copy the object --> try to serialize/deserialize the object\n        try {\n            byte[] byteArray = InstantiationUtil.serializeToByteArray(serializer, from);\n            return InstantiationUtil.deserializeFromByteArray(serializer, reuse, byteArray);\n        } catch (IOException ioe) {\n            throw new RuntimeException(\"Could not copy object by serializing/deserializing\" + \" it.\", ioe);\n        }\n    }\n}"
      ],
      "constructors": [
        "KryoUtils() {\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.util.InstantiationUtil"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class KryoUtilscopy_KryocopyFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.skip(int)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.KryoException.<init>(java.lang.Throwable)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.skip(int)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.skip(int)",
        "com.esotericsoftware.kryo.KryoException.<init>(java.lang.Throwable)"
      ],
      "methodSources": [
        "@Override\npublic void skip(int count) throws KryoException {\n    try {\n        inputStream.skip(count);\n    } catch (IOException ex) {\n        throw new KryoException(ex);\n    }\n}"
      ],
      "constructors": [
        "public NoFetchingInput(InputStream inputStream) {\n    super(inputStream, 8);\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.io.Input"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class NoFetchingInputskip_KryoExceptionmethodFikaTest {\n\n    @Test\n    public void testSkip() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter.<init>(org.snakeyaml.engine.v2.api.DumpSettings)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.representer.StandardRepresenter.<init>(org.snakeyaml.engine.v2.api.DumpSettings)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter.<init>(org.snakeyaml.engine.v2.api.DumpSettings)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter.<init>(org.snakeyaml.engine.v2.api.DumpSettings)",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter.<init>(org.snakeyaml.engine.v2.api.DumpSettings)"
      ],
      "methodSources": [
        "public (DumpSettings dumpSettings) // PATH: Test should invoke the next new StandardRepresenter(...) [step in execution path]\n{\n    this(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "constructors": [
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class FlinkConfigRepresentermethod_StandardRepresentermethodFikaTest {\n\n    @Test\n    public void testMethod() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.Keys.ExpressionKeys.toString()",
      "thirdPartyMethod": "org.apache.commons.lang3.StringUtils.join(java.lang.Iterable, char)",
      "directCaller": "org.apache.flink.api.common.operators.Keys.ExpressionKeys.toString()",
      "path": [
        "org.apache.flink.api.common.operators.Keys.ExpressionKeys.toString()",
        "org.apache.commons.lang3.StringUtils.join(java.lang.Iterable, char)"
      ],
      "methodSources": [
        "@Override\npublic String toString() {\n    // PATH: Test should invoke the next StringUtils.join(...) [step in execution path]\n    return \"ExpressionKeys: \" + StringUtils.join(keyFields, '.');\n}"
      ],
      "constructors": [
        "/**\n * ExpressionKeys that is defined by the full data type.\n */\npublic ExpressionKeys(TypeInformation<T> type) {\n    this(SELECT_ALL_CHAR, type);\n}",
        "/**\n * Create int-based (non-nested) field position keys on a tuple type.\n */\npublic ExpressionKeys(int keyPosition, TypeInformation<T> type) {\n    this(new int[]{ keyPosition }, type, false);\n}",
        "/**\n * Create int-based (non-nested) field position keys on a tuple type.\n */\npublic ExpressionKeys(int[] keyPositions, TypeInformation<T> type) {\n    this(keyPositions, type, false);\n}",
        "/**\n * Create int-based (non-nested) field position keys on a tuple type.\n */\npublic ExpressionKeys(int[] keyPositions, TypeInformation<T> type, boolean allowEmpty) {\n    if ((!type.isTupleType()) || (!(type instanceof CompositeType))) {\n        throw new InvalidProgramException((\"Specifying keys via field positions is only valid \" + \"for tuple data types. Type: \") + type);\n    }\n    if (type.getArity() == 0) {\n        throw new InvalidProgramException(\"Tuple size must be greater than 0. Size: \" + type.getArity());\n    }\n    if ((!allowEmpty) && ((keyPositions == null) || (keyPositions.length == 0))) {\n        throw new IllegalArgumentException(\"The grouping fields must not be empty.\");\n    }\n    this.keyFields = new ArrayList<>();\n    if ((keyPositions == null) || (keyPositions.length == 0)) {\n        // use all tuple fields as key fields\n        keyPositions = createIncrIntArray(type.getArity());\n    } else {\n        rangeCheckFields(keyPositions, type.getArity() - 1);\n    }\n    checkArgument(keyPositions.length > 0, \"Grouping fields can not be empty at this point\");\n    // extract key field types\n    CompositeType<T> cType = ((CompositeType<T>) (type));\n    this.keyFields = new ArrayList<>(type.getTotalFields());\n    // for each key position, find all (nested) field types\n    String[] fieldNames = cType.getFieldNames();\n    this.originalKeyTypes = new TypeInformation<?>[keyPositions.length];\n    ArrayList<FlatFieldDescriptor> tmpList = new ArrayList<>();\n    for (int i = 0; i < keyPositions.length; i++) {\n        int keyPos = keyPositions[i];\n        tmpList.clear();\n        // get all flat fields\n        this.originalKeyTypes[i] = cType.getTypeAt(keyPos);\n        cType.getFlatFields(fieldNames[keyPos], 0, tmpList);\n        // check if fields are of key type\n        for (FlatFieldDescriptor ffd : tmpList) {\n            if (!ffd.getType().isKeyType()) {\n                throw new InvalidProgramException((\"This type (\" + ffd.getType()) + \") cannot be used as key.\");\n            }\n        }\n        this.keyFields.addAll(tmpList);\n    }\n}",
        "/**\n * Create String-based (nested) field expression keys on a composite type.\n */\npublic ExpressionKeys(String keyExpression, TypeInformation<T> type) {\n    this(new String[]{ keyExpression }, type);\n}",
        "/**\n * Create String-based (nested) field expression keys on a composite type.\n */\npublic ExpressionKeys(String[] keyExpressions, TypeInformation<T> type) {\n    checkNotNull(keyExpressions, \"Field expression cannot be null.\");\n    this.keyFields = new ArrayList<>(keyExpressions.length);\n    if (type instanceof CompositeType) {\n        CompositeType<T> cType = ((CompositeType<T>) (type));\n        this.originalKeyTypes = new TypeInformation<?>[keyExpressions.length];\n        // extract the keys on their flat position\n        for (int i = 0; i < keyExpressions.length; i++) {\n            String keyExpr = keyExpressions[i];\n            if (keyExpr == null) {\n                throw new InvalidProgramException(\"Expression key may not be null.\");\n            }\n            // strip off whitespace\n            keyExpr = keyExpr.trim();\n            List<FlatFieldDescriptor> flatFields = cType.getFlatFields(keyExpr);\n            if (flatFields.size() == 0) {\n                throw new InvalidProgramException(((\"Unable to extract key from expression '\" + keyExpr) + \"' on key \") + cType);\n            }\n            // check if all nested fields can be used as keys\n            for (FlatFieldDescriptor field : flatFields) {\n                if (!field.getType().isKeyType()) {\n                    throw new InvalidProgramException((\"This type (\" + field.getType()) + \") cannot be used as key.\");\n                }\n            }\n            // add flat fields to key fields\n            keyFields.addAll(flatFields);\n            String strippedKeyExpr = WILD_CARD_REGEX.matcher(keyExpr).replaceAll(\"\");\n            if (strippedKeyExpr.isEmpty()) {\n                this.originalKeyTypes[i] = type;\n            } else {\n                this.originalKeyTypes[i] = cType.getTypeAt(strippedKeyExpr);\n            }\n        }\n    } else {\n        if (!type.isKeyType()) {\n            throw new InvalidProgramException((\"This type (\" + type) + \") cannot be used as key.\");\n        }\n        // check that all key expressions are valid\n        for (String keyExpr : keyExpressions) {\n            if (keyExpr == null) {\n                throw new InvalidProgramException(\"Expression key may not be null.\");\n            }\n            // strip off whitespace\n            keyExpr = keyExpr.trim();\n            // check that full type is addressed\n            if (!(SELECT_ALL_CHAR.equals(keyExpr) || SELECT_ALL_CHAR_SCALA.equals(keyExpr))) {\n                throw new InvalidProgramException((((\"Field expression must be equal to '\" + SELECT_ALL_CHAR) + \"' or '\") + SELECT_ALL_CHAR_SCALA) + \"' for non-composite types.\");\n            }\n            // add full type as key\n            keyFields.add(new FlatFieldDescriptor(0, type));\n        }\n        this.originalKeyTypes = new TypeInformation[]{ type };\n    }\n}"
      ],
      "fieldDeclarations": [
        "public static final String SELECT_ALL_CHAR = \"*\";",
        "public static final String SELECT_ALL_CHAR_SCALA = \"_\";",
        "private static final Pattern WILD_CARD_REGEX = Pattern.compile((((((\"[\\\\.]?(\" + \"\\\\\") + SELECT_ALL_CHAR) + \"|\") + \"\\\\\") + SELECT_ALL_CHAR_SCALA) + \")$\");",
        "// Flattened fields representing keys fields\nprivate List<FlatFieldDescriptor> keyFields;",
        "private TypeInformation<?>[] originalKeyTypes;"
      ],
      "setters": [
        "@Override\npublic <E> void validateCustomPartitioner(Partitioner<E> partitioner, TypeInformation<E> typeInfo) {\n    if (keyFields.size() != 1) {\n        throw new InvalidProgramException(\"Custom partitioners can only be used with keys that have one key field.\");\n    }\n    if (typeInfo == null) {\n        // try to extract key type from partitioner\n        try {\n            typeInfo = TypeExtractor.getPartitionerTypes(partitioner);\n        } catch (Throwable t) {\n            // best effort check, so we ignore exceptions\n        }\n    }\n    if ((typeInfo != null) && (!(typeInfo instanceof GenericTypeInfo))) {\n        // only check type compatibility if type is known and not a generic type\n        TypeInformation<?> keyType = keyFields.get(0).getType();\n        if (!keyType.equals(typeInfo)) {\n            throw new InvalidProgramException((((\"The partitioner is incompatible with the key type. \" + \"Partitioner type: \") + typeInfo) + \" , key type: \") + keyType);\n        }\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.lang3.StringUtils",
        "org.apache.flink.api.common.InvalidProgramException",
        "org.apache.flink.api.common.functions.Partitioner",
        "org.apache.flink.api.common.operators.Keys.ExpressionKeys",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.CompositeType.FlatFieldDescriptor",
        "org.apache.flink.api.java.typeutils.GenericTypeInfo",
        "org.apache.flink.api.java.typeutils.TypeExtractor",
        "org.apache.flink.util.Preconditions"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators;\n\npublic class ExpressionKeystoString_StringUtilsjoinFikaTest {\n\n    @Test\n    public void testToString() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.loadFromString(java.lang.String)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
        "org.snakeyaml.engine.v2.api.Load.loadFromString(java.lang.String)"
      ],
      "methodSources": [
        "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return // PATH: Test should invoke the next Load.loadFromString(...) [step in execution path]\n        type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsconvertToObject_LoadloadFromStringFikaTest {\n\n    @Test\n    public void testConvertToObject() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.util.JoinHashMap.<init>(org.apache.flink.api.common.typeutils.TypeSerializer, org.apache.flink.api.common.typeutils.TypeComparator)",
      "thirdPartyMethod": "org.apache.commons.collections.map.AbstractHashedMap.<init>(int)",
      "directCaller": "org.apache.flink.api.common.operators.util.JoinHashMap.<init>(org.apache.flink.api.common.typeutils.TypeSerializer, org.apache.flink.api.common.typeutils.TypeComparator)",
      "path": [
        "org.apache.flink.api.common.operators.util.JoinHashMap.<init>(org.apache.flink.api.common.typeutils.TypeSerializer, org.apache.flink.api.common.typeutils.TypeComparator)",
        "org.apache.commons.collections.map.AbstractHashedMap.<init>(int)"
      ],
      "methodSources": [
        "public (TypeSerializer<BT> buildSerializer, TypeComparator<BT> buildComparator) // PATH: Test should invoke the next new AbstractHashedMap(...) [step in execution path]\n{\n    this(64);\n    this.buildSerializer = buildSerializer;\n    this.buildComparator = buildComparator;\n}"
      ],
      "constructors": [
        "public JoinHashMap(TypeSerializer<BT> buildSerializer, TypeComparator<BT> buildComparator) {\n    super(64);\n    this.buildSerializer = buildSerializer;\n    this.buildComparator = buildComparator;\n}",
        "public Prober(TypeComparator<PT> probeComparator, TypePairComparator<PT, BT> pairComparator) {\n    this.probeComparator = probeComparator;\n    this.pairComparator = pairComparator;\n}"
      ],
      "fieldDeclarations": [
        "private final TypeSerializer<BT> buildSerializer;",
        "private final TypeComparator<BT> buildComparator;"
      ],
      "setters": [
        "@SuppressWarnings(\"unchecked\")\npublic void insertOrReplace(BT record) {\n    int hashCode = hash(buildComparator.hash(record));\n    int index = hashIndex(hashCode, data.length);\n    buildComparator.setReference(record);\n    HashEntry entry = data[index];\n    while (entry != null) {\n        if ((entryHashCode(entry) == hashCode) && buildComparator.equalToReference(((BT) (entry.getValue())))) {\n            entry.setValue(record);\n            return;\n        }\n        entry = entryNext(entry);\n    } \n    addMapping(index, hashCode, null, record);\n}"
      ],
      "imports": [
        "org.apache.commons.collections.map.AbstractHashedMap",
        "org.apache.commons.collections.map.AbstractHashedMap.HashEntry",
        "org.apache.flink.api.common.operators.util.JoinHashMap.Prober",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypePairComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators.util;\n\npublic class JoinHashMapmethod_AbstractHashedMapmethodFikaTest {\n\n    @Test\n    public void testMethod() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.common.io.compression.XZInputStreamFactory.create(java.io.InputStream)",
      "thirdPartyMethod": "org.apache.commons.compress.compressors.xz.XZCompressorInputStream.<init>(java.io.InputStream, boolean)",
      "directCaller": "org.apache.flink.api.common.io.compression.XZInputStreamFactory.create(java.io.InputStream)",
      "path": [
        "org.apache.flink.api.common.io.compression.XZInputStreamFactory.create(java.io.InputStream)",
        "org.apache.commons.compress.compressors.xz.XZCompressorInputStream.<init>(java.io.InputStream, boolean)"
      ],
      "methodSources": [
        "@Override\npublic XZCompressorInputStream create(InputStream in) throws IOException {\n    return new XZCompressorInputStream(in, true);\n}"
      ],
      "constructors": [
        "XZInputStreamFactory() {\n}"
      ],
      "fieldDeclarations": [
        "private static final XZInputStreamFactory INSTANCE = new XZInputStreamFactory();"
      ],
      "setters": [],
      "imports": [
        "org.apache.commons.compress.compressors.xz.XZCompressorInputStream"
      ],
      "testTemplate": "package org.apache.flink.api.common.io.compression;\n\npublic class XZInputStreamFactorycreate_XZCompressorInputStreammethodFikaTest {\n\n    @Test\n    public void testCreate() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.<init>(java.io.InputStream)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.io.Input.<init>(java.io.InputStream, int)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.<init>(java.io.InputStream)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.<init>(java.io.InputStream)",
        "com.esotericsoftware.kryo.io.Input.<init>(java.io.InputStream, int)"
      ],
      "methodSources": [
        "public (InputStream inputStream) // PATH: Test should invoke the next new Input(...) [step in execution path]\n{\n    this(inputStream, 8);\n}"
      ],
      "constructors": [
        "public NoFetchingInput(InputStream inputStream) {\n    super(inputStream, 8);\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.io.Input"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class NoFetchingInputmethod_InputmethodFikaTest {\n\n    @Test\n    public void testMethod() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.KryoUtils.copy(java.lang.Object, com.esotericsoftware.kryo.Kryo, org.apache.flink.api.common.typeutils.TypeSerializer)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.copy(java.lang.Object)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.KryoUtils.copy(java.lang.Object, com.esotericsoftware.kryo.Kryo, org.apache.flink.api.common.typeutils.TypeSerializer)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils.copy(java.lang.Object, com.esotericsoftware.kryo.Kryo, org.apache.flink.api.common.typeutils.TypeSerializer)",
        "com.esotericsoftware.kryo.Kryo.copy(java.lang.Object)"
      ],
      "methodSources": [
        "/**\n * Tries to copy the given record from using the provided Kryo instance. If this fails, then the\n * record from is copied by serializing it into a byte buffer and deserializing it from there.\n *\n * @param from\n * \t\tElement to copy\n * @param kryo\n * \t\tKryo instance to use\n * @param serializer\n * \t\tTypeSerializer which is used in case of a Kryo failure\n * @param <T>\n * \t\tType of the element to be copied\n * @return Copied element\n */\npublic static <T> T copy(T from, Kryo kryo, TypeSerializer<T> serializer) {\n    try {\n        // PATH: Test should invoke the next Kryo.copy(...) [step in execution path]\n        return kryo.copy(from);\n    } catch (KryoException ke) {\n        // Kryo could not copy the object --> try to serialize/deserialize the object\n        try {\n            byte[] byteArray = InstantiationUtil.serializeToByteArray(serializer, from);\n            return InstantiationUtil.deserializeFromByteArray(serializer, byteArray);\n        } catch (IOException ioe) {\n            throw new RuntimeException(\"Could not copy object by serializing/deserializing\" + \" it.\", ioe);\n        }\n    }\n}"
      ],
      "constructors": [
        "KryoUtils() {\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.util.InstantiationUtil"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class KryoUtilscopy_KryocopyFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.io.compression.Bzip2InputStreamFactory.create(java.io.InputStream)",
      "thirdPartyMethod": "org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream.<init>(java.io.InputStream)",
      "directCaller": "org.apache.flink.api.common.io.compression.Bzip2InputStreamFactory.create(java.io.InputStream)",
      "path": [
        "org.apache.flink.api.common.io.compression.Bzip2InputStreamFactory.create(java.io.InputStream)",
        "org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream.<init>(java.io.InputStream)"
      ],
      "methodSources": [
        "@Override\npublic BZip2CompressorInputStream create(InputStream in) throws IOException {\n    return new BZip2CompressorInputStream(in);\n}"
      ],
      "constructors": [
        "Bzip2InputStreamFactory() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Bzip2InputStreamFactory INSTANCE = new Bzip2InputStreamFactory();"
      ],
      "setters": [],
      "imports": [
        "org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream"
      ],
      "testTemplate": "package org.apache.flink.api.common.io.compression;\n\npublic class Bzip2InputStreamFactorycreate_BZip2CompressorInputStreammethodFikaTest {\n\n    @Test\n    public void testCreate() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.PojoTypeInfo.toString()",
      "thirdPartyMethod": "org.apache.commons.lang3.StringUtils.join(java.lang.Iterable, java.lang.String)",
      "directCaller": "org.apache.flink.api.java.typeutils.PojoTypeInfo.toString()",
      "path": [
        "org.apache.flink.api.java.typeutils.PojoTypeInfo.toString()",
        "org.apache.commons.lang3.StringUtils.join(java.lang.Iterable, java.lang.String)"
      ],
      "methodSources": [
        "@Override\npublic String toString() {\n    List<String> fieldStrings = new ArrayList<String>();\n    for (PojoField field : fields) {\n        fieldStrings.add((field.getField().getName() + \": \") + field.getTypeInformation().toString());\n    }\n    // PATH: Test should invoke the next StringUtils.join(...) [step in execution path]\n    return ((((\"PojoType<\" + getTypeClass().getName()) + \", fields = [\") + StringUtils.join(fieldStrings, \", \")) + \"]\") + \">\";\n}"
      ],
      "constructors": [
        "@PublicEvolving\npublic PojoTypeInfo(Class<T> typeClass, List<PojoField> fields) {\n    super(typeClass);\n    checkArgument(Modifier.isPublic(typeClass.getModifiers()), \"POJO %s is not public\", typeClass);\n    this.fields = fields.toArray(new PojoField[fields.size()]);\n    Arrays.sort(this.fields, new Comparator<PojoField>() {\n        @Override\n        public int compare(PojoField o1, PojoField o2) {\n            return o1.getField().getName().compareTo(o2.getField().getName());\n        }\n    });\n    int counterFields = 0;\n    for (PojoField field : fields) {\n        counterFields += field.getTypeInformation().getTotalFields();\n    }\n    totalFields = counterFields;\n}",
        "1() {\n}",
        "public PojoTypeComparatorBuilder() {\n    fieldComparators = new ArrayList<TypeComparator>();\n    keyFields = new ArrayList<Field>();\n}",
        "public NamedFlatFieldDescriptor(String name, int keyPosition, TypeInformation<?> type) {\n    super(keyPosition, type);\n    this.fieldName = name;\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private static final String REGEX_FIELD = \"[\\\\p{L}_\\\\$][\\\\p{L}\\\\p{Digit}_\\\\$]*\";",
        "private static final String REGEX_NESTED_FIELDS = (\"(\" + REGEX_FIELD) + \")(\\\\.(.+))?\";",
        "private static final String REGEX_NESTED_FIELDS_WILDCARD = (((REGEX_NESTED_FIELDS + \"|\\\\\") + ExpressionKeys.SELECT_ALL_CHAR) + \"|\\\\\") + ExpressionKeys.SELECT_ALL_CHAR_SCALA;",
        "private static final Pattern PATTERN_NESTED_FIELDS = Pattern.compile(REGEX_NESTED_FIELDS);",
        "private static final Pattern PATTERN_NESTED_FIELDS_WILDCARD = Pattern.compile(REGEX_NESTED_FIELDS_WILDCARD);",
        "private final PojoField[] fields;",
        "private final int totalFields;"
      ],
      "setters": [
        "@Override\n@PublicEvolving\npublic void getFlatFields(String fieldExpression, int offset, List<FlatFieldDescriptor> result) {\n    Matcher matcher = PATTERN_NESTED_FIELDS_WILDCARD.matcher(fieldExpression);\n    if (!matcher.matches()) {\n        throw new InvalidFieldReferenceException((\"Invalid POJO field reference \\\"\" + fieldExpression) + \"\\\".\");\n    }\n    String field = matcher.group(0);\n    if (field.equals(ExpressionKeys.SELECT_ALL_CHAR) || field.equals(ExpressionKeys.SELECT_ALL_CHAR_SCALA)) {\n        // handle select all\n        int keyPosition = 0;\n        for (PojoField pField : fields) {\n            if (pField.getTypeInformation() instanceof CompositeType) {\n                CompositeType<?> cType = ((CompositeType<?>) (pField.getTypeInformation()));\n                cType.getFlatFields(String.valueOf(ExpressionKeys.SELECT_ALL_CHAR), offset + keyPosition, result);\n                keyPosition += cType.getTotalFields() - 1;\n            } else {\n                result.add(new NamedFlatFieldDescriptor(pField.getField().getName(), offset + keyPosition, pField.getTypeInformation()));\n            }\n            keyPosition++;\n        }\n        return;\n    } else {\n        field = matcher.group(1);\n    }\n    // get field\n    int fieldPos = -1;\n    TypeInformation<?> fieldType = null;\n    for (int i = 0; i < fields.length; i++) {\n        if (fields[i].getField().getName().equals(field)) {\n            fieldPos = i;\n            fieldType = fields[i].getTypeInformation();\n            break;\n        }\n    }\n    if (fieldPos == (-1)) {\n        throw new InvalidFieldReferenceException((((\"Unable to find field \\\"\" + field) + \"\\\" in type \") + this) + \".\");\n    }\n    String tail = matcher.group(3);\n    if (tail == null) {\n        if (fieldType instanceof CompositeType) {\n            // forward offset\n            for (int i = 0; i < fieldPos; i++) {\n                offset += this.getTypeAt(i).getTotalFields();\n            }\n            // add all fields of composite type\n            ((CompositeType<?>) (fieldType)).getFlatFields(\"*\", offset, result);\n        } else {\n            // we found the field to add\n            // compute flat field position by adding skipped fields\n            int flatFieldPos = offset;\n            for (int i = 0; i < fieldPos; i++) {\n                flatFieldPos += this.getTypeAt(i).getTotalFields();\n            }\n            result.add(new FlatFieldDescriptor(flatFieldPos, fieldType));\n        }\n    } else if (fieldType instanceof CompositeType<?>) {\n        // forward offset\n        for (int i = 0; i < fieldPos; i++) {\n            offset += this.getTypeAt(i).getTotalFields();\n        }\n        ((CompositeType<?>) (fieldType)).getFlatFields(tail, offset, result);\n    } else {\n        throw new InvalidFieldReferenceException((((\"Nested field expression \\\"\" + tail) + \"\\\" not possible on atomic type \") + fieldType) + \".\");\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.lang3.StringUtils",
        "org.apache.flink.api.common.operators.Keys",
        "org.apache.flink.api.common.operators.Keys.ExpressionKeys",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.CompositeType.FlatFieldDescriptor",
        "org.apache.flink.api.common.typeutils.CompositeType.InvalidFieldReferenceException",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.java.typeutils.PojoTypeInfo.NamedFlatFieldDescriptor",
        "org.apache.flink.api.java.typeutils.PojoTypeInfo.PojoTypeComparatorBuilder",
        "org.apache.flink.util.Preconditions"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils;\n\npublic class PojoTypeInfotoString_StringUtilsjoinFikaTest {\n\n    @Test\n    public void testToString() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Output, java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.KryoException.<init>(java.lang.String, java.lang.Throwable)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Output, java.lang.Object)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Output, java.lang.Object)",
        "com.esotericsoftware.kryo.KryoException.<init>(java.lang.String, java.lang.Throwable)"
      ],
      "methodSources": [
        "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic void write(Kryo kryo, Output output, T o) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectOutputStream objectStream = ((ObjectOutputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            objectStream = new ObjectOutputStream(output);\n            graphContext.put(this, objectStream);\n        }\n        objectStream.writeObject(o);\n        objectStream.flush();\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java serialization.\", ex);\n    }\n}"
      ],
      "constructors": [
        "public JavaSerializer() {\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.ObjectMap"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class JavaSerializerwrite_KryoExceptionmethodFikaTest {\n\n    @Test\n    public void testWrite() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.util.ObjectMap.get(java.lang.Object)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
        "com.esotericsoftware.kryo.util.ObjectMap.get(java.lang.Object)"
      ],
      "methodSources": [
        "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic T read(Kryo kryo, Input input, Class aClass) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        // PATH: Test should invoke the next ObjectMap.get(...) [step in execution path]\n        ObjectInputStream objectStream = ((ObjectInputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            // make sure we use Kryo's classloader\n            objectStream = new InstantiationUtil.ClassLoaderObjectInputStream(input, kryo.getClassLoader());\n            graphContext.put(this, objectStream);\n        }\n        return ((T) (objectStream.readObject()));\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java deserialization.\", ex);\n    }\n}"
      ],
      "constructors": [
        "public JavaSerializer() {\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.util.ObjectMap",
        "org.apache.flink.util.InstantiationUtil",
        "org.apache.flink.util.InstantiationUtil.ClassLoaderObjectInputStream"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class JavaSerializerread_ObjectMapgetFikaTest {\n\n    @Test\n    public void testRead() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Output, java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.util.ObjectMap.put(java.lang.Object, java.lang.Object)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Output, java.lang.Object)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Output, java.lang.Object)",
        "com.esotericsoftware.kryo.util.ObjectMap.put(java.lang.Object, java.lang.Object)"
      ],
      "methodSources": [
        "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic void write(Kryo kryo, Output output, T o) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectOutputStream objectStream = ((ObjectOutputStream) (graphContext.get(this)));\n        if (objectStream == null) // PATH: Test should invoke the next ObjectMap.put(...) [step in execution path]\n        {\n            objectStream = new ObjectOutputStream(output);\n            graphContext.put(this, objectStream);\n        }\n        objectStream.writeObject(o);\n        objectStream.flush();\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java serialization.\", ex);\n    }\n}"
      ],
      "constructors": [
        "public JavaSerializer() {\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.ObjectMap"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class JavaSerializerwrite_ObjectMapputFikaTest {\n\n    @Test\n    public void testWrite() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Output, java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.getGraphContext()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Output, java.lang.Object)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Output, java.lang.Object)",
        "com.esotericsoftware.kryo.Kryo.getGraphContext()"
      ],
      "methodSources": [
        "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic void write(Kryo kryo, Output output, T o) {\n    try {\n        // PATH: Test should invoke the next Kryo.getGraphContext(...) [step in execution path]\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectOutputStream objectStream = ((ObjectOutputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            objectStream = new ObjectOutputStream(output);\n            graphContext.put(this, objectStream);\n        }\n        objectStream.writeObject(o);\n        objectStream.flush();\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java serialization.\", ex);\n    }\n}"
      ],
      "constructors": [
        "public JavaSerializer() {\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.ObjectMap"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class JavaSerializerwrite_KryogetGraphContextFikaTest {\n\n    @Test\n    public void testWrite() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.getClassLoader()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
        "com.esotericsoftware.kryo.Kryo.getClassLoader()"
      ],
      "methodSources": [
        "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic T read(Kryo kryo, Input input, Class aClass) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectInputStream objectStream = ((ObjectInputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            // make sure we use Kryo's classloader\n            objectStream = // PATH: Test should invoke the next Kryo.getClassLoader(...) [step in execution path]\n            new InstantiationUtil.ClassLoaderObjectInputStream(input, kryo.getClassLoader());\n            graphContext.put(this, objectStream);\n        }\n        return ((T) (objectStream.readObject()));\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java deserialization.\", ex);\n    }\n}"
      ],
      "constructors": [
        "public JavaSerializer() {\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.util.ObjectMap",
        "org.apache.flink.util.InstantiationUtil",
        "org.apache.flink.util.InstantiationUtil.ClassLoaderObjectInputStream"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class JavaSerializerread_KryogetClassLoaderFikaTest {\n\n    @Test\n    public void testRead() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.KryoException.<init>(java.lang.String, java.lang.Throwable)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
        "com.esotericsoftware.kryo.KryoException.<init>(java.lang.String, java.lang.Throwable)"
      ],
      "methodSources": [
        "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic T read(Kryo kryo, Input input, Class aClass) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectInputStream objectStream = ((ObjectInputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            // make sure we use Kryo's classloader\n            objectStream = new InstantiationUtil.ClassLoaderObjectInputStream(input, kryo.getClassLoader());\n            graphContext.put(this, objectStream);\n        }\n        return ((T) (objectStream.readObject()));\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java deserialization.\", ex);\n    }\n}"
      ],
      "constructors": [
        "public JavaSerializer() {\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.util.ObjectMap",
        "org.apache.flink.util.InstantiationUtil",
        "org.apache.flink.util.InstantiationUtil.ClassLoaderObjectInputStream"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class JavaSerializerread_KryoExceptionmethodFikaTest {\n\n    @Test\n    public void testRead() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.newInstance(java.lang.Class)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
        "com.esotericsoftware.kryo.Kryo.newInstance(java.lang.Class)"
      ],
      "methodSources": [
        "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            // PATH: Test should invoke the next Kryo.newInstance(...) [step in execution path]\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializercreateInstance_KryonewInstanceFikaTest {\n\n    @Test\n    public void testCreateInstance() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Output, java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.util.ObjectMap.get(java.lang.Object)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Output, java.lang.Object)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Output, java.lang.Object)",
        "com.esotericsoftware.kryo.util.ObjectMap.get(java.lang.Object)"
      ],
      "methodSources": [
        "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic void write(Kryo kryo, Output output, T o) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        // PATH: Test should invoke the next ObjectMap.get(...) [step in execution path]\n        ObjectOutputStream objectStream = ((ObjectOutputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            objectStream = new ObjectOutputStream(output);\n            graphContext.put(this, objectStream);\n        }\n        objectStream.writeObject(o);\n        objectStream.flush();\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java serialization.\", ex);\n    }\n}"
      ],
      "constructors": [
        "public JavaSerializer() {\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.ObjectMap"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class JavaSerializerwrite_ObjectMapgetFikaTest {\n\n    @Test\n    public void testWrite() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.read(byte[], int, int)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.KryoException.<init>(java.lang.Throwable)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.read(byte[], int, int)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.read(byte[], int, int)",
        "com.esotericsoftware.kryo.KryoException.<init>(java.lang.Throwable)"
      ],
      "methodSources": [
        "@Override\npublic int read(byte[] bytes, int offset, int count) throws KryoException {\n    if (bytes == null) {\n        throw new IllegalArgumentException(\"bytes cannot be null.\");\n    }\n    try {\n        return inputStream.read(bytes, offset, count);\n    } catch (IOException ex) {\n        throw new KryoException(ex);\n    }\n}"
      ],
      "constructors": [
        "public NoFetchingInput(InputStream inputStream) {\n    super(inputStream, 8);\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.io.Input"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class NoFetchingInputread_KryoExceptionmethodFikaTest {\n\n    @Test\n    public void testRead() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.util.ObjectMap.put(java.lang.Object, java.lang.Object)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
        "com.esotericsoftware.kryo.util.ObjectMap.put(java.lang.Object, java.lang.Object)"
      ],
      "methodSources": [
        "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic T read(Kryo kryo, Input input, Class aClass) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectInputStream objectStream = ((ObjectInputStream) (graphContext.get(this)));\n        if (objectStream == null) // PATH: Test should invoke the next ObjectMap.put(...) [step in execution path]\n        {\n            // make sure we use Kryo's classloader\n            objectStream = new InstantiationUtil.ClassLoaderObjectInputStream(input, kryo.getClassLoader());\n            graphContext.put(this, objectStream);\n        }\n        return ((T) (objectStream.readObject()));\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java deserialization.\", ex);\n    }\n}"
      ],
      "constructors": [
        "public JavaSerializer() {\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.util.ObjectMap",
        "org.apache.flink.util.InstantiationUtil",
        "org.apache.flink.util.InstantiationUtil.ClassLoaderObjectInputStream"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class JavaSerializerread_ObjectMapputFikaTest {\n\n    @Test\n    public void testRead() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.getGraphContext()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read(com.esotericsoftware.kryo.Kryo, com.esotericsoftware.kryo.io.Input, java.lang.Class)",
        "com.esotericsoftware.kryo.Kryo.getGraphContext()"
      ],
      "methodSources": [
        "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic T read(Kryo kryo, Input input, Class aClass) {\n    try {\n        // PATH: Test should invoke the next Kryo.getGraphContext(...) [step in execution path]\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectInputStream objectStream = ((ObjectInputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            // make sure we use Kryo's classloader\n            objectStream = new InstantiationUtil.ClassLoaderObjectInputStream(input, kryo.getClassLoader());\n            graphContext.put(this, objectStream);\n        }\n        return ((T) (objectStream.readObject()));\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java deserialization.\", ex);\n    }\n}"
      ],
      "constructors": [
        "public JavaSerializer() {\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.util.ObjectMap",
        "org.apache.flink.util.InstantiationUtil",
        "org.apache.flink.util.InstantiationUtil.ClassLoaderObjectInputStream"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class JavaSerializerread_KryogetGraphContextFikaTest {\n\n    @Test\n    public void testRead() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.loadFromInputStream(java.io.InputStream)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
        "org.snakeyaml.engine.v2.api.Load.loadFromInputStream(java.io.InputStream)"
      ],
      "methodSources": [
        "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        // PATH: Test should invoke the next Load.loadFromInputStream(...) [step in execution path]\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsloadYamlFile_LoadloadFromInputStreamFikaTest {\n\n    @Test\n    public void testLoadYamlFile() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettings.getBestLineBreak()",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
        "org.snakeyaml.engine.v2.api.DumpSettings.getBestLineBreak()"
      ],
      "methodSources": [
        "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        // PATH: Test should invoke the next DumpSettings.getBestLineBreak(...) [step in execution path]\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsconvertAndDumpYamlFromFlatMap_DumpSettingsgetBestLineBreakFikaTest {\n\n    @Test\n    public void testConvertAndDumpYamlFromFlatMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.util.JoinHashMap.insertOrReplace(java.lang.Object)",
      "thirdPartyMethod": "org.apache.commons.collections.map.AbstractHashedMap.HashEntry.setValue(java.lang.Object)",
      "directCaller": "org.apache.flink.api.common.operators.util.JoinHashMap.insertOrReplace(java.lang.Object)",
      "path": [
        "org.apache.flink.api.common.operators.util.JoinHashMap.insertOrReplace(java.lang.Object)",
        "org.apache.commons.collections.map.AbstractHashedMap.HashEntry.setValue(java.lang.Object)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\npublic void insertOrReplace(BT record) {\n    int hashCode = hash(buildComparator.hash(record));\n    int index = hashIndex(hashCode, data.length);\n    buildComparator.setReference(record);\n    HashEntry entry = data[index];\n    while (entry != null) {\n        if ((entryHashCode(entry) == hashCode) && buildComparator.equalToReference(((BT) (entry.getValue())))) // PATH: Test should invoke the next AbstractHashedMap$HashEntry.setValue(...) [step in execution path]\n        {\n            entry.setValue(record);\n            return;\n        }\n        entry = entryNext(entry);\n    } \n    addMapping(index, hashCode, null, record);\n}"
      ],
      "constructors": [
        "public JoinHashMap(TypeSerializer<BT> buildSerializer, TypeComparator<BT> buildComparator) {\n    super(64);\n    this.buildSerializer = buildSerializer;\n    this.buildComparator = buildComparator;\n}",
        "public Prober(TypeComparator<PT> probeComparator, TypePairComparator<PT, BT> pairComparator) {\n    this.probeComparator = probeComparator;\n    this.pairComparator = pairComparator;\n}"
      ],
      "fieldDeclarations": [
        "private final TypeSerializer<BT> buildSerializer;",
        "private final TypeComparator<BT> buildComparator;"
      ],
      "setters": [
        "@SuppressWarnings(\"unchecked\")\npublic void insertOrReplace(BT record) {\n    int hashCode = hash(buildComparator.hash(record));\n    int index = hashIndex(hashCode, data.length);\n    buildComparator.setReference(record);\n    HashEntry entry = data[index];\n    while (entry != null) {\n        if ((entryHashCode(entry) == hashCode) && buildComparator.equalToReference(((BT) (entry.getValue())))) {\n            entry.setValue(record);\n            return;\n        }\n        entry = entryNext(entry);\n    } \n    addMapping(index, hashCode, null, record);\n}"
      ],
      "imports": [
        "org.apache.commons.collections.map.AbstractHashedMap",
        "org.apache.commons.collections.map.AbstractHashedMap.HashEntry",
        "org.apache.flink.api.common.operators.util.JoinHashMap.Prober",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypePairComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators.util;\n\npublic class JoinHashMapinsertOrReplace_HashEntrysetValueFikaTest {\n\n    @Test\n    public void testInsertOrReplace() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.util.JoinHashMap.insertOrReplace(java.lang.Object)",
      "thirdPartyMethod": "org.apache.commons.collections.map.AbstractHashedMap.HashEntry.getValue()",
      "directCaller": "org.apache.flink.api.common.operators.util.JoinHashMap.insertOrReplace(java.lang.Object)",
      "path": [
        "org.apache.flink.api.common.operators.util.JoinHashMap.insertOrReplace(java.lang.Object)",
        "org.apache.commons.collections.map.AbstractHashedMap.HashEntry.getValue()"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\npublic void insertOrReplace(BT record) {\n    int hashCode = hash(buildComparator.hash(record));\n    int index = hashIndex(hashCode, data.length);\n    buildComparator.setReference(record);\n    HashEntry entry = data[index];\n    while (entry != null) {\n        if ((entryHashCode(entry) == hashCode) && // PATH: Test should invoke the next AbstractHashedMap$HashEntry.getValue(...) [step in execution path]\n        buildComparator.equalToReference(((BT) (entry.getValue())))) {\n            entry.setValue(record);\n            return;\n        }\n        entry = entryNext(entry);\n    } \n    addMapping(index, hashCode, null, record);\n}"
      ],
      "constructors": [
        "public JoinHashMap(TypeSerializer<BT> buildSerializer, TypeComparator<BT> buildComparator) {\n    super(64);\n    this.buildSerializer = buildSerializer;\n    this.buildComparator = buildComparator;\n}",
        "public Prober(TypeComparator<PT> probeComparator, TypePairComparator<PT, BT> pairComparator) {\n    this.probeComparator = probeComparator;\n    this.pairComparator = pairComparator;\n}"
      ],
      "fieldDeclarations": [
        "private final TypeSerializer<BT> buildSerializer;",
        "private final TypeComparator<BT> buildComparator;"
      ],
      "setters": [
        "@SuppressWarnings(\"unchecked\")\npublic void insertOrReplace(BT record) {\n    int hashCode = hash(buildComparator.hash(record));\n    int index = hashIndex(hashCode, data.length);\n    buildComparator.setReference(record);\n    HashEntry entry = data[index];\n    while (entry != null) {\n        if ((entryHashCode(entry) == hashCode) && buildComparator.equalToReference(((BT) (entry.getValue())))) {\n            entry.setValue(record);\n            return;\n        }\n        entry = entryNext(entry);\n    } \n    addMapping(index, hashCode, null, record);\n}"
      ],
      "imports": [
        "org.apache.commons.collections.map.AbstractHashedMap",
        "org.apache.commons.collections.map.AbstractHashedMap.HashEntry",
        "org.apache.flink.api.common.operators.util.JoinHashMap.Prober",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypePairComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators.util;\n\npublic class JoinHashMapinsertOrReplace_HashEntrygetValueFikaTest {\n\n    @Test\n    public void testInsertOrReplace() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Dump.dumpToString(java.lang.Object)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
        "org.snakeyaml.engine.v2.api.Dump.dumpToString(java.lang.Object)"
      ],
      "methodSources": [
        "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        // PATH: Test should invoke the next Dump.dumpToString(...) [step in execution path]\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsconvertAndDumpYamlFromFlatMap_DumpdumpToStringFikaTest {\n\n    @Test\n    public void testConvertAndDumpYamlFromFlatMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.util.JoinHashMap.Prober.lookupMatch(java.lang.Object)",
      "thirdPartyMethod": "org.apache.commons.collections.map.AbstractHashedMap.HashEntry.getValue()",
      "directCaller": "org.apache.flink.api.common.operators.util.JoinHashMap.Prober.lookupMatch(java.lang.Object)",
      "path": [
        "org.apache.flink.api.common.operators.util.JoinHashMap.Prober.lookupMatch(java.lang.Object)",
        "org.apache.commons.collections.map.AbstractHashedMap.HashEntry.getValue()"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\npublic BT lookupMatch(PT record) {\n    int hashCode = hash(probeComparator.hash(record));\n    int index = hashIndex(hashCode, data.length);\n    pairComparator.setReference(record);\n    HashEntry entry = data[index];\n    while (entry != null) {\n        if ((entryHashCode(entry) == hashCode) && // PATH: Test should invoke the next AbstractHashedMap$HashEntry.getValue(...) [step in execution path]\n        pairComparator.equalToReference(((BT) (entry.getValue())))) {\n            return ((BT) (entry.getValue()));\n        }\n        entry = entryNext(entry);\n    } \n    return null;\n}"
      ],
      "constructors": [
        "public Prober(TypeComparator<PT> probeComparator, TypePairComparator<PT, BT> pairComparator) {\n    this.probeComparator = probeComparator;\n    this.pairComparator = pairComparator;\n}"
      ],
      "fieldDeclarations": [
        "private final TypeComparator<PT> probeComparator;",
        "private final TypePairComparator<PT, BT> pairComparator;"
      ],
      "setters": [],
      "imports": [
        "org.apache.commons.collections.map.AbstractHashedMap",
        "org.apache.commons.collections.map.AbstractHashedMap.HashEntry",
        "org.apache.flink.api.common.operators.util.JoinHashMap.Prober",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypePairComparator"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators.util;\n\npublic class ProberlookupMatch_HashEntrygetValueFikaTest {\n\n    @Test\n    public void testLookupMatch() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 2,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.io.Output.<init>(java.io.OutputStream)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
        "com.esotericsoftware.kryo.io.Output.<init>(java.io.OutputStream)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) // PATH: Test should invoke the next new Output(...) [step in execution path]\n        {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializercopy_OutputmethodFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 3,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.writeObject(com.esotericsoftware.kryo.io.Output, java.lang.Object)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
        "com.esotericsoftware.kryo.Kryo.writeObject(com.esotericsoftware.kryo.io.Output, java.lang.Object)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) // PATH: Test should invoke the next Kryo.writeObject(...) [step in execution path]\n        {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializercopy_KryowriteObjectFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 3,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.copy(java.lang.Object)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
        "com.esotericsoftware.kryo.Kryo.copy(java.lang.Object)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            // PATH: Test should invoke the next Kryo.copy(...) [step in execution path]\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializercopy_KryocopyFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 3,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.readObject(com.esotericsoftware.kryo.io.Input, java.lang.Class)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
        "com.esotericsoftware.kryo.Kryo.readObject(com.esotericsoftware.kryo.io.Input, java.lang.Class)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            // PATH: Test should invoke the next Kryo.readObject(...) [step in execution path]\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializercopy_KryoreadObjectFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 3,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.io.Input.<init>(java.io.InputStream)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
        "com.esotericsoftware.kryo.io.Input.<init>(java.io.InputStream)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializercopy_InputmethodFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 3,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.KryoException.getCause()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
        "com.esotericsoftware.kryo.KryoException.getCause()"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerdeserialize_KryoExceptiongetCauseFikaTest {\n\n    @Test\n    public void testDeserialize() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.readClassAndObject(com.esotericsoftware.kryo.io.Input)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
        "com.esotericsoftware.kryo.Kryo.readClassAndObject(com.esotericsoftware.kryo.io.Input)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            // PATH: Test should invoke the next Kryo.readClassAndObject(...) [step in execution path]\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerdeserialize_KryoreadClassAndObjectFikaTest {\n\n    @Test\n    public void testDeserialize() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.util.ParameterTool.fromArgs(java.lang.String[])",
      "thirdPartyMethod": "org.apache.commons.lang3.math.NumberUtils.isNumber(java.lang.String)",
      "directCaller": "org.apache.flink.util.ParameterTool.fromArgs(java.lang.String[])",
      "path": [
        "org.apache.flink.util.ParameterTool.fromArgs(java.lang.String[])",
        "org.apache.commons.lang3.math.NumberUtils.isNumber(java.lang.String)"
      ],
      "methodSources": [
        "// ------------------ Constructors ------------------------\n/**\n * Returns {@link ParameterTool} for the given arguments. The arguments are keys followed by\n * values. Keys have to start with '-' or '--'\n *\n * <p><strong>Example arguments:</strong> --key1 value1 --key2 value2 -key3 value3\n *\n * @param args\n * \t\tInput array arguments\n * @return A {@link ParameterTool}\n */\npublic static ParameterTool fromArgs(String[] args) {\n    final Map<String, String> map = CollectionUtil.newHashMapWithExpectedSize(args.length / 2);\n    int i = 0;\n    while (i < args.length) {\n        final String key = Utils.getKeyFromArgs(args, i);\n        if (key.isEmpty()) {\n            throw new IllegalArgumentException((\"The input \" + Arrays.toString(args)) + \" contains an empty argument\");\n        }\n        i += 1;// try to find the value\n\n        if (i >= args.length) {\n            map.put(key, AbstractParameterTool.NO_VALUE_KEY);\n        } else // PATH: Test should invoke the next NumberUtils.isNumber(...) [step in execution path]\n        if (NumberUtils.isNumber(args[i])) {\n            map.put(key, args[i]);\n            i += 1;\n        } else if (args[i].startsWith(\"--\") || args[i].startsWith(\"-\")) {\n            // the argument cannot be a negative number because we checked earlier\n            // -> the next argument is a parameter name\n            map.put(key, AbstractParameterTool.NO_VALUE_KEY);\n        } else {\n            map.put(key, args[i]);\n            i += 1;\n        }\n    } \n    return fromMap(map);\n}"
      ],
      "constructors": [
        "private ParameterTool(Map<String, String> data) {\n    this.data = Collections.unmodifiableMap(new HashMap<>(data));\n    this.defaultData = new ConcurrentHashMap<>(data.size());\n    this.unrequestedParameters = Collections.newSetFromMap(new ConcurrentHashMap<>(data.size()));\n    unrequestedParameters.addAll(data.keySet());\n}",
        "// ------------------ Constructors ------------------------\n/**\n * Returns {@link ParameterTool} for the given arguments. The arguments are keys followed by\n * values. Keys have to start with '-' or '--'\n *\n * <p><strong>Example arguments:</strong> --key1 value1 --key2 value2 -key3 value3\n *\n * @param args\n * \t\tInput array arguments\n * @return A {@link ParameterTool}\n */\npublic static ParameterTool fromArgs(String[] args) {\n    final Map<String, String> map = CollectionUtil.newHashMapWithExpectedSize(args.length / 2);\n    int i = 0;\n    while (i < args.length) {\n        final String key = Utils.getKeyFromArgs(args, i);\n        if (key.isEmpty()) {\n            throw new IllegalArgumentException((\"The input \" + Arrays.toString(args)) + \" contains an empty argument\");\n        }\n        i += 1;// try to find the value\n\n        if (i >= args.length) {\n            map.put(key, AbstractParameterTool.NO_VALUE_KEY);\n        } else if (NumberUtils.isNumber(args[i])) {\n            map.put(key, args[i]);\n            i += 1;\n        } else if (args[i].startsWith(\"--\") || args[i].startsWith(\"-\")) {\n            // the argument cannot be a negative number because we checked earlier\n            // -> the next argument is a parameter name\n            map.put(key, AbstractParameterTool.NO_VALUE_KEY);\n        } else {\n            map.put(key, args[i]);\n            i += 1;\n        }\n    } \n    return fromMap(map);\n}",
        "/**\n * Returns {@link ParameterTool} for the given map.\n *\n * @param map\n * \t\tA map of arguments. Both Key and Value have to be Strings\n * @return A {@link ParameterTool}\n */\npublic static ParameterTool fromMap(Map<String, String> map) {\n    Preconditions.checkNotNull(map, \"Unable to initialize from empty map\");\n    return new ParameterTool(map);\n}",
        "/**\n * Returns {@link ParameterTool} for the given {@link Properties} file.\n *\n * @param file\n * \t\tFile object to the properties file\n * @return A {@link ParameterTool}\n * @throws IOException\n * \t\tIf the file does not exist\n * @see Properties\n */\npublic static ParameterTool fromPropertiesFile(File file) throws IOException {\n    if (!file.exists()) {\n        throw new FileNotFoundException((\"Properties file \" + file.getAbsolutePath()) + \" does not exist\");\n    }\n    try (FileInputStream fis = new FileInputStream(file)) {\n        return fromPropertiesFile(fis);\n    }\n}",
        "/**\n * Returns {@link ParameterTool} for the given InputStream from {@link Properties} file.\n *\n * @param inputStream\n * \t\tInputStream from the properties file\n * @return A {@link ParameterTool}\n * @throws IOException\n * \t\tIf the file does not exist\n * @see Properties\n */\npublic static ParameterTool fromPropertiesFile(InputStream inputStream) throws IOException {\n    Properties props = new Properties();\n    props.load(inputStream);\n    return fromMap(((Map) (props)));\n}",
        "/**\n * Returns {@link ParameterTool} for the given {@link Properties} file.\n *\n * @param path\n * \t\tPath to the properties file\n * @return A {@link ParameterTool}\n * @throws IOException\n * \t\tIf the file does not exist\n * @see Properties\n */\npublic static ParameterTool fromPropertiesFile(String path) throws IOException {\n    File propertiesFile = new File(path);\n    return fromPropertiesFile(propertiesFile);\n}",
        "/**\n * Returns {@link ParameterTool} from the system properties. Example on how to pass system\n * properties: -Dkey1=value1 -Dkey2=value2\n *\n * @return A {@link ParameterTool}\n */\npublic static ParameterTool fromSystemProperties() {\n    return fromMap(((Map) (System.getProperties())));\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "// ------------------ ParameterUtil  ------------------------\nprotected final Map<String, String> data;"
      ],
      "setters": [
        "// ------------------------- Serialization ---------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    defaultData = new ConcurrentHashMap<>(data.size());\n    unrequestedParameters = Collections.newSetFromMap(new ConcurrentHashMap<>(data.size()));\n}"
      ],
      "imports": [
        "org.apache.commons.lang3.math.NumberUtils",
        "org.apache.flink.configuration.Configuration"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class ParameterToolfromArgs_NumberUtilsisNumberFikaTest {\n\n    @Test\n    public void testFromArgs() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.io.Output.position()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
        "com.esotericsoftware.kryo.io.Output.position()"
      ],
      "methodSources": [
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        // PATH: Test should invoke the next Output.position(...) [step in execution path]\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerserialize_OutputpositionFikaTest {\n\n    @Test\n    public void testSerialize() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.writeClassAndObject(com.esotericsoftware.kryo.io.Output, java.lang.Object)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
        "com.esotericsoftware.kryo.Kryo.writeClassAndObject(com.esotericsoftware.kryo.io.Output, java.lang.Object)"
      ],
      "methodSources": [
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try // PATH: Test should invoke the next Kryo.writeClassAndObject(...) [step in execution path]\n        {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerserialize_KryowriteClassAndObjectFikaTest {\n\n    @Test\n    public void testSerialize() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.io.Output.<init>(java.io.OutputStream)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
        "com.esotericsoftware.kryo.io.Output.<init>(java.io.OutputStream)"
      ],
      "methodSources": [
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        // PATH: Test should invoke the next new Output(...) [step in execution path]\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerserialize_OutputmethodFikaTest {\n\n    @Test\n    public void testSerialize() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 3,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(byte[], int, int)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.KryoException.<init>(java.lang.Throwable)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(byte[], int, int)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(byte[], int, int)",
        "com.esotericsoftware.kryo.KryoException.<init>(java.lang.Throwable)"
      ],
      "methodSources": [
        "@Override\npublic void readBytes(byte[] bytes, int offset, int count) throws KryoException {\n    if (bytes == null) {\n        throw new IllegalArgumentException(\"bytes cannot be null.\");\n    }\n    if (count == 0) {\n        return;\n    }\n    try {\n        int bytesRead = 0;\n        int c;\n        while (true) {\n            c = inputStream.read(bytes, offset + bytesRead, count - bytesRead);\n            if (c == (-1)) {\n                throw new KryoException(new EOFException(\"No more bytes left.\"));\n            }\n            bytesRead += c;\n            if (bytesRead == count) {\n                break;\n            }\n        } \n    } catch (IOException ex) {\n        throw new KryoException(ex);\n    }\n}"
      ],
      "constructors": [
        "public NoFetchingInput(InputStream inputStream) {\n    super(inputStream, 8);\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.io.Input"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class NoFetchingInputreadBytes_KryoExceptionmethodFikaTest {\n\n    @Test\n    public void testReadBytes() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.zip.ZipArchiveEntry.isDirectory()",
      "directCaller": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
        "org.apache.commons.compress.archivers.zip.ZipArchiveEntry.isDirectory()"
      ],
      "methodSources": [
        "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            // PATH: Test should invoke the next ZipArchiveEntry.isDirectory(...) [step in execution path]\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.zip.ZipArchiveEntry",
        "org.apache.commons.compress.archivers.zip.ZipFile",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsextractZipFileWithPermissions_ZipArchiveEntryisDirectoryFikaTest {\n\n    @Test\n    public void testExtractZipFileWithPermissions() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.zip.ZipFile.close()",
      "directCaller": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
        "org.apache.commons.compress.archivers.zip.ZipFile.close()"
      ],
      "methodSources": [
        "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.zip.ZipArchiveEntry",
        "org.apache.commons.compress.archivers.zip.ZipFile",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsextractZipFileWithPermissions_ZipFilecloseFikaTest {\n\n    @Test\n    public void testExtractZipFileWithPermissions() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.zip.ZipFile.getEntries()",
      "directCaller": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
        "org.apache.commons.compress.archivers.zip.ZipFile.getEntries()"
      ],
      "methodSources": [
        "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        // PATH: Test should invoke the next ZipFile.getEntries(...) [step in execution path]\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.zip.ZipArchiveEntry",
        "org.apache.commons.compress.archivers.zip.ZipFile",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsextractZipFileWithPermissions_ZipFilegetEntriesFikaTest {\n\n    @Test\n    public void testExtractZipFileWithPermissions() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.zip.ZipArchiveEntry.getUnixMode()",
      "directCaller": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
        "org.apache.commons.compress.archivers.zip.ZipArchiveEntry.getUnixMode()"
      ],
      "methodSources": [
        "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                // PATH: Test should invoke the next ZipArchiveEntry.getUnixMode(...) [step in execution path]\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.zip.ZipArchiveEntry",
        "org.apache.commons.compress.archivers.zip.ZipFile",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsextractZipFileWithPermissions_ZipArchiveEntrygetUnixModeFikaTest {\n\n    @Test\n    public void testExtractZipFileWithPermissions() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.zip.ZipArchiveEntry.isUnixSymlink()",
      "directCaller": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
        "org.apache.commons.compress.archivers.zip.ZipArchiveEntry.isUnixSymlink()"
      ],
      "methodSources": [
        "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                // PATH: Test should invoke the next ZipArchiveEntry.isUnixSymlink(...) [step in execution path]\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.zip.ZipArchiveEntry",
        "org.apache.commons.compress.archivers.zip.ZipFile",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsextractZipFileWithPermissions_ZipArchiveEntryisUnixSymlinkFikaTest {\n\n    @Test\n    public void testExtractZipFileWithPermissions() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.zip.ZipArchiveEntry.getName()",
      "directCaller": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
        "org.apache.commons.compress.archivers.zip.ZipArchiveEntry.getName()"
      ],
      "methodSources": [
        "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = // PATH: Test should invoke the next ZipArchiveEntry.getName(...) [step in execution path]\n            new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.zip.ZipArchiveEntry",
        "org.apache.commons.compress.archivers.zip.ZipFile",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsextractZipFileWithPermissions_ZipArchiveEntrygetNameFikaTest {\n\n    @Test\n    public void testExtractZipFileWithPermissions() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 2,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.zip.ZipFile.<init>(java.lang.String)",
      "directCaller": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
        "org.apache.commons.compress.archivers.zip.ZipFile.<init>(java.lang.String)"
      ],
      "methodSources": [
        "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        // PATH: Test should invoke the next new ZipFile(...) [step in execution path]\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.zip.ZipArchiveEntry",
        "org.apache.commons.compress.archivers.zip.ZipFile",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsextractZipFileWithPermissions_ZipFilemethodFikaTest {\n\n    @Test\n    public void testExtractZipFileWithPermissions() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 3,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.zip.ZipFile.getInputStream(org.apache.commons.compress.archivers.zip.ZipArchiveEntry)",
      "directCaller": "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions(java.lang.String, java.lang.String)",
        "org.apache.commons.compress.archivers.zip.ZipFile.getInputStream(org.apache.commons.compress.archivers.zip.ZipArchiveEntry)"
      ],
      "methodSources": [
        "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    // PATH: Test should invoke the next ZipFile.getInputStream(...) [step in execution path]\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.zip.ZipArchiveEntry",
        "org.apache.commons.compress.archivers.zip.ZipFile",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsextractZipFileWithPermissions_ZipFilegetInputStreamFikaTest {\n\n    @Test\n    public void testExtractZipFileWithPermissions() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 2,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)"
      ],
      "methodSources": [
        "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_DumpSettingsBuildersetDefaultFlowStyleFikaTest {\n\n    @Test\n    public void testConvertToObject() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_DumpSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testConvertToObject() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)"
      ],
      "methodSources": [
        "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_DumpmethodFikaTest {\n\n    @Test\n    public void testConvertToObject() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_LoadSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testConvertToObject() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)"
      ],
      "methodSources": [
        "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_LoadmethodFikaTest {\n\n    @Test\n    public void testConvertToObject() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines(org.apache.flink.configuration.Configuration, boolean)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines(org.apache.flink.configuration.Configuration, boolean)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)"
      ],
      "methodSources": [
        "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpSettingsBuildersetDefaultFlowStyleFikaTest {\n\n    @Test\n    public void testConvertConfigToWritableLines() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)"
      ],
      "methodSources": [
        "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_DumpSettingsBuildersetDefaultFlowStyleFikaTest {\n\n    @Test\n    public void testToYAMLString() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.Configuration.toFileWritableMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.Configuration.toFileWritableMap()",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)"
      ],
      "methodSources": [
        "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Creates a new empty configuration.\n */\npublic Configuration() {\n    this.confData = new HashMap<>();\n}",
        "/**\n * Creates a new configuration with the copy of the given configuration.\n *\n * @param other\n * \t\tThe configuration to copy the entries from.\n */\npublic Configuration(Configuration other) {\n    this.confData = new HashMap<>(other.confData);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private static final byte TYPE_STRING = 0;",
        "private static final byte TYPE_INT = 1;",
        "private static final byte TYPE_LONG = 2;",
        "private static final byte TYPE_BOOLEAN = 3;",
        "private static final byte TYPE_FLOAT = 4;",
        "private static final byte TYPE_DOUBLE = 5;",
        "private static final byte TYPE_BYTES = 6;",
        "/**\n * The log object used for debugging.\n */\nprivate static final Logger LOG = LoggerFactory.getLogger(Configuration.class);",
        "/**\n * Stores the concrete key/value pairs of this configuration object.\n *\n * <p>NOTE: This map stores the values that are actually used, and does not include any escaping\n * that is required by the standard YAML syntax.\n */\nprotected final HashMap<String, Object> confData;"
      ],
      "setters": [
        "public void addAll(Configuration other) {\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            this.confData.putAll(other.confData);\n        }\n    }\n}",
        "/**\n * Adds all entries from the given configuration into this configuration. The keys are prepended\n * with the given prefix.\n *\n * @param other\n * \t\tThe configuration whose entries are added to this configuration.\n * @param prefix\n * \t\tThe prefix to prepend.\n */\npublic void addAll(Configuration other, String prefix) {\n    final StringBuilder bld = new StringBuilder();\n    bld.append(prefix);\n    final int pl = bld.length();\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            for (Map.Entry<String, Object> entry : other.confData.entrySet()) {\n                bld.setLength(pl);\n                bld.append(entry.getKey());\n                this.confData.put(bld.toString(), entry.getValue());\n            }\n        }\n    }\n}",
        "/**\n * Adds all entries in this {@code Configuration} to the given {@link Properties}.\n */\npublic void addAllToProperties(Properties props) {\n    synchronized(this.confData) {\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            props.put(entry.getKey(), entry.getValue());\n        }\n    }\n}",
        "private void loggingFallback(FallbackKey fallbackKey, ConfigOption<?> configOption) {\n    if (fallbackKey.isDeprecated()) {\n        LOG.warn(\"Config uses deprecated configuration key '{}' instead of proper key '{}'\", fallbackKey.getKey(), configOption.key());\n    } else {\n        LOG.info(\"Config uses fallback configuration key '{}' instead of key '{}'\", fallbackKey.getKey(), configOption.key());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// Serialization\n// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    synchronized(this.confData) {\n        final int numberOfProperties = in.readInt();\n        for (int i = 0; i < numberOfProperties; i++) {\n            String key = StringValue.readString(in);\n            Object value;\n            byte type = in.readByte();\n            switch (type) {\n                case TYPE_STRING :\n                    value = StringValue.readString(in);\n                    break;\n                case TYPE_INT :\n                    value = in.readInt();\n                    break;\n                case TYPE_LONG :\n                    value = in.readLong();\n                    break;\n                case TYPE_FLOAT :\n                    value = in.readFloat();\n                    break;\n                case TYPE_DOUBLE :\n                    value = in.readDouble();\n                    break;\n                case TYPE_BOOLEAN :\n                    value = in.readBoolean();\n                    break;\n                case TYPE_BYTES :\n                    byte[] bytes = new byte[in.readInt()];\n                    in.readFully(bytes);\n                    value = bytes;\n                    break;\n                default :\n                    throw new IOException(String.format(\"Unrecognized type: %s. This method is deprecated and\" + \" might not work for all supported types.\", type));\n            }\n            this.confData.put(key, value);\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n<T> void setValueInternal(String key, T value, boolean canBePrefixMap) {\n    if (key == null) {\n        throw new NullPointerException(\"Key must not be null.\");\n    }\n    if (value == null) {\n        throw new NullPointerException(\"Value must not be null.\");\n    }\n    synchronized(this.confData) {\n        if (canBePrefixMap) {\n            removePrefixMap(this.confData, key);\n        }\n        this.confData.put(key, value);\n    }\n}",
        "@Override\npublic void write(final DataOutputView out) throws IOException {\n    synchronized(this.confData) {\n        out.writeInt(this.confData.size());\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            String key = entry.getKey();\n            Object val = entry.getValue();\n            StringValue.writeString(key, out);\n            Class<?> clazz = val.getClass();\n            if (clazz == String.class) {\n                out.write(TYPE_STRING);\n                StringValue.writeString(((String) (val)), out);\n            } else if (clazz == Integer.class) {\n                out.write(TYPE_INT);\n                out.writeInt(((Integer) (val)));\n            } else if (clazz == Long.class) {\n                out.write(TYPE_LONG);\n                out.writeLong(((Long) (val)));\n            } else if (clazz == Float.class) {\n                out.write(TYPE_FLOAT);\n                out.writeFloat(((Float) (val)));\n            } else if (clazz == Double.class) {\n                out.write(TYPE_DOUBLE);\n                out.writeDouble(((Double) (val)));\n            } else if (clazz == byte[].class) {\n                out.write(TYPE_BYTES);\n                byte[] bytes = ((byte[]) (val));\n                out.writeInt(bytes.length);\n                out.write(bytes);\n            } else if (clazz == Boolean.class) {\n                out.write(TYPE_BOOLEAN);\n                out.writeBoolean(((Boolean) (val)));\n            } else {\n                throw new IllegalArgumentException(\"Unrecognized type. This method is deprecated and might not work\" + \" for all supported types.\");\n            }\n        }\n    }\n}"
      ],
      "imports": [
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.ExecutionConfig.GlobalJobParameters",
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.types.StringValue",
        "org.apache.flink.util.CollectionUtil",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class Configuration_YamlParserUtilsmethod_DumpSettingsBuildersetDefaultFlowStyleFikaTest {\n\n    @Test\n    public void testToFileWritableMap() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference(org.apache.flink.types.Value)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.ValueComparator.checkKryoInitialized()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference(org.apache.flink.types.Value)",
        "org.apache.flink.api.java.typeutils.runtime.ValueComparator.checkKryoInitialized()",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@Override\npublic void setReference(T toCompare) // PATH: Test should invoke the next ValueComparator.checkKryoInitialized(...) [step in execution path]\n{\n    checkKryoInitialized();\n    reference = KryoUtils.copy(toCompare, kryo, new ValueSerializer<T>(type));\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) // PATH: Test should invoke the next DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(...) [step in execution path]\n    {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy instantiatorStrategy = new DefaultInstantiatorStrategy();\n        instantiatorStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(instantiatorStrategy);\n        this.kryo.register(type);\n    }\n}"
      ],
      "constructors": [
        "public ValueComparator(boolean ascending, Class<T> type) {\n    this.type = type;\n    this.ascendingComparison = ascending;\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private final Class<T> type;",
        "private final boolean ascendingComparison;",
        "private transient T reference;",
        "private transient T tempReference;",
        "private transient Kryo kryo;",
        "@SuppressWarnings(\"rawtypes\")\nprivate final TypeComparator[] comparators = new TypeComparator[]{ this };"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy instantiatorStrategy = new DefaultInstantiatorStrategy();\n        instantiatorStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(instantiatorStrategy);\n        this.kryo.register(type);\n    }\n}",
        "@Override\npublic void setReference(T toCompare) {\n    checkKryoInitialized();\n    reference = KryoUtils.copy(toCompare, kryo, new ValueSerializer<T>(type));\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.Registration",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.io.IOReadableWritable",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.types.NormalizableKey",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.InstantiationUtil",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class ValueComparatorcheckKryoInitialized_DefaultInstantiatorStrategysetFallbackInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testSetReference() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines(org.apache.flink.configuration.Configuration, boolean)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines(org.apache.flink.configuration.Configuration, boolean)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testConvertConfigToWritableLines() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_DumpSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testToYAMLString() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.Configuration.toFileWritableMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.Configuration.toFileWritableMap()",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Creates a new empty configuration.\n */\npublic Configuration() {\n    this.confData = new HashMap<>();\n}",
        "/**\n * Creates a new configuration with the copy of the given configuration.\n *\n * @param other\n * \t\tThe configuration to copy the entries from.\n */\npublic Configuration(Configuration other) {\n    this.confData = new HashMap<>(other.confData);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private static final byte TYPE_STRING = 0;",
        "private static final byte TYPE_INT = 1;",
        "private static final byte TYPE_LONG = 2;",
        "private static final byte TYPE_BOOLEAN = 3;",
        "private static final byte TYPE_FLOAT = 4;",
        "private static final byte TYPE_DOUBLE = 5;",
        "private static final byte TYPE_BYTES = 6;",
        "/**\n * The log object used for debugging.\n */\nprivate static final Logger LOG = LoggerFactory.getLogger(Configuration.class);",
        "/**\n * Stores the concrete key/value pairs of this configuration object.\n *\n * <p>NOTE: This map stores the values that are actually used, and does not include any escaping\n * that is required by the standard YAML syntax.\n */\nprotected final HashMap<String, Object> confData;"
      ],
      "setters": [
        "public void addAll(Configuration other) {\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            this.confData.putAll(other.confData);\n        }\n    }\n}",
        "/**\n * Adds all entries from the given configuration into this configuration. The keys are prepended\n * with the given prefix.\n *\n * @param other\n * \t\tThe configuration whose entries are added to this configuration.\n * @param prefix\n * \t\tThe prefix to prepend.\n */\npublic void addAll(Configuration other, String prefix) {\n    final StringBuilder bld = new StringBuilder();\n    bld.append(prefix);\n    final int pl = bld.length();\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            for (Map.Entry<String, Object> entry : other.confData.entrySet()) {\n                bld.setLength(pl);\n                bld.append(entry.getKey());\n                this.confData.put(bld.toString(), entry.getValue());\n            }\n        }\n    }\n}",
        "/**\n * Adds all entries in this {@code Configuration} to the given {@link Properties}.\n */\npublic void addAllToProperties(Properties props) {\n    synchronized(this.confData) {\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            props.put(entry.getKey(), entry.getValue());\n        }\n    }\n}",
        "private void loggingFallback(FallbackKey fallbackKey, ConfigOption<?> configOption) {\n    if (fallbackKey.isDeprecated()) {\n        LOG.warn(\"Config uses deprecated configuration key '{}' instead of proper key '{}'\", fallbackKey.getKey(), configOption.key());\n    } else {\n        LOG.info(\"Config uses fallback configuration key '{}' instead of key '{}'\", fallbackKey.getKey(), configOption.key());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// Serialization\n// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    synchronized(this.confData) {\n        final int numberOfProperties = in.readInt();\n        for (int i = 0; i < numberOfProperties; i++) {\n            String key = StringValue.readString(in);\n            Object value;\n            byte type = in.readByte();\n            switch (type) {\n                case TYPE_STRING :\n                    value = StringValue.readString(in);\n                    break;\n                case TYPE_INT :\n                    value = in.readInt();\n                    break;\n                case TYPE_LONG :\n                    value = in.readLong();\n                    break;\n                case TYPE_FLOAT :\n                    value = in.readFloat();\n                    break;\n                case TYPE_DOUBLE :\n                    value = in.readDouble();\n                    break;\n                case TYPE_BOOLEAN :\n                    value = in.readBoolean();\n                    break;\n                case TYPE_BYTES :\n                    byte[] bytes = new byte[in.readInt()];\n                    in.readFully(bytes);\n                    value = bytes;\n                    break;\n                default :\n                    throw new IOException(String.format(\"Unrecognized type: %s. This method is deprecated and\" + \" might not work for all supported types.\", type));\n            }\n            this.confData.put(key, value);\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n<T> void setValueInternal(String key, T value, boolean canBePrefixMap) {\n    if (key == null) {\n        throw new NullPointerException(\"Key must not be null.\");\n    }\n    if (value == null) {\n        throw new NullPointerException(\"Value must not be null.\");\n    }\n    synchronized(this.confData) {\n        if (canBePrefixMap) {\n            removePrefixMap(this.confData, key);\n        }\n        this.confData.put(key, value);\n    }\n}",
        "@Override\npublic void write(final DataOutputView out) throws IOException {\n    synchronized(this.confData) {\n        out.writeInt(this.confData.size());\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            String key = entry.getKey();\n            Object val = entry.getValue();\n            StringValue.writeString(key, out);\n            Class<?> clazz = val.getClass();\n            if (clazz == String.class) {\n                out.write(TYPE_STRING);\n                StringValue.writeString(((String) (val)), out);\n            } else if (clazz == Integer.class) {\n                out.write(TYPE_INT);\n                out.writeInt(((Integer) (val)));\n            } else if (clazz == Long.class) {\n                out.write(TYPE_LONG);\n                out.writeLong(((Long) (val)));\n            } else if (clazz == Float.class) {\n                out.write(TYPE_FLOAT);\n                out.writeFloat(((Float) (val)));\n            } else if (clazz == Double.class) {\n                out.write(TYPE_DOUBLE);\n                out.writeDouble(((Double) (val)));\n            } else if (clazz == byte[].class) {\n                out.write(TYPE_BYTES);\n                byte[] bytes = ((byte[]) (val));\n                out.writeInt(bytes.length);\n                out.write(bytes);\n            } else if (clazz == Boolean.class) {\n                out.write(TYPE_BOOLEAN);\n                out.writeBoolean(((Boolean) (val)));\n            } else {\n                throw new IllegalArgumentException(\"Unrecognized type. This method is deprecated and might not work\" + \" for all supported types.\");\n            }\n        }\n    }\n}"
      ],
      "imports": [
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.ExecutionConfig.GlobalJobParameters",
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.types.StringValue",
        "org.apache.flink.util.CollectionUtil",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class Configuration_YamlParserUtilsmethod_DumpSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testToFileWritableMap() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference(org.apache.flink.types.Value)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.ValueComparator.checkKryoInitialized()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference(org.apache.flink.types.Value)",
        "org.apache.flink.api.java.typeutils.runtime.ValueComparator.checkKryoInitialized()",
        "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@Override\npublic void setReference(T toCompare) // PATH: Test should invoke the next ValueComparator.checkKryoInitialized(...) [step in execution path]\n{\n    checkKryoInitialized();\n    reference = KryoUtils.copy(toCompare, kryo, new ValueSerializer<T>(type));\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) // PATH: Test should invoke the next Kryo.setInstantiatorStrategy(...) [step in execution path]\n    {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy instantiatorStrategy = new DefaultInstantiatorStrategy();\n        instantiatorStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(instantiatorStrategy);\n        this.kryo.register(type);\n    }\n}"
      ],
      "constructors": [
        "public ValueComparator(boolean ascending, Class<T> type) {\n    this.type = type;\n    this.ascendingComparison = ascending;\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private final Class<T> type;",
        "private final boolean ascendingComparison;",
        "private transient T reference;",
        "private transient T tempReference;",
        "private transient Kryo kryo;",
        "@SuppressWarnings(\"rawtypes\")\nprivate final TypeComparator[] comparators = new TypeComparator[]{ this };"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy instantiatorStrategy = new DefaultInstantiatorStrategy();\n        instantiatorStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(instantiatorStrategy);\n        this.kryo.register(type);\n    }\n}",
        "@Override\npublic void setReference(T toCompare) {\n    checkKryoInitialized();\n    reference = KryoUtils.copy(toCompare, kryo, new ValueSerializer<T>(type));\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.Registration",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.io.IOReadableWritable",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.types.NormalizableKey",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.InstantiationUtil",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class ValueComparatorcheckKryoInitialized_KryosetInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testSetReference() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy(org.apache.flink.types.Value)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy(org.apache.flink.types.Value)",
        "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized()",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@Override\npublic T copy(T from) // PATH: Test should invoke the next ValueSerializer.checkKryoInitialized(...) [step in execution path]\n{\n    checkKryoInitialized();\n    return KryoUtils.copy(from, kryo, this);\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) // PATH: Test should invoke the next DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(...) [step in execution path]\n    {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic ValueSerializer(Class<T> type) {\n    this.type = checkNotNull(type);\n    this.kryoRegistrations = asKryoRegistrations(type);\n}",
        "@SuppressWarnings(\"unused\")\npublic ValueSerializerSnapshot() {\n}",
        "ValueSerializerSnapshot(Class<T> typeClass) {\n    super(typeClass);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private final Class<T> type;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n *\n * <p>Currently, we only have one single registration for the value type. Nevertheless, we keep\n * this information here for future compatibility.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private transient Kryo kryo;",
        "private transient T copyInstance;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (this.copyInstance == null) {\n        this.copyInstance = InstantiationUtil.instantiate(type);\n    }\n    this.copyInstance.read(source);\n    this.copyInstance.write(target);\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this value serializer is deserialized from an old\n    // version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = asKryoRegistrations(type);\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "org.apache.flink.api.common.typeutils.GenericTypeSerializerSnapshot",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.io.IOReadableWritable",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.InstantiationUtil",
        "org.apache.flink.util.Preconditions",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class ValueSerializercheckKryoInitialized_DefaultInstantiatorStrategysetFallbackInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines(org.apache.flink.configuration.Configuration, boolean)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines(org.apache.flink.configuration.Configuration, boolean)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)"
      ],
      "methodSources": [
        "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpmethodFikaTest {\n\n    @Test\n    public void testConvertConfigToWritableLines() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)"
      ],
      "methodSources": [
        "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_DumpmethodFikaTest {\n\n    @Test\n    public void testToYAMLString() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.Configuration.toFileWritableMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.Configuration.toFileWritableMap()",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)"
      ],
      "methodSources": [
        "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Creates a new empty configuration.\n */\npublic Configuration() {\n    this.confData = new HashMap<>();\n}",
        "/**\n * Creates a new configuration with the copy of the given configuration.\n *\n * @param other\n * \t\tThe configuration to copy the entries from.\n */\npublic Configuration(Configuration other) {\n    this.confData = new HashMap<>(other.confData);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private static final byte TYPE_STRING = 0;",
        "private static final byte TYPE_INT = 1;",
        "private static final byte TYPE_LONG = 2;",
        "private static final byte TYPE_BOOLEAN = 3;",
        "private static final byte TYPE_FLOAT = 4;",
        "private static final byte TYPE_DOUBLE = 5;",
        "private static final byte TYPE_BYTES = 6;",
        "/**\n * The log object used for debugging.\n */\nprivate static final Logger LOG = LoggerFactory.getLogger(Configuration.class);",
        "/**\n * Stores the concrete key/value pairs of this configuration object.\n *\n * <p>NOTE: This map stores the values that are actually used, and does not include any escaping\n * that is required by the standard YAML syntax.\n */\nprotected final HashMap<String, Object> confData;"
      ],
      "setters": [
        "public void addAll(Configuration other) {\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            this.confData.putAll(other.confData);\n        }\n    }\n}",
        "/**\n * Adds all entries from the given configuration into this configuration. The keys are prepended\n * with the given prefix.\n *\n * @param other\n * \t\tThe configuration whose entries are added to this configuration.\n * @param prefix\n * \t\tThe prefix to prepend.\n */\npublic void addAll(Configuration other, String prefix) {\n    final StringBuilder bld = new StringBuilder();\n    bld.append(prefix);\n    final int pl = bld.length();\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            for (Map.Entry<String, Object> entry : other.confData.entrySet()) {\n                bld.setLength(pl);\n                bld.append(entry.getKey());\n                this.confData.put(bld.toString(), entry.getValue());\n            }\n        }\n    }\n}",
        "/**\n * Adds all entries in this {@code Configuration} to the given {@link Properties}.\n */\npublic void addAllToProperties(Properties props) {\n    synchronized(this.confData) {\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            props.put(entry.getKey(), entry.getValue());\n        }\n    }\n}",
        "private void loggingFallback(FallbackKey fallbackKey, ConfigOption<?> configOption) {\n    if (fallbackKey.isDeprecated()) {\n        LOG.warn(\"Config uses deprecated configuration key '{}' instead of proper key '{}'\", fallbackKey.getKey(), configOption.key());\n    } else {\n        LOG.info(\"Config uses fallback configuration key '{}' instead of key '{}'\", fallbackKey.getKey(), configOption.key());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// Serialization\n// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    synchronized(this.confData) {\n        final int numberOfProperties = in.readInt();\n        for (int i = 0; i < numberOfProperties; i++) {\n            String key = StringValue.readString(in);\n            Object value;\n            byte type = in.readByte();\n            switch (type) {\n                case TYPE_STRING :\n                    value = StringValue.readString(in);\n                    break;\n                case TYPE_INT :\n                    value = in.readInt();\n                    break;\n                case TYPE_LONG :\n                    value = in.readLong();\n                    break;\n                case TYPE_FLOAT :\n                    value = in.readFloat();\n                    break;\n                case TYPE_DOUBLE :\n                    value = in.readDouble();\n                    break;\n                case TYPE_BOOLEAN :\n                    value = in.readBoolean();\n                    break;\n                case TYPE_BYTES :\n                    byte[] bytes = new byte[in.readInt()];\n                    in.readFully(bytes);\n                    value = bytes;\n                    break;\n                default :\n                    throw new IOException(String.format(\"Unrecognized type: %s. This method is deprecated and\" + \" might not work for all supported types.\", type));\n            }\n            this.confData.put(key, value);\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n<T> void setValueInternal(String key, T value, boolean canBePrefixMap) {\n    if (key == null) {\n        throw new NullPointerException(\"Key must not be null.\");\n    }\n    if (value == null) {\n        throw new NullPointerException(\"Value must not be null.\");\n    }\n    synchronized(this.confData) {\n        if (canBePrefixMap) {\n            removePrefixMap(this.confData, key);\n        }\n        this.confData.put(key, value);\n    }\n}",
        "@Override\npublic void write(final DataOutputView out) throws IOException {\n    synchronized(this.confData) {\n        out.writeInt(this.confData.size());\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            String key = entry.getKey();\n            Object val = entry.getValue();\n            StringValue.writeString(key, out);\n            Class<?> clazz = val.getClass();\n            if (clazz == String.class) {\n                out.write(TYPE_STRING);\n                StringValue.writeString(((String) (val)), out);\n            } else if (clazz == Integer.class) {\n                out.write(TYPE_INT);\n                out.writeInt(((Integer) (val)));\n            } else if (clazz == Long.class) {\n                out.write(TYPE_LONG);\n                out.writeLong(((Long) (val)));\n            } else if (clazz == Float.class) {\n                out.write(TYPE_FLOAT);\n                out.writeFloat(((Float) (val)));\n            } else if (clazz == Double.class) {\n                out.write(TYPE_DOUBLE);\n                out.writeDouble(((Double) (val)));\n            } else if (clazz == byte[].class) {\n                out.write(TYPE_BYTES);\n                byte[] bytes = ((byte[]) (val));\n                out.writeInt(bytes.length);\n                out.write(bytes);\n            } else if (clazz == Boolean.class) {\n                out.write(TYPE_BOOLEAN);\n                out.writeBoolean(((Boolean) (val)));\n            } else {\n                throw new IllegalArgumentException(\"Unrecognized type. This method is deprecated and might not work\" + \" for all supported types.\");\n            }\n        }\n    }\n}"
      ],
      "imports": [
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.ExecutionConfig.GlobalJobParameters",
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.types.StringValue",
        "org.apache.flink.util.CollectionUtil",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class Configuration_YamlParserUtilsmethod_DumpmethodFikaTest {\n\n    @Test\n    public void testToFileWritableMap() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines(org.apache.flink.configuration.Configuration, boolean)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines(org.apache.flink.configuration.Configuration, boolean)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_LoadSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testConvertConfigToWritableLines() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_LoadSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testToYAMLString() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.Configuration.toFileWritableMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.Configuration.toFileWritableMap()",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Creates a new empty configuration.\n */\npublic Configuration() {\n    this.confData = new HashMap<>();\n}",
        "/**\n * Creates a new configuration with the copy of the given configuration.\n *\n * @param other\n * \t\tThe configuration to copy the entries from.\n */\npublic Configuration(Configuration other) {\n    this.confData = new HashMap<>(other.confData);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private static final byte TYPE_STRING = 0;",
        "private static final byte TYPE_INT = 1;",
        "private static final byte TYPE_LONG = 2;",
        "private static final byte TYPE_BOOLEAN = 3;",
        "private static final byte TYPE_FLOAT = 4;",
        "private static final byte TYPE_DOUBLE = 5;",
        "private static final byte TYPE_BYTES = 6;",
        "/**\n * The log object used for debugging.\n */\nprivate static final Logger LOG = LoggerFactory.getLogger(Configuration.class);",
        "/**\n * Stores the concrete key/value pairs of this configuration object.\n *\n * <p>NOTE: This map stores the values that are actually used, and does not include any escaping\n * that is required by the standard YAML syntax.\n */\nprotected final HashMap<String, Object> confData;"
      ],
      "setters": [
        "public void addAll(Configuration other) {\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            this.confData.putAll(other.confData);\n        }\n    }\n}",
        "/**\n * Adds all entries from the given configuration into this configuration. The keys are prepended\n * with the given prefix.\n *\n * @param other\n * \t\tThe configuration whose entries are added to this configuration.\n * @param prefix\n * \t\tThe prefix to prepend.\n */\npublic void addAll(Configuration other, String prefix) {\n    final StringBuilder bld = new StringBuilder();\n    bld.append(prefix);\n    final int pl = bld.length();\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            for (Map.Entry<String, Object> entry : other.confData.entrySet()) {\n                bld.setLength(pl);\n                bld.append(entry.getKey());\n                this.confData.put(bld.toString(), entry.getValue());\n            }\n        }\n    }\n}",
        "/**\n * Adds all entries in this {@code Configuration} to the given {@link Properties}.\n */\npublic void addAllToProperties(Properties props) {\n    synchronized(this.confData) {\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            props.put(entry.getKey(), entry.getValue());\n        }\n    }\n}",
        "private void loggingFallback(FallbackKey fallbackKey, ConfigOption<?> configOption) {\n    if (fallbackKey.isDeprecated()) {\n        LOG.warn(\"Config uses deprecated configuration key '{}' instead of proper key '{}'\", fallbackKey.getKey(), configOption.key());\n    } else {\n        LOG.info(\"Config uses fallback configuration key '{}' instead of key '{}'\", fallbackKey.getKey(), configOption.key());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// Serialization\n// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    synchronized(this.confData) {\n        final int numberOfProperties = in.readInt();\n        for (int i = 0; i < numberOfProperties; i++) {\n            String key = StringValue.readString(in);\n            Object value;\n            byte type = in.readByte();\n            switch (type) {\n                case TYPE_STRING :\n                    value = StringValue.readString(in);\n                    break;\n                case TYPE_INT :\n                    value = in.readInt();\n                    break;\n                case TYPE_LONG :\n                    value = in.readLong();\n                    break;\n                case TYPE_FLOAT :\n                    value = in.readFloat();\n                    break;\n                case TYPE_DOUBLE :\n                    value = in.readDouble();\n                    break;\n                case TYPE_BOOLEAN :\n                    value = in.readBoolean();\n                    break;\n                case TYPE_BYTES :\n                    byte[] bytes = new byte[in.readInt()];\n                    in.readFully(bytes);\n                    value = bytes;\n                    break;\n                default :\n                    throw new IOException(String.format(\"Unrecognized type: %s. This method is deprecated and\" + \" might not work for all supported types.\", type));\n            }\n            this.confData.put(key, value);\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n<T> void setValueInternal(String key, T value, boolean canBePrefixMap) {\n    if (key == null) {\n        throw new NullPointerException(\"Key must not be null.\");\n    }\n    if (value == null) {\n        throw new NullPointerException(\"Value must not be null.\");\n    }\n    synchronized(this.confData) {\n        if (canBePrefixMap) {\n            removePrefixMap(this.confData, key);\n        }\n        this.confData.put(key, value);\n    }\n}",
        "@Override\npublic void write(final DataOutputView out) throws IOException {\n    synchronized(this.confData) {\n        out.writeInt(this.confData.size());\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            String key = entry.getKey();\n            Object val = entry.getValue();\n            StringValue.writeString(key, out);\n            Class<?> clazz = val.getClass();\n            if (clazz == String.class) {\n                out.write(TYPE_STRING);\n                StringValue.writeString(((String) (val)), out);\n            } else if (clazz == Integer.class) {\n                out.write(TYPE_INT);\n                out.writeInt(((Integer) (val)));\n            } else if (clazz == Long.class) {\n                out.write(TYPE_LONG);\n                out.writeLong(((Long) (val)));\n            } else if (clazz == Float.class) {\n                out.write(TYPE_FLOAT);\n                out.writeFloat(((Float) (val)));\n            } else if (clazz == Double.class) {\n                out.write(TYPE_DOUBLE);\n                out.writeDouble(((Double) (val)));\n            } else if (clazz == byte[].class) {\n                out.write(TYPE_BYTES);\n                byte[] bytes = ((byte[]) (val));\n                out.writeInt(bytes.length);\n                out.write(bytes);\n            } else if (clazz == Boolean.class) {\n                out.write(TYPE_BOOLEAN);\n                out.writeBoolean(((Boolean) (val)));\n            } else {\n                throw new IllegalArgumentException(\"Unrecognized type. This method is deprecated and might not work\" + \" for all supported types.\");\n            }\n        }\n    }\n}"
      ],
      "imports": [
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.ExecutionConfig.GlobalJobParameters",
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.types.StringValue",
        "org.apache.flink.util.CollectionUtil",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class Configuration_YamlParserUtilsmethod_LoadSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testToFileWritableMap() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference(org.apache.flink.types.Value)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.register(java.lang.Class)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.ValueComparator.checkKryoInitialized()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference(org.apache.flink.types.Value)",
        "org.apache.flink.api.java.typeutils.runtime.ValueComparator.checkKryoInitialized()",
        "com.esotericsoftware.kryo.Kryo.register(java.lang.Class)"
      ],
      "methodSources": [
        "@Override\npublic void setReference(T toCompare) // PATH: Test should invoke the next ValueComparator.checkKryoInitialized(...) [step in execution path]\n{\n    checkKryoInitialized();\n    reference = KryoUtils.copy(toCompare, kryo, new ValueSerializer<T>(type));\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) // PATH: Test should invoke the next Kryo.register(...) [step in execution path]\n    {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy instantiatorStrategy = new DefaultInstantiatorStrategy();\n        instantiatorStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(instantiatorStrategy);\n        this.kryo.register(type);\n    }\n}"
      ],
      "constructors": [
        "public ValueComparator(boolean ascending, Class<T> type) {\n    this.type = type;\n    this.ascendingComparison = ascending;\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private final Class<T> type;",
        "private final boolean ascendingComparison;",
        "private transient T reference;",
        "private transient T tempReference;",
        "private transient Kryo kryo;",
        "@SuppressWarnings(\"rawtypes\")\nprivate final TypeComparator[] comparators = new TypeComparator[]{ this };"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy instantiatorStrategy = new DefaultInstantiatorStrategy();\n        instantiatorStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(instantiatorStrategy);\n        this.kryo.register(type);\n    }\n}",
        "@Override\npublic void setReference(T toCompare) {\n    checkKryoInitialized();\n    reference = KryoUtils.copy(toCompare, kryo, new ValueSerializer<T>(type));\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.Registration",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.io.IOReadableWritable",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.types.NormalizableKey",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.InstantiationUtil",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class ValueComparatorcheckKryoInitialized_KryoregisterFikaTest {\n\n    @Test\n    public void testSetReference() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy(org.apache.flink.types.Value)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy(org.apache.flink.types.Value)",
        "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized()",
        "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@Override\npublic T copy(T from) // PATH: Test should invoke the next ValueSerializer.checkKryoInitialized(...) [step in execution path]\n{\n    checkKryoInitialized();\n    return KryoUtils.copy(from, kryo, this);\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) // PATH: Test should invoke the next Kryo.setInstantiatorStrategy(...) [step in execution path]\n    {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic ValueSerializer(Class<T> type) {\n    this.type = checkNotNull(type);\n    this.kryoRegistrations = asKryoRegistrations(type);\n}",
        "@SuppressWarnings(\"unused\")\npublic ValueSerializerSnapshot() {\n}",
        "ValueSerializerSnapshot(Class<T> typeClass) {\n    super(typeClass);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private final Class<T> type;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n *\n * <p>Currently, we only have one single registration for the value type. Nevertheless, we keep\n * this information here for future compatibility.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private transient Kryo kryo;",
        "private transient T copyInstance;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (this.copyInstance == null) {\n        this.copyInstance = InstantiationUtil.instantiate(type);\n    }\n    this.copyInstance.read(source);\n    this.copyInstance.write(target);\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this value serializer is deserialized from an old\n    // version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = asKryoRegistrations(type);\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "org.apache.flink.api.common.typeutils.GenericTypeSerializerSnapshot",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.io.IOReadableWritable",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.InstantiationUtil",
        "org.apache.flink.util.Preconditions",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class ValueSerializercheckKryoInitialized_KryosetInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines(org.apache.flink.configuration.Configuration, boolean)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines(org.apache.flink.configuration.Configuration, boolean)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)"
      ],
      "methodSources": [
        "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_LoadmethodFikaTest {\n\n    @Test\n    public void testConvertConfigToWritableLines() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)"
      ],
      "methodSources": [
        "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_LoadmethodFikaTest {\n\n    @Test\n    public void testToYAMLString() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.Configuration.toFileWritableMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.Configuration.toFileWritableMap()",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)"
      ],
      "methodSources": [
        "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Creates a new empty configuration.\n */\npublic Configuration() {\n    this.confData = new HashMap<>();\n}",
        "/**\n * Creates a new configuration with the copy of the given configuration.\n *\n * @param other\n * \t\tThe configuration to copy the entries from.\n */\npublic Configuration(Configuration other) {\n    this.confData = new HashMap<>(other.confData);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private static final byte TYPE_STRING = 0;",
        "private static final byte TYPE_INT = 1;",
        "private static final byte TYPE_LONG = 2;",
        "private static final byte TYPE_BOOLEAN = 3;",
        "private static final byte TYPE_FLOAT = 4;",
        "private static final byte TYPE_DOUBLE = 5;",
        "private static final byte TYPE_BYTES = 6;",
        "/**\n * The log object used for debugging.\n */\nprivate static final Logger LOG = LoggerFactory.getLogger(Configuration.class);",
        "/**\n * Stores the concrete key/value pairs of this configuration object.\n *\n * <p>NOTE: This map stores the values that are actually used, and does not include any escaping\n * that is required by the standard YAML syntax.\n */\nprotected final HashMap<String, Object> confData;"
      ],
      "setters": [
        "public void addAll(Configuration other) {\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            this.confData.putAll(other.confData);\n        }\n    }\n}",
        "/**\n * Adds all entries from the given configuration into this configuration. The keys are prepended\n * with the given prefix.\n *\n * @param other\n * \t\tThe configuration whose entries are added to this configuration.\n * @param prefix\n * \t\tThe prefix to prepend.\n */\npublic void addAll(Configuration other, String prefix) {\n    final StringBuilder bld = new StringBuilder();\n    bld.append(prefix);\n    final int pl = bld.length();\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            for (Map.Entry<String, Object> entry : other.confData.entrySet()) {\n                bld.setLength(pl);\n                bld.append(entry.getKey());\n                this.confData.put(bld.toString(), entry.getValue());\n            }\n        }\n    }\n}",
        "/**\n * Adds all entries in this {@code Configuration} to the given {@link Properties}.\n */\npublic void addAllToProperties(Properties props) {\n    synchronized(this.confData) {\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            props.put(entry.getKey(), entry.getValue());\n        }\n    }\n}",
        "private void loggingFallback(FallbackKey fallbackKey, ConfigOption<?> configOption) {\n    if (fallbackKey.isDeprecated()) {\n        LOG.warn(\"Config uses deprecated configuration key '{}' instead of proper key '{}'\", fallbackKey.getKey(), configOption.key());\n    } else {\n        LOG.info(\"Config uses fallback configuration key '{}' instead of key '{}'\", fallbackKey.getKey(), configOption.key());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// Serialization\n// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    synchronized(this.confData) {\n        final int numberOfProperties = in.readInt();\n        for (int i = 0; i < numberOfProperties; i++) {\n            String key = StringValue.readString(in);\n            Object value;\n            byte type = in.readByte();\n            switch (type) {\n                case TYPE_STRING :\n                    value = StringValue.readString(in);\n                    break;\n                case TYPE_INT :\n                    value = in.readInt();\n                    break;\n                case TYPE_LONG :\n                    value = in.readLong();\n                    break;\n                case TYPE_FLOAT :\n                    value = in.readFloat();\n                    break;\n                case TYPE_DOUBLE :\n                    value = in.readDouble();\n                    break;\n                case TYPE_BOOLEAN :\n                    value = in.readBoolean();\n                    break;\n                case TYPE_BYTES :\n                    byte[] bytes = new byte[in.readInt()];\n                    in.readFully(bytes);\n                    value = bytes;\n                    break;\n                default :\n                    throw new IOException(String.format(\"Unrecognized type: %s. This method is deprecated and\" + \" might not work for all supported types.\", type));\n            }\n            this.confData.put(key, value);\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n<T> void setValueInternal(String key, T value, boolean canBePrefixMap) {\n    if (key == null) {\n        throw new NullPointerException(\"Key must not be null.\");\n    }\n    if (value == null) {\n        throw new NullPointerException(\"Value must not be null.\");\n    }\n    synchronized(this.confData) {\n        if (canBePrefixMap) {\n            removePrefixMap(this.confData, key);\n        }\n        this.confData.put(key, value);\n    }\n}",
        "@Override\npublic void write(final DataOutputView out) throws IOException {\n    synchronized(this.confData) {\n        out.writeInt(this.confData.size());\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            String key = entry.getKey();\n            Object val = entry.getValue();\n            StringValue.writeString(key, out);\n            Class<?> clazz = val.getClass();\n            if (clazz == String.class) {\n                out.write(TYPE_STRING);\n                StringValue.writeString(((String) (val)), out);\n            } else if (clazz == Integer.class) {\n                out.write(TYPE_INT);\n                out.writeInt(((Integer) (val)));\n            } else if (clazz == Long.class) {\n                out.write(TYPE_LONG);\n                out.writeLong(((Long) (val)));\n            } else if (clazz == Float.class) {\n                out.write(TYPE_FLOAT);\n                out.writeFloat(((Float) (val)));\n            } else if (clazz == Double.class) {\n                out.write(TYPE_DOUBLE);\n                out.writeDouble(((Double) (val)));\n            } else if (clazz == byte[].class) {\n                out.write(TYPE_BYTES);\n                byte[] bytes = ((byte[]) (val));\n                out.writeInt(bytes.length);\n                out.write(bytes);\n            } else if (clazz == Boolean.class) {\n                out.write(TYPE_BOOLEAN);\n                out.writeBoolean(((Boolean) (val)));\n            } else {\n                throw new IllegalArgumentException(\"Unrecognized type. This method is deprecated and might not work\" + \" for all supported types.\");\n            }\n        }\n    }\n}"
      ],
      "imports": [
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.ExecutionConfig.GlobalJobParameters",
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.types.StringValue",
        "org.apache.flink.util.CollectionUtil",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class Configuration_YamlParserUtilsmethod_LoadmethodFikaTest {\n\n    @Test\n    public void testToFileWritableMap() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)"
      ],
      "methodSources": [
        "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_DumpSettingsBuildersetDefaultFlowStyleFikaTest {\n\n    @Test\n    public void testConvertAndDumpYamlFromFlatMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap()",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)"
      ],
      "methodSources": [
        "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Default constructor for serialization. Creates an empty delegating configuration.\n */\npublic DelegatingConfiguration() {\n    this(new Configuration(), \"\");\n}",
        "/**\n * Creates a new delegating configuration which stores its key/value pairs in the given\n * configuration using the specifies key prefix.\n *\n * @param backingConfig\n * \t\tThe configuration holding the actual config data.\n * @param prefix\n * \t\tThe prefix prepended to all config keys.\n */\npublic DelegatingConfiguration(Configuration backingConfig, String prefix) {\n    this.backingConfig = Preconditions.checkNotNull(backingConfig);\n    this.prefix = Preconditions.checkNotNull(prefix, \"The 'prefix' attribute mustn't be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private final Configuration backingConfig;// the configuration actually storing the data\n",
        "@Nonnull\nprivate String prefix;// the prefix key by which keys for this config are marked\n"
      ],
      "setters": [
        "@Override\npublic void addAll(Configuration other, String prefix) {\n    this.backingConfig.addAll(other, this.prefix + prefix);\n}",
        "@Override\npublic void addAllToProperties(Properties props) {\n    // only add keys with our prefix\n    synchronized(backingConfig.confData) {\n        for (Map.Entry<String, Object> entry : backingConfig.confData.entrySet()) {\n            if (entry.getKey().startsWith(prefix)) {\n                String keyWithoutPrefix = entry.getKey().substring(prefix.length(), entry.getKey().length());\n                props.put(keyWithoutPrefix, entry.getValue());\n            } else {\n                // don't add stuff that doesn't have our prefix\n            }\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    this.prefix = Preconditions.checkNotNull(in.readUTF());\n    this.backingConfig.read(in);\n}",
        "@Override\npublic void setBytes(final String key, final byte[] bytes) {\n    this.backingConfig.setBytes(this.prefix + key, bytes);\n}",
        "@Override\npublic void setString(String key, String value) {\n    this.backingConfig.setString(this.prefix + key, value);\n}",
        "@Override\npublic void write(DataOutputView out) throws IOException {\n    out.writeUTF(this.prefix);\n    this.backingConfig.write(out);\n}"
      ],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.Preconditions",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class DelegatingConfiguration_YamlParserUtilsmethod_DumpSettingsBuildersetDefaultFlowStyleFikaTest {\n\n    @Test\n    public void testToFileWritableMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)"
      ],
      "methodSources": [
        "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_DumpSettingsBuildersetDefaultFlowStyleFikaTest {\n\n    @Test\n    public void testLoadYamlFile() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertToList(java.lang.Object, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertToList(java.lang.Object, java.lang.Class)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpSettingsBuildersetDefaultFlowStyleFikaTest {\n\n    @Test\n    public void testConvertToList() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_DumpSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testConvertAndDumpYamlFromFlatMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap()",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Default constructor for serialization. Creates an empty delegating configuration.\n */\npublic DelegatingConfiguration() {\n    this(new Configuration(), \"\");\n}",
        "/**\n * Creates a new delegating configuration which stores its key/value pairs in the given\n * configuration using the specifies key prefix.\n *\n * @param backingConfig\n * \t\tThe configuration holding the actual config data.\n * @param prefix\n * \t\tThe prefix prepended to all config keys.\n */\npublic DelegatingConfiguration(Configuration backingConfig, String prefix) {\n    this.backingConfig = Preconditions.checkNotNull(backingConfig);\n    this.prefix = Preconditions.checkNotNull(prefix, \"The 'prefix' attribute mustn't be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private final Configuration backingConfig;// the configuration actually storing the data\n",
        "@Nonnull\nprivate String prefix;// the prefix key by which keys for this config are marked\n"
      ],
      "setters": [
        "@Override\npublic void addAll(Configuration other, String prefix) {\n    this.backingConfig.addAll(other, this.prefix + prefix);\n}",
        "@Override\npublic void addAllToProperties(Properties props) {\n    // only add keys with our prefix\n    synchronized(backingConfig.confData) {\n        for (Map.Entry<String, Object> entry : backingConfig.confData.entrySet()) {\n            if (entry.getKey().startsWith(prefix)) {\n                String keyWithoutPrefix = entry.getKey().substring(prefix.length(), entry.getKey().length());\n                props.put(keyWithoutPrefix, entry.getValue());\n            } else {\n                // don't add stuff that doesn't have our prefix\n            }\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    this.prefix = Preconditions.checkNotNull(in.readUTF());\n    this.backingConfig.read(in);\n}",
        "@Override\npublic void setBytes(final String key, final byte[] bytes) {\n    this.backingConfig.setBytes(this.prefix + key, bytes);\n}",
        "@Override\npublic void setString(String key, String value) {\n    this.backingConfig.setString(this.prefix + key, value);\n}",
        "@Override\npublic void write(DataOutputView out) throws IOException {\n    out.writeUTF(this.prefix);\n    this.backingConfig.write(out);\n}"
      ],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.Preconditions",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class DelegatingConfiguration_YamlParserUtilsmethod_DumpSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testToFileWritableMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_DumpSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testLoadYamlFile() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertToList(java.lang.Object, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertToList(java.lang.Object, java.lang.Class)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testConvertToList() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.of(java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.utils.Sets.newHashSet(java.lang.Object[])",
      "directCaller": "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<clinit>()",
      "path": [
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.of(java.lang.String)",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<clinit>()",
        "org.apache.commons.compress.utils.Sets.newHashSet(java.lang.Object[])"
      ],
      "methodSources": [
        "/**\n * Return the corresponding RestartStrategyType based on the displayed value.\n */\npublic static RestartStrategyType of(String value) {\n    for (RestartStrategyType restartStrategyType : RestartStrategyType.values()) {\n        if (restartStrategyType.getAllAvailableValues().contains(value)) {\n            return restartStrategyType;\n        }\n    }\n    throw new IllegalArgumentException(String.format(\"%s is an unknown value of RestartStrategyType.\", value));\n}",
        "org.apache.flink.configuration.RestartStrategyOptions$RestartStrategyType\n\n// Static field initializations\nNO_RESTART_STRATEGY(\"disable\", Sets.newHashSet(\"none\", \"off\"))\nFIXED_DELAY(\"fixed-delay\", Sets.newHashSet(\"fixeddelay\"))\nFAILURE_RATE(\"failure-rate\", Sets.newHashSet(\"failurerate\"))\nEXPONENTIAL_DELAY(\"exponential-delay\", Sets.newHashSet(\"exponentialdelay\"))\n"
      ],
      "constructors": [
        "RestartStrategyType(String mainValue, Set<String> otherAvailableValues) {\n    this.mainValue = mainValue;\n    this.allAvailableValues = Sets.newHashSet(mainValue);\n    allAvailableValues.addAll(otherAvailableValues);\n}",
        "/**\n * Return the corresponding RestartStrategyType based on the displayed value.\n */\npublic static RestartStrategyType of(String value) {\n    for (RestartStrategyType restartStrategyType : RestartStrategyType.values()) {\n        if (restartStrategyType.getAllAvailableValues().contains(value)) {\n            return restartStrategyType;\n        }\n    }\n    throw new IllegalArgumentException(String.format(\"%s is an unknown value of RestartStrategyType.\", value));\n}"
      ],
      "fieldDeclarations": [
        "NO_RESTART_STRATEGY(\"disable\", Sets.newHashSet(\"none\", \"off\"))",
        "FIXED_DELAY(\"fixed-delay\", Sets.newHashSet(\"fixeddelay\"))",
        "FAILURE_RATE(\"failure-rate\", Sets.newHashSet(\"failurerate\"))",
        "EXPONENTIAL_DELAY(\"exponential-delay\", Sets.newHashSet(\"exponentialdelay\"))",
        "private final String mainValue;",
        "private final Set<String> allAvailableValues;"
      ],
      "setters": [],
      "imports": [
        "org.apache.commons.compress.utils.Sets",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType",
        "org.apache.flink.configuration.description.TextElement"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class RestartStrategyTypemethod_SetsnewHashSetFikaTest {\n\n    @Test\n    public void testOf() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy(org.apache.flink.types.Value, org.apache.flink.types.Value)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy(org.apache.flink.types.Value, org.apache.flink.types.Value)",
        "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized()",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (this.copyInstance == null) {\n        this.copyInstance = InstantiationUtil.instantiate(type);\n    }\n    this.copyInstance.read(source);\n    this.copyInstance.write(target);\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) // PATH: Test should invoke the next DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(...) [step in execution path]\n    {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic ValueSerializer(Class<T> type) {\n    this.type = checkNotNull(type);\n    this.kryoRegistrations = asKryoRegistrations(type);\n}",
        "@SuppressWarnings(\"unused\")\npublic ValueSerializerSnapshot() {\n}",
        "ValueSerializerSnapshot(Class<T> typeClass) {\n    super(typeClass);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private final Class<T> type;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n *\n * <p>Currently, we only have one single registration for the value type. Nevertheless, we keep\n * this information here for future compatibility.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private transient Kryo kryo;",
        "private transient T copyInstance;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (this.copyInstance == null) {\n        this.copyInstance = InstantiationUtil.instantiate(type);\n    }\n    this.copyInstance.read(source);\n    this.copyInstance.write(target);\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this value serializer is deserialized from an old\n    // version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = asKryoRegistrations(type);\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "org.apache.flink.api.common.typeutils.GenericTypeSerializerSnapshot",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.io.IOReadableWritable",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.InstantiationUtil",
        "org.apache.flink.util.Preconditions",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class ValueSerializercheckKryoInitialized_DefaultInstantiatorStrategysetFallbackInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)"
      ],
      "methodSources": [
        "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_DumpmethodFikaTest {\n\n    @Test\n    public void testConvertAndDumpYamlFromFlatMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap()",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)"
      ],
      "methodSources": [
        "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Default constructor for serialization. Creates an empty delegating configuration.\n */\npublic DelegatingConfiguration() {\n    this(new Configuration(), \"\");\n}",
        "/**\n * Creates a new delegating configuration which stores its key/value pairs in the given\n * configuration using the specifies key prefix.\n *\n * @param backingConfig\n * \t\tThe configuration holding the actual config data.\n * @param prefix\n * \t\tThe prefix prepended to all config keys.\n */\npublic DelegatingConfiguration(Configuration backingConfig, String prefix) {\n    this.backingConfig = Preconditions.checkNotNull(backingConfig);\n    this.prefix = Preconditions.checkNotNull(prefix, \"The 'prefix' attribute mustn't be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private final Configuration backingConfig;// the configuration actually storing the data\n",
        "@Nonnull\nprivate String prefix;// the prefix key by which keys for this config are marked\n"
      ],
      "setters": [
        "@Override\npublic void addAll(Configuration other, String prefix) {\n    this.backingConfig.addAll(other, this.prefix + prefix);\n}",
        "@Override\npublic void addAllToProperties(Properties props) {\n    // only add keys with our prefix\n    synchronized(backingConfig.confData) {\n        for (Map.Entry<String, Object> entry : backingConfig.confData.entrySet()) {\n            if (entry.getKey().startsWith(prefix)) {\n                String keyWithoutPrefix = entry.getKey().substring(prefix.length(), entry.getKey().length());\n                props.put(keyWithoutPrefix, entry.getValue());\n            } else {\n                // don't add stuff that doesn't have our prefix\n            }\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    this.prefix = Preconditions.checkNotNull(in.readUTF());\n    this.backingConfig.read(in);\n}",
        "@Override\npublic void setBytes(final String key, final byte[] bytes) {\n    this.backingConfig.setBytes(this.prefix + key, bytes);\n}",
        "@Override\npublic void setString(String key, String value) {\n    this.backingConfig.setString(this.prefix + key, value);\n}",
        "@Override\npublic void write(DataOutputView out) throws IOException {\n    out.writeUTF(this.prefix);\n    this.backingConfig.write(out);\n}"
      ],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.Preconditions",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class DelegatingConfiguration_YamlParserUtilsmethod_DumpmethodFikaTest {\n\n    @Test\n    public void testToFileWritableMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)"
      ],
      "methodSources": [
        "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_DumpmethodFikaTest {\n\n    @Test\n    public void testLoadYamlFile() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertToList(java.lang.Object, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertToList(java.lang.Object, java.lang.Class)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpmethodFikaTest {\n\n    @Test\n    public void testConvertToList() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_LoadSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testConvertAndDumpYamlFromFlatMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap()",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Default constructor for serialization. Creates an empty delegating configuration.\n */\npublic DelegatingConfiguration() {\n    this(new Configuration(), \"\");\n}",
        "/**\n * Creates a new delegating configuration which stores its key/value pairs in the given\n * configuration using the specifies key prefix.\n *\n * @param backingConfig\n * \t\tThe configuration holding the actual config data.\n * @param prefix\n * \t\tThe prefix prepended to all config keys.\n */\npublic DelegatingConfiguration(Configuration backingConfig, String prefix) {\n    this.backingConfig = Preconditions.checkNotNull(backingConfig);\n    this.prefix = Preconditions.checkNotNull(prefix, \"The 'prefix' attribute mustn't be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private final Configuration backingConfig;// the configuration actually storing the data\n",
        "@Nonnull\nprivate String prefix;// the prefix key by which keys for this config are marked\n"
      ],
      "setters": [
        "@Override\npublic void addAll(Configuration other, String prefix) {\n    this.backingConfig.addAll(other, this.prefix + prefix);\n}",
        "@Override\npublic void addAllToProperties(Properties props) {\n    // only add keys with our prefix\n    synchronized(backingConfig.confData) {\n        for (Map.Entry<String, Object> entry : backingConfig.confData.entrySet()) {\n            if (entry.getKey().startsWith(prefix)) {\n                String keyWithoutPrefix = entry.getKey().substring(prefix.length(), entry.getKey().length());\n                props.put(keyWithoutPrefix, entry.getValue());\n            } else {\n                // don't add stuff that doesn't have our prefix\n            }\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    this.prefix = Preconditions.checkNotNull(in.readUTF());\n    this.backingConfig.read(in);\n}",
        "@Override\npublic void setBytes(final String key, final byte[] bytes) {\n    this.backingConfig.setBytes(this.prefix + key, bytes);\n}",
        "@Override\npublic void setString(String key, String value) {\n    this.backingConfig.setString(this.prefix + key, value);\n}",
        "@Override\npublic void write(DataOutputView out) throws IOException {\n    out.writeUTF(this.prefix);\n    this.backingConfig.write(out);\n}"
      ],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.Preconditions",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class DelegatingConfiguration_YamlParserUtilsmethod_LoadSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testToFileWritableMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_LoadSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testLoadYamlFile() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertToList(java.lang.Object, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertToList(java.lang.Object, java.lang.Class)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_LoadSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testConvertToList() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy(org.apache.flink.types.Value, org.apache.flink.types.Value)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy(org.apache.flink.types.Value, org.apache.flink.types.Value)",
        "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized()",
        "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (this.copyInstance == null) {\n        this.copyInstance = InstantiationUtil.instantiate(type);\n    }\n    this.copyInstance.read(source);\n    this.copyInstance.write(target);\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) // PATH: Test should invoke the next Kryo.setInstantiatorStrategy(...) [step in execution path]\n    {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic ValueSerializer(Class<T> type) {\n    this.type = checkNotNull(type);\n    this.kryoRegistrations = asKryoRegistrations(type);\n}",
        "@SuppressWarnings(\"unused\")\npublic ValueSerializerSnapshot() {\n}",
        "ValueSerializerSnapshot(Class<T> typeClass) {\n    super(typeClass);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private final Class<T> type;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n *\n * <p>Currently, we only have one single registration for the value type. Nevertheless, we keep\n * this information here for future compatibility.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private transient Kryo kryo;",
        "private transient T copyInstance;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (this.copyInstance == null) {\n        this.copyInstance = InstantiationUtil.instantiate(type);\n    }\n    this.copyInstance.read(source);\n    this.copyInstance.write(target);\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this value serializer is deserialized from an old\n    // version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = asKryoRegistrations(type);\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "org.apache.flink.api.common.typeutils.GenericTypeSerializerSnapshot",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.io.IOReadableWritable",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.InstantiationUtil",
        "org.apache.flink.util.Preconditions",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class ValueSerializercheckKryoInitialized_KryosetInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)"
      ],
      "methodSources": [
        "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_LoadmethodFikaTest {\n\n    @Test\n    public void testConvertAndDumpYamlFromFlatMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap()",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)"
      ],
      "methodSources": [
        "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Default constructor for serialization. Creates an empty delegating configuration.\n */\npublic DelegatingConfiguration() {\n    this(new Configuration(), \"\");\n}",
        "/**\n * Creates a new delegating configuration which stores its key/value pairs in the given\n * configuration using the specifies key prefix.\n *\n * @param backingConfig\n * \t\tThe configuration holding the actual config data.\n * @param prefix\n * \t\tThe prefix prepended to all config keys.\n */\npublic DelegatingConfiguration(Configuration backingConfig, String prefix) {\n    this.backingConfig = Preconditions.checkNotNull(backingConfig);\n    this.prefix = Preconditions.checkNotNull(prefix, \"The 'prefix' attribute mustn't be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private final Configuration backingConfig;// the configuration actually storing the data\n",
        "@Nonnull\nprivate String prefix;// the prefix key by which keys for this config are marked\n"
      ],
      "setters": [
        "@Override\npublic void addAll(Configuration other, String prefix) {\n    this.backingConfig.addAll(other, this.prefix + prefix);\n}",
        "@Override\npublic void addAllToProperties(Properties props) {\n    // only add keys with our prefix\n    synchronized(backingConfig.confData) {\n        for (Map.Entry<String, Object> entry : backingConfig.confData.entrySet()) {\n            if (entry.getKey().startsWith(prefix)) {\n                String keyWithoutPrefix = entry.getKey().substring(prefix.length(), entry.getKey().length());\n                props.put(keyWithoutPrefix, entry.getValue());\n            } else {\n                // don't add stuff that doesn't have our prefix\n            }\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    this.prefix = Preconditions.checkNotNull(in.readUTF());\n    this.backingConfig.read(in);\n}",
        "@Override\npublic void setBytes(final String key, final byte[] bytes) {\n    this.backingConfig.setBytes(this.prefix + key, bytes);\n}",
        "@Override\npublic void setString(String key, String value) {\n    this.backingConfig.setString(this.prefix + key, value);\n}",
        "@Override\npublic void write(DataOutputView out) throws IOException {\n    out.writeUTF(this.prefix);\n    this.backingConfig.write(out);\n}"
      ],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.Preconditions",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class DelegatingConfiguration_YamlParserUtilsmethod_LoadmethodFikaTest {\n\n    @Test\n    public void testToFileWritableMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)"
      ],
      "methodSources": [
        "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilsmethod_LoadmethodFikaTest {\n\n    @Test\n    public void testLoadYamlFile() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertToList(java.lang.Object, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertToList(java.lang.Object, java.lang.Class)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_LoadmethodFikaTest {\n\n    @Test\n    public void testConvertToList() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.util.Utils.getSerializerTree(org.apache.flink.api.common.typeinfo.TypeInformation)",
      "thirdPartyMethod": "org.apache.commons.lang3.StringUtils.repeat(char, int)",
      "directCaller": "org.apache.flink.util.Utils.getSerializerTree(org.apache.flink.api.common.typeinfo.TypeInformation, int)",
      "path": [
        "org.apache.flink.util.Utils.getSerializerTree(org.apache.flink.api.common.typeinfo.TypeInformation)",
        "org.apache.flink.util.Utils.getSerializerTree(org.apache.flink.api.common.typeinfo.TypeInformation, int)",
        "org.apache.commons.lang3.StringUtils.repeat(char, int)"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Debugging utility to understand the hierarchy of serializers created by the Java API. Tested\n * in GroupReduceITCase.testGroupByGenericType()\n */\npublic static <T> String getSerializerTree(TypeInformation<T> ti) {\n    // PATH: Test should invoke the next Utils.getSerializerTree(...) [step in execution path]\n    return Utils.getSerializerTree(ti, 0);\n}",
        "private static <T> String getSerializerTree(TypeInformation<T> ti, int indent) {\n    String ret = \"\";\n    if (ti instanceof CompositeType) {\n        // PATH: Test should invoke the next StringUtils.repeat(...) [step in execution path]\n        ret += (StringUtils.repeat(' ', indent) + ti.getClass().getSimpleName()) + \"\\n\";\n        CompositeType<T> cti = ((CompositeType<T>) (ti));\n        String[] fieldNames = cti.getFieldNames();\n        for (int i = 0; i < cti.getArity(); i++) {\n            TypeInformation<?> fieldType = cti.getTypeAt(i);\n            ret += ((StringUtils.repeat(' ', indent + 2) + fieldNames[i]) + \":\") + Utils.getSerializerTree(fieldType, indent);\n        }\n    } else if (ti instanceof GenericTypeInfo) {\n        ret += ((StringUtils.repeat(' ', indent) + \"GenericTypeInfo (\") + ti.getTypeClass().getSimpleName()) + \")\\n\";\n        ret += getGenericTypeTree(ti.getTypeClass(), indent + 4);\n    } else {\n        ret += (StringUtils.repeat(' ', indent) + ti.toString()) + \"\\n\";\n    }\n    return ret;\n}"
      ],
      "constructors": [
        "public CountHelper(String id) {\n    this.id = id;\n    this.counter = 0L;\n}",
        "public CollectHelper(String id, TypeSerializer<T> serializer) {\n    this.id = id;\n    this.serializer = serializer;\n}",
        "public ChecksumHashCode() {\n}",
        "public ChecksumHashCode(long count, long checksum) {\n    this.count = count;\n    this.checksum = checksum;\n}",
        "public ChecksumHashCodeHelper(String id) {\n    this.id = id;\n    this.counter = 0L;\n    this.checksum = 0L;\n}",
        "/**\n * Private constructor to prevent instantiation.\n */\nprivate Utils() {\n    throw new RuntimeException();\n}"
      ],
      "fieldDeclarations": [
        "public static final Random RNG = new Random();"
      ],
      "setters": [],
      "imports": [
        "org.apache.commons.lang3.StringUtils",
        "org.apache.flink.api.common.io.RichOutputFormat",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.GenericTypeInfo",
        "org.apache.flink.util.Utils.ChecksumHashCode",
        "org.apache.flink.util.Utils.ChecksumHashCodeHelper",
        "org.apache.flink.util.Utils.CollectHelper",
        "org.apache.flink.util.Utils.CountHelper"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class UtilsgetSerializerTree_StringUtilsrepeatFikaTest {\n\n    @Test\n    public void testGetSerializerTree() {\n    }\n}",
      "conditionCount": 3,
      "callCount": 4,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo()",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setClassLoader(java.lang.ClassLoader)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "com.esotericsoftware.kryo.Kryo.setClassLoader(java.lang.ClassLoader)"
      ],
      "methodSources": [
        "@VisibleForTesting\npublic Kryo getKryo() // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n{\n    checkKryoInitialized();\n    return this.kryo;\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) // PATH: Test should invoke the next Kryo.setClassLoader(...) [step in execution path]\n    {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializercheckKryoInitialized_KryosetClassLoaderFikaTest {\n\n    @Test\n    public void testGetKryo() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.Mark.getColumn()",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.Mark.getColumn()"
      ],
      "methodSources": [
        "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n * key1: secret1\n * ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n * key1: secret2\n * ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    // PATH: Test should invoke the next Mark.getColumn(...) [step in execution path]\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = YamlParserUtils.wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_MarkgetColumnFikaTest {\n\n    @Test\n    public void testConvertToObject() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 2,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.YamlEngineException.setStackTrace(java.lang.StackTraceElement[])",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException.setStackTrace(java.lang.StackTraceElement[])"
      ],
      "methodSources": [
        "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_YamlEngineExceptionsetStackTraceFikaTest {\n\n    @Test\n    public void testConvertToObject() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.YamlEngineException.<init>(java.lang.String, java.lang.Throwable)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException.<init>(java.lang.String, java.lang.Throwable)"
      ],
      "methodSources": [
        "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_YamlEngineExceptionmethodFikaTest {\n\n    @Test\n    public void testConvertToObject() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.Mark.getLine()",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertToObject(java.lang.String, java.lang.Class)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.Mark.getLine()"
      ],
      "methodSources": [
        "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n * key1: secret1\n * ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n * key1: secret2\n * ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    // PATH: Test should invoke the next Mark.getLine(...) [step in execution path]\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = YamlParserUtils.wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_MarkgetLineFikaTest {\n\n    @Test\n    public void testConvertToObject() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 2,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.tar.TarArchiveInputStream.close()",
      "directCaller": "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava(java.lang.String, java.lang.String, boolean)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
        "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava(java.lang.String, java.lang.String, boolean)",
        "org.apache.commons.compress.archivers.tar.TarArchiveInputStream.close()"
      ],
      "methodSources": [
        "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else // PATH: Test should invoke the next CompressionUtils.extractTarFileUsingJava(...) [step in execution path]\n    {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}",
        "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.tar.TarArchiveEntry",
        "org.apache.commons.compress.archivers.tar.TarArchiveInputStream",
        "org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsextractTarFileUsingJava_TarArchiveInputStreamcloseFikaTest {\n\n    @Test\n    public void testExtractTarFile() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.tar.TarArchiveInputStream.<init>(java.io.InputStream)",
      "directCaller": "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava(java.lang.String, java.lang.String, boolean)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
        "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava(java.lang.String, java.lang.String, boolean)",
        "org.apache.commons.compress.archivers.tar.TarArchiveInputStream.<init>(java.io.InputStream)"
      ],
      "methodSources": [
        "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else // PATH: Test should invoke the next CompressionUtils.extractTarFileUsingJava(...) [step in execution path]\n    {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}",
        "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while (// PATH: Test should invoke the next new TarArchiveInputStream(...) [step in execution path]\n        (entry = tai.getNextTarEntry()) != null) {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.tar.TarArchiveEntry",
        "org.apache.commons.compress.archivers.tar.TarArchiveInputStream",
        "org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsextractTarFileUsingJava_TarArchiveInputStreammethodFikaTest {\n\n    @Test\n    public void testExtractTarFile() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry()",
      "directCaller": "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava(java.lang.String, java.lang.String, boolean)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
        "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava(java.lang.String, java.lang.String, boolean)",
        "org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry()"
      ],
      "methodSources": [
        "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else // PATH: Test should invoke the next CompressionUtils.extractTarFileUsingJava(...) [step in execution path]\n    {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}",
        "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while (// PATH: Test should invoke the next TarArchiveInputStream.getNextTarEntry(...) [step in execution path]\n        (entry = tai.getNextTarEntry()) != null) {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.tar.TarArchiveEntry",
        "org.apache.commons.compress.archivers.tar.TarArchiveInputStream",
        "org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsextractTarFileUsingJava_TarArchiveInputStreamgetNextTarEntryFikaTest {\n\n    @Test\n    public void testExtractTarFile() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.<init>(java.io.InputStream)",
      "directCaller": "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava(java.lang.String, java.lang.String, boolean)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
        "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava(java.lang.String, java.lang.String, boolean)",
        "org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.<init>(java.io.InputStream)"
      ],
      "methodSources": [
        "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else // PATH: Test should invoke the next CompressionUtils.extractTarFileUsingJava(...) [step in execution path]\n    {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}",
        "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.tar.TarArchiveEntry",
        "org.apache.commons.compress.archivers.tar.TarArchiveInputStream",
        "org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsextractTarFileUsingJava_GzipCompressorInputStreammethodFikaTest {\n\n    @Test\n    public void testExtractTarFile() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setClassLoader(java.lang.ClassLoader)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "com.esotericsoftware.kryo.Kryo.setClassLoader(java.lang.ClassLoader)"
      ],
      "methodSources": [
        "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) // PATH: Test should invoke the next Kryo.setClassLoader(...) [step in execution path]\n    {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializercheckKryoInitialized_KryosetClassLoaderFikaTest {\n\n    @Test\n    public void testCreateInstance() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.Mark.getColumn()",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.Mark.getColumn()"
      ],
      "methodSources": [
        "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n * key1: secret1\n * ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n * key1: secret2\n * ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    // PATH: Test should invoke the next Mark.getColumn(...) [step in execution path]\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = YamlParserUtils.wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_MarkgetColumnFikaTest {\n\n    @Test\n    public void testToYAMLString() {\n    }\n}",
      "conditionCount": 6,
      "callCount": 2,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.YamlEngineException.setStackTrace(java.lang.StackTraceElement[])",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException.setStackTrace(java.lang.StackTraceElement[])"
      ],
      "methodSources": [
        "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_YamlEngineExceptionsetStackTraceFikaTest {\n\n    @Test\n    public void testToYAMLString() {\n    }\n}",
      "conditionCount": 6,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.YamlEngineException.<init>(java.lang.String, java.lang.Throwable)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException.<init>(java.lang.String, java.lang.Throwable)"
      ],
      "methodSources": [
        "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_YamlEngineExceptionmethodFikaTest {\n\n    @Test\n    public void testToYAMLString() {\n    }\n}",
      "conditionCount": 6,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.Mark.getLine()",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.toYAMLString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.Mark.getLine()"
      ],
      "methodSources": [
        "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n * key1: secret1\n * ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n * key1: secret2\n * ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    // PATH: Test should invoke the next Mark.getLine(...) [step in execution path]\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = YamlParserUtils.wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_MarkgetLineFikaTest {\n\n    @Test\n    public void testToYAMLString() {\n    }\n}",
      "conditionCount": 6,
      "callCount": 2,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.Mark.getColumn()",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.Mark.getColumn()"
      ],
      "methodSources": [
        "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n * key1: secret1\n * ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n * key1: secret2\n * ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    // PATH: Test should invoke the next Mark.getColumn(...) [step in execution path]\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = YamlParserUtils.wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_MarkgetColumnFikaTest {\n\n    @Test\n    public void testConvertAndDumpYamlFromFlatMap() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 2,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.Mark.getColumn()",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.Mark.getColumn()"
      ],
      "methodSources": [
        "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n * key1: secret1\n * ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n * key1: secret2\n * ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    // PATH: Test should invoke the next Mark.getColumn(...) [step in execution path]\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = YamlParserUtils.wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_MarkgetColumnFikaTest {\n\n    @Test\n    public void testLoadYamlFile() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 2,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.YamlEngineException.setStackTrace(java.lang.StackTraceElement[])",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException.setStackTrace(java.lang.StackTraceElement[])"
      ],
      "methodSources": [
        "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_YamlEngineExceptionsetStackTraceFikaTest {\n\n    @Test\n    public void testConvertAndDumpYamlFromFlatMap() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.YamlEngineException.setStackTrace(java.lang.StackTraceElement[])",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException.setStackTrace(java.lang.StackTraceElement[])"
      ],
      "methodSources": [
        "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_YamlEngineExceptionsetStackTraceFikaTest {\n\n    @Test\n    public void testLoadYamlFile() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.YamlEngineException.<init>(java.lang.String, java.lang.Throwable)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException.<init>(java.lang.String, java.lang.Throwable)"
      ],
      "methodSources": [
        "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_YamlEngineExceptionmethodFikaTest {\n\n    @Test\n    public void testConvertAndDumpYamlFromFlatMap() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.YamlEngineException.<init>(java.lang.String, java.lang.Throwable)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException.<init>(java.lang.String, java.lang.Throwable)"
      ],
      "methodSources": [
        "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_YamlEngineExceptionmethodFikaTest {\n\n    @Test\n    public void testLoadYamlFile() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.Mark.getLine()",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap(java.util.Map)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.Mark.getLine()"
      ],
      "methodSources": [
        "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n * key1: secret1\n * ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n * key1: secret2\n * ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    // PATH: Test should invoke the next Mark.getLine(...) [step in execution path]\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = YamlParserUtils.wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_MarkgetLineFikaTest {\n\n    @Test\n    public void testConvertAndDumpYamlFromFlatMap() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 2,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.exceptions.Mark.getLine()",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
      "path": [
        "org.apache.flink.configuration.YamlParserUtils.loadYamlFile(java.io.File)",
        "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData(org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException)",
        "org.snakeyaml.engine.v2.exceptions.Mark.getLine()"
      ],
      "methodSources": [
        "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            // PATH: Test should invoke the next YamlParserUtils.wrapExceptionToHiddenSensitiveData(...) [step in execution path]\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}",
        "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n * key1: secret1\n * ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n * key1: secret2\n * ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    // PATH: Test should invoke the next Mark.getLine(...) [step in execution path]\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = YamlParserUtils.wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}"
      ],
      "constructors": [
        "YamlParserUtils() {\n}",
        "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);",
        "private static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();",
        "private static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));",
        "private static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));",
        "private static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.RepresentToNode",
        "org.snakeyaml.engine.v2.exceptions.Mark",
        "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException",
        "org.snakeyaml.engine.v2.exceptions.YamlEngineException",
        "org.snakeyaml.engine.v2.nodes.Node",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.representer.StandardRepresenter"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class YamlParserUtilswrapExceptionToHiddenSensitiveData_MarkgetLineFikaTest {\n\n    @Test\n    public void testLoadYamlFile() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 2,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.read()",
      "thirdPartyMethod": "com.esotericsoftware.kryo.KryoException.<init>(java.lang.String)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.require(int)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.read()",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.require(int)",
        "com.esotericsoftware.kryo.KryoException.<init>(java.lang.String)"
      ],
      "methodSources": [
        "@Override\npublic int read() throws KryoException // PATH: Test should invoke the next NoFetchingInput.require(...) [step in execution path]\n{\n    require(1);\n    return buffer[position++] & 0xff;\n}",
        "/**\n * Require makes sure that at least required number of bytes are kept in the buffer. If not,\n * then it will load exactly the difference between required and currently available number of\n * bytes. Thus, it will only load the data which is required and never prefetch data.\n *\n * @param required\n * \t\tthe number of bytes being available in the buffer\n * @return The number of bytes remaining in the buffer, which will be at least <code>required\n</code> bytes.\n * @throws KryoException\n */\n@Override\nprotected int require(int required) throws KryoException {\n    // The main change between this and Kryo 5 Input.require is this will never read more bytes\n    // than required.\n    // There are also formatting changes to be compliant with the Flink project styling rules.\n    int remaining = limit - position;\n    if (remaining >= required) {\n        return remaining;\n    }\n    if (required > capacity) {\n        throw new KryoException(((\"Buffer too small: capacity: \" + capacity) + \", required: \") + required);\n    }\n    int count;\n    // Try to fill the buffer.\n    if (remaining > 0) {\n        // Logical change 1 (from Kryo Input.require): \"capacity - limit\" -> \"required - limit\"\n        count = fill(buffer, limit, required - limit);\n        if (count == (-1)) {\n            throw new KryoBufferUnderflowException(\"Buffer underflow.\");\n        }\n        remaining += count;\n        if (remaining >= required) {\n            limit += count;\n            return remaining;\n        }\n    }\n    // Was not enough, compact and try again.\n    System.arraycopy(buffer, position, buffer, 0, remaining);\n    total += position;\n    position = 0;\n    do {\n        // Logical change 2 (from Kryo Input.require): \"capacity - remaining\" -> \"required -\n        // remaining\"\n        count = fill(buffer, remaining, required - remaining);\n        if (count == (-1)) {\n            throw new KryoBufferUnderflowException(\"Buffer underflow.\");\n        }\n        remaining += count;\n    } while (remaining < required );\n    limit = remaining;\n    return remaining;\n}"
      ],
      "constructors": [
        "public NoFetchingInput(InputStream inputStream) {\n    super(inputStream, 8);\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class NoFetchingInputrequire_KryoExceptionmethodFikaTest {\n\n    @Test\n    public void testRead() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setClassLoader(java.lang.ClassLoader)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "com.esotericsoftware.kryo.Kryo.setClassLoader(java.lang.ClassLoader)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) // PATH: Test should invoke the next Kryo.setClassLoader(...) [step in execution path]\n    {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializercheckKryoInitialized_KryosetClassLoaderFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(org.apache.flink.core.memory.DataInputView, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setClassLoader(java.lang.ClassLoader)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(org.apache.flink.core.memory.DataInputView, org.apache.flink.core.memory.DataOutputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "com.esotericsoftware.kryo.Kryo.setClassLoader(java.lang.ClassLoader)"
      ],
      "methodSources": [
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) // PATH: Test should invoke the next Kryo.setClassLoader(...) [step in execution path]\n    {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializercheckKryoInitialized_KryosetClassLoaderFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.read()",
      "thirdPartyMethod": "com.esotericsoftware.kryo.io.KryoBufferUnderflowException.<init>(java.lang.String)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.require(int)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.read()",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.require(int)",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException.<init>(java.lang.String)"
      ],
      "methodSources": [
        "@Override\npublic int read() throws KryoException // PATH: Test should invoke the next NoFetchingInput.require(...) [step in execution path]\n{\n    require(1);\n    return buffer[position++] & 0xff;\n}",
        "/**\n * Require makes sure that at least required number of bytes are kept in the buffer. If not,\n * then it will load exactly the difference between required and currently available number of\n * bytes. Thus, it will only load the data which is required and never prefetch data.\n *\n * @param required\n * \t\tthe number of bytes being available in the buffer\n * @return The number of bytes remaining in the buffer, which will be at least <code>required\n</code> bytes.\n * @throws KryoException\n */\n@Override\nprotected int require(int required) throws KryoException {\n    // The main change between this and Kryo 5 Input.require is this will never read more bytes\n    // than required.\n    // There are also formatting changes to be compliant with the Flink project styling rules.\n    int remaining = limit - position;\n    if (remaining >= required) {\n        return remaining;\n    }\n    if (required > capacity) {\n        throw new KryoException(((\"Buffer too small: capacity: \" + capacity) + \", required: \") + required);\n    }\n    int count;\n    // Try to fill the buffer.\n    if (remaining > 0) {\n        // Logical change 1 (from Kryo Input.require): \"capacity - limit\" -> \"required - limit\"\n        count = fill(buffer, limit, required - limit);\n        if (count == (-1)) {\n            throw new KryoBufferUnderflowException(\"Buffer underflow.\");\n        }\n        remaining += count;\n        if (remaining >= required) {\n            limit += count;\n            return remaining;\n        }\n    }\n    // Was not enough, compact and try again.\n    System.arraycopy(buffer, position, buffer, 0, remaining);\n    total += position;\n    position = 0;\n    do {\n        // Logical change 2 (from Kryo Input.require): \"capacity - remaining\" -> \"required -\n        // remaining\"\n        count = fill(buffer, remaining, required - remaining);\n        if (count == (-1)) {\n            throw new KryoBufferUnderflowException(\"Buffer underflow.\");\n        }\n        remaining += count;\n    } while (remaining < required );\n    limit = remaining;\n    return remaining;\n}"
      ],
      "constructors": [
        "public NoFetchingInput(InputStream inputStream) {\n    super(inputStream, 8);\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime;\n\npublic class NoFetchingInputrequire_KryoBufferUnderflowExceptionmethodFikaTest {\n\n    @Test\n    public void testRead() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setClassLoader(java.lang.ClassLoader)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "com.esotericsoftware.kryo.Kryo.setClassLoader(java.lang.ClassLoader)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) // PATH: Test should invoke the next Kryo.setClassLoader(...) [step in execution path]\n    {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializercheckKryoInitialized_KryosetClassLoaderFikaTest {\n\n    @Test\n    public void testDeserialize() {\n    }\n}",
      "conditionCount": 8,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setClassLoader(java.lang.ClassLoader)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "com.esotericsoftware.kryo.Kryo.setClassLoader(java.lang.ClassLoader)"
      ],
      "methodSources": [
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) // PATH: Test should invoke the next Kryo.setClassLoader(...) [step in execution path]\n    {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializercheckKryoInitialized_KryosetClassLoaderFikaTest {\n\n    @Test\n    public void testSerialize() {\n    }\n}",
      "conditionCount": 9,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.ClosureCleaner.clean(java.lang.Object, org.apache.flink.api.common.ExecutionConfig.ClosureCleanerLevel, boolean)",
      "thirdPartyMethod": "org.apache.commons.lang3.ClassUtils.isPrimitiveOrWrapper(java.lang.Class)",
      "directCaller": "org.apache.flink.api.java.ClosureCleaner.clean(java.lang.Object, org.apache.flink.api.common.ExecutionConfig.ClosureCleanerLevel, boolean, java.util.Set)",
      "path": [
        "org.apache.flink.api.java.ClosureCleaner.clean(java.lang.Object, org.apache.flink.api.common.ExecutionConfig.ClosureCleanerLevel, boolean)",
        "org.apache.flink.api.java.ClosureCleaner.clean(java.lang.Object, org.apache.flink.api.common.ExecutionConfig.ClosureCleanerLevel, boolean, java.util.Set)",
        "org.apache.commons.lang3.ClassUtils.isPrimitiveOrWrapper(java.lang.Class)"
      ],
      "methodSources": [
        "/**\n * Tries to clean the closure of the given object, if the object is a non-static inner class.\n *\n * @param func\n * \t\tThe object whose closure should be cleaned.\n * @param level\n * \t\tthe clean up level.\n * @param checkSerializable\n * \t\tFlag to indicate whether serializability should be checked after the\n * \t\tclosure cleaning attempt.\n * @throws InvalidProgramException\n * \t\tThrown, if 'checkSerializable' is true, and the object was\n * \t\tnot serializable after the closure cleaning.\n * @throws RuntimeException\n * \t\tA RuntimeException may be thrown, if the code of the class could not\n * \t\tbe loaded, in order to process during the closure cleaning.\n */\npublic static void clean(Object func, ExecutionConfig.ClosureCleanerLevel level, boolean checkSerializable) // PATH: Test should invoke the next ClosureCleaner.clean(...) [step in execution path]\n{\n    ClosureCleaner.clean(func, level, checkSerializable, Collections.newSetFromMap(new IdentityHashMap<>()));\n}",
        "private static void clean(Object func, ExecutionConfig.ClosureCleanerLevel level, boolean checkSerializable, Set<Object> visited) {\n    if (func == null) {\n        return;\n    }\n    if (!visited.add(func)) {\n        return;\n    }\n    final Class<?> cls = func.getClass();\n    // PATH: Test should invoke the next ClassUtils.isPrimitiveOrWrapper(...) [step in execution path]\n    if (ClassUtils.isPrimitiveOrWrapper(cls)) {\n        return;\n    }\n    if (usesCustomSerialization(cls)) {\n        return;\n    }\n    if (canBeSerialized(func)) {\n        return;\n    }\n    // serialization failed; try cleaning closure as a fallback\n    // First find the field name of the \"this$0\" field, this can\n    // be \"this$x\" depending on the nesting\n    boolean closureAccessed = false;\n    for (Field f : cls.getDeclaredFields()) {\n        if (f.getName().startsWith(\"this$\")) {\n            // found a closure referencing field - now try to clean\n            closureAccessed |= cleanThis0(func, cls, f.getName());\n        } else {\n            Object fieldObject;\n            try {\n                f.setAccessible(true);\n                fieldObject = f.get(func);\n            } catch (IllegalAccessException e) {\n                throw new RuntimeException(String.format(\"Can not access to the %s field in Class %s\", f.getName(), func.getClass()));\n            }\n            /* we should do a deep clean when we encounter an anonymous class, inner class and local class, but should\n            skip the class with custom serialize method.\n\n            There are five kinds of classes (or interfaces):\n            a) Top level classes\n            b) Nested classes (static member classes)\n            c) Inner classes (non-static member classes)\n            d) Local classes (named classes declared within a method)\n            e) Anonymous classes\n             */\n            if ((level == ExecutionConfig.ClosureCleanerLevel.RECURSIVE) && needsRecursion(f, fieldObject)) {\n                if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"Dig to clean the {}\", fieldObject.getClass().getName());\n                }\n                ClosureCleaner.clean(fieldObject, ExecutionConfig.ClosureCleanerLevel.RECURSIVE, true, visited);\n            }\n        }\n    }\n    if (checkSerializable) {\n        try {\n            InstantiationUtil.serializeObject(func);\n        } catch (Exception e) {\n            String functionType = getSuperClassOrInterfaceName(func.getClass());\n            String msg = (functionType == null) ? func + \" is not serializable.\" : (\"The implementation of the \" + functionType) + \" is not serializable.\";\n            if (closureAccessed) {\n                msg += ((\" The implementation accesses fields of its enclosing class, which is \" + \"a common reason for non-serializability. \") + \"A common solution is to make the function a proper (non-inner) class, or \") + \"a static inner class.\";\n            } else {\n                msg += \" The object probably contains or references non serializable fields.\";\n            }\n            throw new InvalidProgramException(msg, e);\n        }\n    }\n}"
      ],
      "constructors": [
        "ClosureCleaner() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(ClosureCleaner.class);"
      ],
      "setters": [
        "private static void clean(Object func, ExecutionConfig.ClosureCleanerLevel level, boolean checkSerializable, Set<Object> visited) {\n    if (func == null) {\n        return;\n    }\n    if (!visited.add(func)) {\n        return;\n    }\n    final Class<?> cls = func.getClass();\n    if (ClassUtils.isPrimitiveOrWrapper(cls)) {\n        return;\n    }\n    if (usesCustomSerialization(cls)) {\n        return;\n    }\n    if (canBeSerialized(func)) {\n        return;\n    }\n    // serialization failed; try cleaning closure as a fallback\n    // First find the field name of the \"this$0\" field, this can\n    // be \"this$x\" depending on the nesting\n    boolean closureAccessed = false;\n    for (Field f : cls.getDeclaredFields()) {\n        if (f.getName().startsWith(\"this$\")) {\n            // found a closure referencing field - now try to clean\n            closureAccessed |= cleanThis0(func, cls, f.getName());\n        } else {\n            Object fieldObject;\n            try {\n                f.setAccessible(true);\n                fieldObject = f.get(func);\n            } catch (IllegalAccessException e) {\n                throw new RuntimeException(String.format(\"Can not access to the %s field in Class %s\", f.getName(), func.getClass()));\n            }\n            /* we should do a deep clean when we encounter an anonymous class, inner class and local class, but should\n            skip the class with custom serialize method.\n\n            There are five kinds of classes (or interfaces):\n            a) Top level classes\n            b) Nested classes (static member classes)\n            c) Inner classes (non-static member classes)\n            d) Local classes (named classes declared within a method)\n            e) Anonymous classes\n             */\n            if ((level == ExecutionConfig.ClosureCleanerLevel.RECURSIVE) && needsRecursion(f, fieldObject)) {\n                if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"Dig to clean the {}\", fieldObject.getClass().getName());\n                }\n                clean(fieldObject, ExecutionConfig.ClosureCleanerLevel.RECURSIVE, true, visited);\n            }\n        }\n    }\n    if (checkSerializable) {\n        try {\n            InstantiationUtil.serializeObject(func);\n        } catch (Exception e) {\n            String functionType = getSuperClassOrInterfaceName(func.getClass());\n            String msg = (functionType == null) ? func + \" is not serializable.\" : (\"The implementation of the \" + functionType) + \" is not serializable.\";\n            if (closureAccessed) {\n                msg += ((\" The implementation accesses fields of its enclosing class, which is \" + \"a common reason for non-serializability. \") + \"A common solution is to make the function a proper (non-inner) class, or \") + \"a static inner class.\";\n            } else {\n                msg += \" The object probably contains or references non serializable fields.\";\n            }\n            throw new InvalidProgramException(msg, e);\n        }\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.lang3.ClassUtils",
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.ExecutionConfig.ClosureCleanerLevel",
        "org.apache.flink.api.common.InvalidProgramException",
        "org.apache.flink.shaded.asm9.org.objectweb.asm.ClassReader",
        "org.apache.flink.shaded.asm9.org.objectweb.asm.ClassVisitor",
        "org.apache.flink.util.InstantiationUtil",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java;\n\npublic class ClosureCleanerclean_ClassUtilsisPrimitiveOrWrapperFikaTest {\n\n    @Test\n    public void testClean() {\n    }\n}",
      "conditionCount": 12,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.common.ExecutionConfig.configure(org.apache.flink.configuration.ReadableConfig, java.lang.ClassLoader)",
      "thirdPartyMethod": "org.apache.commons.compress.utils.Sets.newHashSet(java.lang.Object[])",
      "directCaller": "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<clinit>()",
      "path": [
        "org.apache.flink.api.common.ExecutionConfig.configure(org.apache.flink.configuration.ReadableConfig, java.lang.ClassLoader)",
        "org.apache.flink.configuration.RestartStrategyOptions.<clinit>()",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<clinit>()",
        "org.apache.commons.compress.utils.Sets.newHashSet(java.lang.Object[])"
      ],
      "methodSources": [
        "/**\n * Sets all relevant options contained in the {@link ReadableConfig} such as e.g. {@link PipelineOptions#CLOSURE_CLEANER_LEVEL}.\n *\n * <p>It will change the value of a setting only if a corresponding option was set in the {@code configuration}. If a key is not present, the current value of a field will remain untouched.\n *\n * @param configuration\n * \t\ta configuration to read the values from\n * @param classLoader\n * \t\ta class loader to use when loading classes\n */\npublic void configure(ReadableConfig configuration, ClassLoader classLoader) {\n    configuration.getOptional(PipelineOptions.AUTO_GENERATE_UIDS).ifPresent(this::setAutoGeneratedUids);\n    configuration.getOptional(PipelineOptions.AUTO_WATERMARK_INTERVAL).ifPresent(this::setAutoWatermarkInterval);\n    configuration.getOptional(PipelineOptions.CLOSURE_CLEANER_LEVEL).ifPresent(this::setClosureCleanerLevel);\n    configuration.getOptional(PipelineOptions.GLOBAL_JOB_PARAMETERS).ifPresent(this::setGlobalJobParameters);\n    configuration.getOptional(MetricOptions.LATENCY_INTERVAL).ifPresent(interval -> setLatencyTrackingInterval(interval.toMillis()));\n    configuration.getOptional(StateChangelogOptions.PERIODIC_MATERIALIZATION_ENABLED).ifPresent(this::enablePeriodicMaterialize);\n    configuration.getOptional(StateChangelogOptions.PERIODIC_MATERIALIZATION_INTERVAL).ifPresent(this::setPeriodicMaterializeIntervalMillis);\n    configuration.getOptional(StateChangelogOptions.MATERIALIZATION_MAX_FAILURES_ALLOWED).ifPresent(this::setMaterializationMaxAllowedFailures);\n    configuration.getOptional(PipelineOptions.MAX_PARALLELISM).ifPresent(this::setMaxParallelism);\n    configuration.getOptional(CoreOptions.DEFAULT_PARALLELISM).ifPresent(this::setParallelism);\n    configuration.getOptional(PipelineOptions.OBJECT_REUSE).ifPresent(this::setObjectReuse);\n    configuration.getOptional(TaskManagerOptions.TASK_CANCELLATION_INTERVAL).ifPresent(interval -> setTaskCancellationInterval(interval.toMillis()));\n    configuration.getOptional(TaskManagerOptions.TASK_CANCELLATION_TIMEOUT).ifPresent(timeout -> setTaskCancellationTimeout(timeout.toMillis()));\n    configuration.getOptional(ExecutionOptions.SNAPSHOT_COMPRESSION).ifPresent(this::setUseSnapshotCompression);\n    configuration.getOptional(RestartStrategyOptions.RESTART_STRATEGY).ifPresent(s -> this.setRestartStrategy(configuration));\n    configuration.getOptional(JobManagerOptions.SCHEDULER).ifPresent(t -> this.configuration.set(JobManagerOptions.SCHEDULER, t));\n    serializerConfig.configure(configuration, classLoader);\n}",
        "org.apache.flink.configuration.RestartStrategyOptions\n\n// Static field initializations\n@Internal\npublic static final String RESTART_STRATEGY_CONFIG_PREFIX = \"restart-strategy\";\npublic static final ConfigOption<String> RESTART_STRATEGY = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".type\").stringType().noDefaultValue().withDeprecatedKeys(RESTART_STRATEGY_CONFIG_PREFIX).withDescription(Description.builder().text(\"Defines the restart strategy to use in case of job failures.\").linebreak().text(\"Accepted values are:\").list(text(\"%s, %s, %s: No restart strategy.\", NO_RESTART_STRATEGY.getAllTextElement()), text(\"%s, %s: Fixed delay restart strategy. More details can be found %s.\", concat(FIXED_DELAY.getAllTextElement(), link(\"{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#fixed-delay-restart-strategy\", \"here\"))), text(\"%s, %s: Failure rate restart strategy. More details can be found %s.\", concat(FAILURE_RATE.getAllTextElement(), link(\"{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#failure-rate-restart-strategy\", \"here\"))), text(\"%s, %s: Exponential delay restart strategy. More details can be found %s.\", concat(EXPONENTIAL_DELAY.getAllTextElement(), link(\"{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#exponential-delay-restart-strategy\", \"here\")))).text(\"If checkpointing is disabled, the default value is %s. \" + \"If checkpointing is enabled, the default value is %s, and the default values of %s related config options will be used.\", code(NO_RESTART_STRATEGY.getMainValue()), code(EXPONENTIAL_DELAY.getMainValue()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Integer> RESTART_STRATEGY_FIXED_DELAY_ATTEMPTS = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".fixed-delay.attempts\").intType().defaultValue(1).withDescription(Description.builder().text(\"The number of times that Flink retries the execution before the job is declared as failed if %s has been set to %s.\", code(RESTART_STRATEGY.key()), code(FIXED_DELAY.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_FIXED_DELAY_DELAY = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".fixed-delay.delay\").durationType().defaultValue(Duration.ofSeconds(1)).withDescription(Description.builder().text(((\"Delay between two consecutive restart attempts if %s has been set to %s. \" + \"Delaying the retries can be helpful when the program interacts with external systems where \") + \"for example connections or pending transactions should reach a timeout before re-execution \") + \"is attempted. It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(FIXED_DELAY.getMainValue())).build());\npublic static final ConfigOption<Integer> RESTART_STRATEGY_FAILURE_RATE_MAX_FAILURES_PER_INTERVAL = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".failure-rate.max-failures-per-interval\").intType().defaultValue(1).withDescription(Description.builder().text(\"Maximum number of restarts in given time interval before failing a job if %s has been set to %s.\", code(RESTART_STRATEGY.key()), code(FAILURE_RATE.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_FAILURE_RATE_FAILURE_RATE_INTERVAL = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".failure-rate.failure-rate-interval\").durationType().defaultValue(Duration.ofMinutes(1)).withDescription(Description.builder().text(\"Time interval for measuring failure rate if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(FAILURE_RATE.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_FAILURE_RATE_DELAY = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".failure-rate.delay\").durationType().defaultValue(Duration.ofSeconds(1)).withDescription(Description.builder().text(\"Delay between two consecutive restart attempts if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(FAILURE_RATE.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_EXPONENTIAL_DELAY_INITIAL_BACKOFF = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.initial-backoff\").durationType().defaultValue(Duration.ofSeconds(1)).withDescription(Description.builder().text(\"Starting duration between restarts if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_EXPONENTIAL_DELAY_MAX_BACKOFF = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.max-backoff\").durationType().defaultValue(Duration.ofMinutes(1)).withDescription(Description.builder().text(\"The highest possible duration between restarts if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Double> RESTART_STRATEGY_EXPONENTIAL_DELAY_BACKOFF_MULTIPLIER = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.backoff-multiplier\").doubleType().defaultValue(1.5).withDescription(Description.builder().text(\"Backoff value is multiplied by this value after every failure,\" + \"until max backoff is reached if %s has been set to %s.\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_EXPONENTIAL_DELAY_RESET_BACKOFF_THRESHOLD = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.reset-backoff-threshold\").durationType().defaultValue(Duration.ofHours(1)).withDescription(Description.builder().text(((\"Threshold when the backoff is reset to its initial value if %s has been set to %s. \" + \"It specifies how long the job must be running without failure \") + \"to reset the exponentially increasing backoff to its initial value. \") + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Double> RESTART_STRATEGY_EXPONENTIAL_DELAY_JITTER_FACTOR = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.jitter-factor\").doubleType().defaultValue(0.1).withDescription(Description.builder().text((\"Jitter specified as a portion of the backoff if %s has been set to %s. \" + \"It represents how large random value will be added or subtracted to the backoff. \") + \"Useful when you want to avoid restarting multiple jobs at the same time.\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\n@Documentation.OverrideDefault(\"infinite\")\npublic static final ConfigOption<Integer> RESTART_STRATEGY_EXPONENTIAL_DELAY_ATTEMPTS = ConfigOptions.key(\"restart-strategy.exponential-delay.attempts-before-reset-backoff\").intType().defaultValue(Integer.MAX_VALUE).withDescription(Description.builder().text(\"The number of times that Flink retries the execution before failing the job if %s has been set to %s. \" + \"The number will be reset once the backoff is reset to its initial value.\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\n",
        "org.apache.flink.configuration.RestartStrategyOptions$RestartStrategyType\n\n// Static field initializations\nNO_RESTART_STRATEGY(\"disable\", Sets.newHashSet(\"none\", \"off\"))\nFIXED_DELAY(\"fixed-delay\", Sets.newHashSet(\"fixeddelay\"))\nFAILURE_RATE(\"failure-rate\", Sets.newHashSet(\"failurerate\"))\nEXPONENTIAL_DELAY(\"exponential-delay\", Sets.newHashSet(\"exponentialdelay\"))\n"
      ],
      "constructors": [
        "public ExecutionConfig() {\n    this(new Configuration());\n}",
        "@Internal\npublic ExecutionConfig(Configuration configuration) {\n    this.configuration = configuration;\n    this.serializerConfig = new SerializerConfigImpl(configuration);\n}",
        "GlobalJobParameters() {\n}",
        "ClosureCleanerLevel(InlineElement description) {\n    this.description = description;\n}",
        "private MapBasedJobParameters(Map<String, String> properties) {\n    this.properties = properties;\n}"
      ],
      "fieldDeclarations": [
        "// NOTE TO IMPLEMENTERS:\n// Please do not add further fields to this class. Use the ConfigOption stack instead!\n// It is currently very tricky to keep this kind of POJO classes in sync with instances of\n// org.apache.flink.configuration.Configuration. Instances of Configuration are way easier to\n// pass, layer, merge, restrict, copy, filter, etc.\n// See ExecutionOptions.RUNTIME_MODE for a reference implementation. If the option is very\n// crucial for the API, we can add a dedicated setter to StreamExecutionEnvironment. Otherwise,\n// introducing a ConfigOption should be enough.\nprivate static final long serialVersionUID = 1L;",
        "/**\n * The flag value indicating use of the default parallelism. This value can be used to reset the\n * parallelism back to the default state.\n */\npublic static final int PARALLELISM_DEFAULT = -1;",
        "/**\n * The flag value indicating an unknown or unset parallelism. This value is not a valid\n * parallelism and indicates that the parallelism should remain unchanged.\n */\npublic static final int PARALLELISM_UNKNOWN = -2;",
        "// --------------------------------------------------------------------------------------------\n/**\n * In the long run, this field should be somehow merged with the {@link Configuration} from\n * StreamExecutionEnvironment.\n */\nprivate final Configuration configuration;",
        "private final SerializerConfig serializerConfig;"
      ],
      "setters": [
        "/**\n * Sets all relevant options contained in the {@link ReadableConfig} such as e.g. {@link PipelineOptions#CLOSURE_CLEANER_LEVEL}.\n *\n * <p>It will change the value of a setting only if a corresponding option was set in the {@code configuration}. If a key is not present, the current value of a field will remain untouched.\n *\n * @param configuration\n * \t\ta configuration to read the values from\n * @param classLoader\n * \t\ta class loader to use when loading classes\n */\npublic void configure(ReadableConfig configuration, ClassLoader classLoader) {\n    configuration.getOptional(PipelineOptions.AUTO_GENERATE_UIDS).ifPresent(this::setAutoGeneratedUids);\n    configuration.getOptional(PipelineOptions.AUTO_WATERMARK_INTERVAL).ifPresent(this::setAutoWatermarkInterval);\n    configuration.getOptional(PipelineOptions.CLOSURE_CLEANER_LEVEL).ifPresent(this::setClosureCleanerLevel);\n    configuration.getOptional(PipelineOptions.GLOBAL_JOB_PARAMETERS).ifPresent(this::setGlobalJobParameters);\n    configuration.getOptional(MetricOptions.LATENCY_INTERVAL).ifPresent(interval -> setLatencyTrackingInterval(interval.toMillis()));\n    configuration.getOptional(StateChangelogOptions.PERIODIC_MATERIALIZATION_ENABLED).ifPresent(this::enablePeriodicMaterialize);\n    configuration.getOptional(StateChangelogOptions.PERIODIC_MATERIALIZATION_INTERVAL).ifPresent(this::setPeriodicMaterializeIntervalMillis);\n    configuration.getOptional(StateChangelogOptions.MATERIALIZATION_MAX_FAILURES_ALLOWED).ifPresent(this::setMaterializationMaxAllowedFailures);\n    configuration.getOptional(PipelineOptions.MAX_PARALLELISM).ifPresent(this::setMaxParallelism);\n    configuration.getOptional(CoreOptions.DEFAULT_PARALLELISM).ifPresent(this::setParallelism);\n    configuration.getOptional(PipelineOptions.OBJECT_REUSE).ifPresent(this::setObjectReuse);\n    configuration.getOptional(TaskManagerOptions.TASK_CANCELLATION_INTERVAL).ifPresent(interval -> setTaskCancellationInterval(interval.toMillis()));\n    configuration.getOptional(TaskManagerOptions.TASK_CANCELLATION_TIMEOUT).ifPresent(timeout -> setTaskCancellationTimeout(timeout.toMillis()));\n    configuration.getOptional(ExecutionOptions.SNAPSHOT_COMPRESSION).ifPresent(this::setUseSnapshotCompression);\n    configuration.getOptional(RestartStrategyOptions.RESTART_STRATEGY).ifPresent(s -> this.setRestartStrategy(configuration));\n    configuration.getOptional(JobManagerOptions.SCHEDULER).ifPresent(t -> this.configuration.set(JobManagerOptions.SCHEDULER, t));\n    serializerConfig.configure(configuration, classLoader);\n}",
        "@Internal\npublic void enablePeriodicMaterialize(boolean enabled) {\n    configuration.set(StateChangelogOptions.PERIODIC_MATERIALIZATION_ENABLED, enabled);\n}",
        "@Internal\npublic void resetParallelism() {\n    configuration.removeConfig(CoreOptions.DEFAULT_PARALLELISM);\n}",
        "private void setAutoGeneratedUids(boolean autoGeneratedUids) {\n    configuration.set(PipelineOptions.AUTO_GENERATE_UIDS, autoGeneratedUids);\n}",
        "private void setGlobalJobParameters(Map<String, String> parameters) {\n    configuration.set(PipelineOptions.GLOBAL_JOB_PARAMETERS, parameters);\n}",
        "@Internal\npublic void setMaterializationMaxAllowedFailures(int materializationMaxAllowedFailures) {\n    configuration.set(StateChangelogOptions.MATERIALIZATION_MAX_FAILURES_ALLOWED, materializationMaxAllowedFailures);\n}",
        "/**\n * Sets the maximum degree of parallelism defined for the program.\n *\n * <p>The maximum degree of parallelism specifies the upper limit for dynamic scaling. It also\n * defines the number of key groups used for partitioned state.\n *\n * @param maxParallelism\n * \t\tMaximum degree of parallelism to be used for the program.\n */\n@PublicEvolving\npublic void setMaxParallelism(int maxParallelism) {\n    checkArgument(maxParallelism > 0, \"The maximum parallelism must be greater than 0.\");\n    configuration.set(PipelineOptions.MAX_PARALLELISM, maxParallelism);\n}",
        "@Internal\npublic void setPeriodicMaterializeIntervalMillis(Duration periodicMaterializeInterval) {\n    configuration.set(StateChangelogOptions.PERIODIC_MATERIALIZATION_INTERVAL, periodicMaterializeInterval);\n}",
        "private void setRestartStrategy(ReadableConfig configuration) {\n    Map<String, String> map = configuration.toMap();\n    Map<String, String> restartStrategyEntries = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(RestartStrategyOptions.RESTART_STRATEGY_CONFIG_PREFIX)) {\n            restartStrategyEntries.put(entry.getKey(), entry.getValue());\n        }\n    }\n    this.configuration.addAll(Configuration.fromMap(restartStrategyEntries));\n}",
        "public void setUseSnapshotCompression(boolean useSnapshotCompression) {\n    configuration.set(ExecutionOptions.SNAPSHOT_COMPRESSION, useSnapshotCompression);\n}"
      ],
      "imports": [
        "org.apache.commons.compress.utils.Sets",
        "org.apache.flink.api.common.ExecutionConfig.ClosureCleanerLevel",
        "org.apache.flink.api.common.ExecutionConfig.GlobalJobParameters",
        "org.apache.flink.api.common.ExecutionConfig.MapBasedJobParameters",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.configuration.ConfigOption",
        "org.apache.flink.configuration.ConfigOptions",
        "org.apache.flink.configuration.ConfigOptions.OptionBuilder",
        "org.apache.flink.configuration.ConfigOptions.TypedConfigOptionBuilder",
        "org.apache.flink.configuration.Configuration",
        "org.apache.flink.configuration.CoreOptions",
        "org.apache.flink.configuration.ExecutionOptions",
        "org.apache.flink.configuration.JobManagerOptions",
        "org.apache.flink.configuration.JobManagerOptions.SchedulerType",
        "org.apache.flink.configuration.MetricOptions",
        "org.apache.flink.configuration.PipelineOptions",
        "org.apache.flink.configuration.ReadableConfig",
        "org.apache.flink.configuration.RestartStrategyOptions",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType",
        "org.apache.flink.configuration.StateChangelogOptions",
        "org.apache.flink.configuration.TaskManagerOptions",
        "org.apache.flink.configuration.description.Description",
        "org.apache.flink.configuration.description.Description.DescriptionBuilder",
        "org.apache.flink.configuration.description.InlineElement",
        "org.apache.flink.configuration.description.LinkElement",
        "org.apache.flink.configuration.description.TextElement",
        "org.apache.flink.util.Preconditions"
      ],
      "testTemplate": "package org.apache.flink.api.common;\n\npublic class ExecutionConfig_RestartStrategyTypemethod_SetsnewHashSetFikaTest {\n\n    @Test\n    public void testConfigure() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.parseMapToString(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.parseMapToString(java.util.Map)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)"
      ],
      "methodSources": [
        "public static String parseMapToString(Map<String, String> map) {\n    // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n    return convertToString(map);\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpSettingsBuildersetDefaultFlowStyleFikaTest {\n\n    @Test\n    public void testParseMapToString() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap(java.lang.String)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap(java.lang.String)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToProperties(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)"
      ],
      "methodSources": [
        "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    // PATH: Test should invoke the next ConfigurationUtils.convertToProperties(...) [step in execution path]\n    return convertToProperties(stringSerializedMap);\n}",
        "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpSettingsBuildersetDefaultFlowStyleFikaTest {\n\n    @Test\n    public void testParseStringToMap() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.parseMapToString(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.parseMapToString(java.util.Map)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "public static String parseMapToString(Map<String, String> map) {\n    // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n    return convertToString(map);\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testParseMapToString() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap(java.lang.String)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap(java.lang.String)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToProperties(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    // PATH: Test should invoke the next ConfigurationUtils.convertToProperties(...) [step in execution path]\n    return convertToProperties(stringSerializedMap);\n}",
        "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testParseStringToMap() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.common.RestartStrategyDescriptionUtils.getRestartStrategyDescription(org.apache.flink.configuration.Configuration)",
      "thirdPartyMethod": "org.apache.commons.compress.utils.Sets.newHashSet(java.lang.Object[])",
      "directCaller": "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<clinit>()",
      "path": [
        "org.apache.flink.api.common.RestartStrategyDescriptionUtils.getRestartStrategyDescription(org.apache.flink.configuration.Configuration)",
        "org.apache.flink.configuration.RestartStrategyOptions.<clinit>()",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<clinit>()",
        "org.apache.commons.compress.utils.Sets.newHashSet(java.lang.Object[])"
      ],
      "methodSources": [
        "/**\n * Returns a descriptive string of the restart strategy configured in the given Configuration\n * object.\n *\n * @param configuration\n * \t\tthe Configuration to extract the restart strategy from\n * @return a description of the restart strategy\n */\npublic static String getRestartStrategyDescription(Configuration configuration) {\n    final Optional<String> restartStrategyNameOptional = configuration.getOptional(RestartStrategyOptions.RESTART_STRATEGY);\n    return restartStrategyNameOptional.map(restartStrategyName -> {\n        switch (RestartStrategyOptions.RestartStrategyType.of(restartStrategyName.toLowerCase())) {\n            case NO_RESTART_STRATEGY :\n                return \"Restart deactivated.\";\n            case FIXED_DELAY :\n                return getFixedDelayDescription(configuration);\n            case FAILURE_RATE :\n                return getFailureRateDescription(configuration);\n            case EXPONENTIAL_DELAY :\n                return getExponentialDelayDescription(configuration);\n            default :\n                throw new IllegalArgumentException((\"Unknown restart strategy \" + restartStrategyName) + \".\");\n        }\n    }).orElse(\"Cluster level default restart strategy\");\n}",
        "org.apache.flink.configuration.RestartStrategyOptions\n\n// Static field initializations\n@Internal\npublic static final String RESTART_STRATEGY_CONFIG_PREFIX = \"restart-strategy\";\npublic static final ConfigOption<String> RESTART_STRATEGY = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".type\").stringType().noDefaultValue().withDeprecatedKeys(RESTART_STRATEGY_CONFIG_PREFIX).withDescription(Description.builder().text(\"Defines the restart strategy to use in case of job failures.\").linebreak().text(\"Accepted values are:\").list(text(\"%s, %s, %s: No restart strategy.\", NO_RESTART_STRATEGY.getAllTextElement()), text(\"%s, %s: Fixed delay restart strategy. More details can be found %s.\", concat(FIXED_DELAY.getAllTextElement(), link(\"{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#fixed-delay-restart-strategy\", \"here\"))), text(\"%s, %s: Failure rate restart strategy. More details can be found %s.\", concat(FAILURE_RATE.getAllTextElement(), link(\"{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#failure-rate-restart-strategy\", \"here\"))), text(\"%s, %s: Exponential delay restart strategy. More details can be found %s.\", concat(EXPONENTIAL_DELAY.getAllTextElement(), link(\"{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#exponential-delay-restart-strategy\", \"here\")))).text(\"If checkpointing is disabled, the default value is %s. \" + \"If checkpointing is enabled, the default value is %s, and the default values of %s related config options will be used.\", code(NO_RESTART_STRATEGY.getMainValue()), code(EXPONENTIAL_DELAY.getMainValue()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Integer> RESTART_STRATEGY_FIXED_DELAY_ATTEMPTS = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".fixed-delay.attempts\").intType().defaultValue(1).withDescription(Description.builder().text(\"The number of times that Flink retries the execution before the job is declared as failed if %s has been set to %s.\", code(RESTART_STRATEGY.key()), code(FIXED_DELAY.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_FIXED_DELAY_DELAY = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".fixed-delay.delay\").durationType().defaultValue(Duration.ofSeconds(1)).withDescription(Description.builder().text(((\"Delay between two consecutive restart attempts if %s has been set to %s. \" + \"Delaying the retries can be helpful when the program interacts with external systems where \") + \"for example connections or pending transactions should reach a timeout before re-execution \") + \"is attempted. It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(FIXED_DELAY.getMainValue())).build());\npublic static final ConfigOption<Integer> RESTART_STRATEGY_FAILURE_RATE_MAX_FAILURES_PER_INTERVAL = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".failure-rate.max-failures-per-interval\").intType().defaultValue(1).withDescription(Description.builder().text(\"Maximum number of restarts in given time interval before failing a job if %s has been set to %s.\", code(RESTART_STRATEGY.key()), code(FAILURE_RATE.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_FAILURE_RATE_FAILURE_RATE_INTERVAL = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".failure-rate.failure-rate-interval\").durationType().defaultValue(Duration.ofMinutes(1)).withDescription(Description.builder().text(\"Time interval for measuring failure rate if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(FAILURE_RATE.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_FAILURE_RATE_DELAY = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".failure-rate.delay\").durationType().defaultValue(Duration.ofSeconds(1)).withDescription(Description.builder().text(\"Delay between two consecutive restart attempts if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(FAILURE_RATE.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_EXPONENTIAL_DELAY_INITIAL_BACKOFF = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.initial-backoff\").durationType().defaultValue(Duration.ofSeconds(1)).withDescription(Description.builder().text(\"Starting duration between restarts if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_EXPONENTIAL_DELAY_MAX_BACKOFF = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.max-backoff\").durationType().defaultValue(Duration.ofMinutes(1)).withDescription(Description.builder().text(\"The highest possible duration between restarts if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Double> RESTART_STRATEGY_EXPONENTIAL_DELAY_BACKOFF_MULTIPLIER = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.backoff-multiplier\").doubleType().defaultValue(1.5).withDescription(Description.builder().text(\"Backoff value is multiplied by this value after every failure,\" + \"until max backoff is reached if %s has been set to %s.\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_EXPONENTIAL_DELAY_RESET_BACKOFF_THRESHOLD = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.reset-backoff-threshold\").durationType().defaultValue(Duration.ofHours(1)).withDescription(Description.builder().text(((\"Threshold when the backoff is reset to its initial value if %s has been set to %s. \" + \"It specifies how long the job must be running without failure \") + \"to reset the exponentially increasing backoff to its initial value. \") + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Double> RESTART_STRATEGY_EXPONENTIAL_DELAY_JITTER_FACTOR = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.jitter-factor\").doubleType().defaultValue(0.1).withDescription(Description.builder().text((\"Jitter specified as a portion of the backoff if %s has been set to %s. \" + \"It represents how large random value will be added or subtracted to the backoff. \") + \"Useful when you want to avoid restarting multiple jobs at the same time.\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\n@Documentation.OverrideDefault(\"infinite\")\npublic static final ConfigOption<Integer> RESTART_STRATEGY_EXPONENTIAL_DELAY_ATTEMPTS = ConfigOptions.key(\"restart-strategy.exponential-delay.attempts-before-reset-backoff\").intType().defaultValue(Integer.MAX_VALUE).withDescription(Description.builder().text(\"The number of times that Flink retries the execution before failing the job if %s has been set to %s. \" + \"The number will be reset once the backoff is reset to its initial value.\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\n",
        "org.apache.flink.configuration.RestartStrategyOptions$RestartStrategyType\n\n// Static field initializations\nNO_RESTART_STRATEGY(\"disable\", Sets.newHashSet(\"none\", \"off\"))\nFIXED_DELAY(\"fixed-delay\", Sets.newHashSet(\"fixeddelay\"))\nFAILURE_RATE(\"failure-rate\", Sets.newHashSet(\"failurerate\"))\nEXPONENTIAL_DELAY(\"exponential-delay\", Sets.newHashSet(\"exponentialdelay\"))\n"
      ],
      "constructors": [
        "RestartStrategyDescriptionUtils() {\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "org.apache.commons.compress.utils.Sets",
        "org.apache.flink.configuration.ConfigOption",
        "org.apache.flink.configuration.ConfigOptions",
        "org.apache.flink.configuration.ConfigOptions.OptionBuilder",
        "org.apache.flink.configuration.ConfigOptions.TypedConfigOptionBuilder",
        "org.apache.flink.configuration.Configuration",
        "org.apache.flink.configuration.RestartStrategyOptions",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType",
        "org.apache.flink.configuration.description.Description",
        "org.apache.flink.configuration.description.Description.DescriptionBuilder",
        "org.apache.flink.configuration.description.InlineElement",
        "org.apache.flink.configuration.description.LinkElement",
        "org.apache.flink.configuration.description.TextElement"
      ],
      "testTemplate": "package org.apache.flink.api.common;\n\npublic class RestartStrategyDescriptionUtils_RestartStrategyTypemethod_SetsnewHashSetFikaTest {\n\n    @Test\n    public void testGetRestartStrategyDescription() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.parseMapToString(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.parseMapToString(java.util.Map)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)"
      ],
      "methodSources": [
        "public static String parseMapToString(Map<String, String> map) {\n    // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n    return convertToString(map);\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpmethodFikaTest {\n\n    @Test\n    public void testParseMapToString() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap(java.lang.String)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap(java.lang.String)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToProperties(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)"
      ],
      "methodSources": [
        "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    // PATH: Test should invoke the next ConfigurationUtils.convertToProperties(...) [step in execution path]\n    return convertToProperties(stringSerializedMap);\n}",
        "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpmethodFikaTest {\n\n    @Test\n    public void testParseStringToMap() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.restoreSerializer()",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.TRACE()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.restoreSerializer()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.TRACE()"
      ],
      "methodSources": [
        "@Override\npublic TypeSerializer<T> restoreSerializer() {\n    return new KryoSerializer<>(snapshotData.getTypeClass(), snapshotData.getDefaultKryoSerializers().unwrapOptionals(), snapshotData.getDefaultKryoSerializerClasses().unwrapOptionals(), snapshotData.getKryoRegistrations().unwrapOptionals());\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.TRACE(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "@SuppressWarnings(\"unused\")\npublic KryoSerializerSnapshot() {\n}",
        "KryoSerializerSnapshot(Class<T> typeClass, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultKryoSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultKryoSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.snapshotData = createFrom(typeClass, defaultKryoSerializers, defaultKryoSerializerClasses, kryoRegistrations);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializerSnapshot.class);",
        "private static final int VERSION = 2;",
        "private KryoSerializerSnapshotData<T> snapshotData;"
      ],
      "setters": [
        "private void logMissingKeys(MergeResult<?, ?> mergeResult) {\n    mergeResult.missingKeys().forEach(key -> LOG.warn((\"The Kryo registration for a previously registered class {} does not have a \" + \"proper serializer, because its previous serializer cannot be loaded or is no \") + \"longer valid but a new serializer is not available\", key));\n}",
        "@Override\npublic void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException {\n    this.snapshotData = createFrom(in, userCodeClassLoader);\n}",
        "@Override\npublic void writeSnapshot(DataOutputView out) throws IOException {\n    snapshotData.writeSnapshotData(out);\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.common.typeutils.TypeSerializerSchemaCompatibility",
        "org.apache.flink.api.common.typeutils.TypeSerializerSnapshot",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.LinkedOptionalMap",
        "org.apache.flink.util.LinkedOptionalMap.MergeResult",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerSnapshot_KryoSerializerconfigureKryoLogging_LogTRACEFikaTest {\n\n    @Test\n    public void testRestoreSerializer() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<init>(java.lang.Class, org.apache.flink.api.common.serialization.SerializerConfig)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.TRACE()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<init>(java.lang.Class, org.apache.flink.api.common.serialization.SerializerConfig)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.TRACE()"
      ],
      "methodSources": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.TRACE(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogTRACEFikaTest {\n\n    @Test\n    public void testMethod() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate()",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.TRACE()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.TRACE()"
      ],
      "methodSources": [
        "@Override\npublic KryoSerializer<T> duplicate() {\n    return new KryoSerializer<>(this);\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.TRACE(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogTRACEFikaTest {\n\n    @Test\n    public void testDuplicate() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.parseMapToString(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.parseMapToString(java.util.Map)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "public static String parseMapToString(Map<String, String> map) {\n    // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n    return convertToString(map);\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_LoadSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testParseMapToString() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap(java.lang.String)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap(java.lang.String)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToProperties(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    // PATH: Test should invoke the next ConfigurationUtils.convertToProperties(...) [step in execution path]\n    return convertToProperties(stringSerializedMap);\n}",
        "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_LoadSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testParseStringToMap() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.restoreSerializer()",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.restoreSerializer()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)"
      ],
      "methodSources": [
        "@Override\npublic TypeSerializer<T> restoreSerializer() {\n    return new KryoSerializer<>(snapshotData.getTypeClass(), snapshotData.getDefaultKryoSerializers().unwrapOptionals(), snapshotData.getDefaultKryoSerializerClasses().unwrapOptionals(), snapshotData.getKryoRegistrations().unwrapOptionals());\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.setLogger(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "@SuppressWarnings(\"unused\")\npublic KryoSerializerSnapshot() {\n}",
        "KryoSerializerSnapshot(Class<T> typeClass, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultKryoSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultKryoSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.snapshotData = createFrom(typeClass, defaultKryoSerializers, defaultKryoSerializerClasses, kryoRegistrations);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializerSnapshot.class);",
        "private static final int VERSION = 2;",
        "private KryoSerializerSnapshotData<T> snapshotData;"
      ],
      "setters": [
        "private void logMissingKeys(MergeResult<?, ?> mergeResult) {\n    mergeResult.missingKeys().forEach(key -> LOG.warn((\"The Kryo registration for a previously registered class {} does not have a \" + \"proper serializer, because its previous serializer cannot be loaded or is no \") + \"longer valid but a new serializer is not available\", key));\n}",
        "@Override\npublic void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException {\n    this.snapshotData = createFrom(in, userCodeClassLoader);\n}",
        "@Override\npublic void writeSnapshot(DataOutputView out) throws IOException {\n    snapshotData.writeSnapshotData(out);\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.common.typeutils.TypeSerializerSchemaCompatibility",
        "org.apache.flink.api.common.typeutils.TypeSerializerSnapshot",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.LinkedOptionalMap",
        "org.apache.flink.util.LinkedOptionalMap.MergeResult",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerSnapshot_KryoSerializerconfigureKryoLogging_LogsetLoggerFikaTest {\n\n    @Test\n    public void testRestoreSerializer() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<init>(java.lang.Class, org.apache.flink.api.common.serialization.SerializerConfig)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<init>(java.lang.Class, org.apache.flink.api.common.serialization.SerializerConfig)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)"
      ],
      "methodSources": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.setLogger(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogsetLoggerFikaTest {\n\n    @Test\n    public void testMethod() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate()",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)"
      ],
      "methodSources": [
        "@Override\npublic KryoSerializer<T> duplicate() {\n    return new KryoSerializer<>(this);\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.setLogger(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogsetLoggerFikaTest {\n\n    @Test\n    public void testDuplicate() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.parseMapToString(java.util.Map)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.parseMapToString(java.util.Map)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)"
      ],
      "methodSources": [
        "public static String parseMapToString(Map<String, String> map) {\n    // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n    return convertToString(map);\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_LoadmethodFikaTest {\n\n    @Test\n    public void testParseMapToString() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap(java.lang.String)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap(java.lang.String)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToProperties(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)"
      ],
      "methodSources": [
        "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    // PATH: Test should invoke the next ConfigurationUtils.convertToProperties(...) [step in execution path]\n    return convertToProperties(stringSerializedMap);\n}",
        "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_LoadmethodFikaTest {\n\n    @Test\n    public void testParseStringToMap() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.Configuration.toMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.Configuration.toMap()",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Creates a new empty configuration.\n */\npublic Configuration() {\n    this.confData = new HashMap<>();\n}",
        "/**\n * Creates a new configuration with the copy of the given configuration.\n *\n * @param other\n * \t\tThe configuration to copy the entries from.\n */\npublic Configuration(Configuration other) {\n    this.confData = new HashMap<>(other.confData);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private static final byte TYPE_STRING = 0;",
        "private static final byte TYPE_INT = 1;",
        "private static final byte TYPE_LONG = 2;",
        "private static final byte TYPE_BOOLEAN = 3;",
        "private static final byte TYPE_FLOAT = 4;",
        "private static final byte TYPE_DOUBLE = 5;",
        "private static final byte TYPE_BYTES = 6;",
        "/**\n * The log object used for debugging.\n */\nprivate static final Logger LOG = LoggerFactory.getLogger(Configuration.class);",
        "/**\n * Stores the concrete key/value pairs of this configuration object.\n *\n * <p>NOTE: This map stores the values that are actually used, and does not include any escaping\n * that is required by the standard YAML syntax.\n */\nprotected final HashMap<String, Object> confData;"
      ],
      "setters": [
        "public void addAll(Configuration other) {\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            this.confData.putAll(other.confData);\n        }\n    }\n}",
        "/**\n * Adds all entries from the given configuration into this configuration. The keys are prepended\n * with the given prefix.\n *\n * @param other\n * \t\tThe configuration whose entries are added to this configuration.\n * @param prefix\n * \t\tThe prefix to prepend.\n */\npublic void addAll(Configuration other, String prefix) {\n    final StringBuilder bld = new StringBuilder();\n    bld.append(prefix);\n    final int pl = bld.length();\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            for (Map.Entry<String, Object> entry : other.confData.entrySet()) {\n                bld.setLength(pl);\n                bld.append(entry.getKey());\n                this.confData.put(bld.toString(), entry.getValue());\n            }\n        }\n    }\n}",
        "/**\n * Adds all entries in this {@code Configuration} to the given {@link Properties}.\n */\npublic void addAllToProperties(Properties props) {\n    synchronized(this.confData) {\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            props.put(entry.getKey(), entry.getValue());\n        }\n    }\n}",
        "private void loggingFallback(FallbackKey fallbackKey, ConfigOption<?> configOption) {\n    if (fallbackKey.isDeprecated()) {\n        LOG.warn(\"Config uses deprecated configuration key '{}' instead of proper key '{}'\", fallbackKey.getKey(), configOption.key());\n    } else {\n        LOG.info(\"Config uses fallback configuration key '{}' instead of key '{}'\", fallbackKey.getKey(), configOption.key());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// Serialization\n// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    synchronized(this.confData) {\n        final int numberOfProperties = in.readInt();\n        for (int i = 0; i < numberOfProperties; i++) {\n            String key = StringValue.readString(in);\n            Object value;\n            byte type = in.readByte();\n            switch (type) {\n                case TYPE_STRING :\n                    value = StringValue.readString(in);\n                    break;\n                case TYPE_INT :\n                    value = in.readInt();\n                    break;\n                case TYPE_LONG :\n                    value = in.readLong();\n                    break;\n                case TYPE_FLOAT :\n                    value = in.readFloat();\n                    break;\n                case TYPE_DOUBLE :\n                    value = in.readDouble();\n                    break;\n                case TYPE_BOOLEAN :\n                    value = in.readBoolean();\n                    break;\n                case TYPE_BYTES :\n                    byte[] bytes = new byte[in.readInt()];\n                    in.readFully(bytes);\n                    value = bytes;\n                    break;\n                default :\n                    throw new IOException(String.format(\"Unrecognized type: %s. This method is deprecated and\" + \" might not work for all supported types.\", type));\n            }\n            this.confData.put(key, value);\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n<T> void setValueInternal(String key, T value, boolean canBePrefixMap) {\n    if (key == null) {\n        throw new NullPointerException(\"Key must not be null.\");\n    }\n    if (value == null) {\n        throw new NullPointerException(\"Value must not be null.\");\n    }\n    synchronized(this.confData) {\n        if (canBePrefixMap) {\n            removePrefixMap(this.confData, key);\n        }\n        this.confData.put(key, value);\n    }\n}",
        "@Override\npublic void write(final DataOutputView out) throws IOException {\n    synchronized(this.confData) {\n        out.writeInt(this.confData.size());\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            String key = entry.getKey();\n            Object val = entry.getValue();\n            StringValue.writeString(key, out);\n            Class<?> clazz = val.getClass();\n            if (clazz == String.class) {\n                out.write(TYPE_STRING);\n                StringValue.writeString(((String) (val)), out);\n            } else if (clazz == Integer.class) {\n                out.write(TYPE_INT);\n                out.writeInt(((Integer) (val)));\n            } else if (clazz == Long.class) {\n                out.write(TYPE_LONG);\n                out.writeLong(((Long) (val)));\n            } else if (clazz == Float.class) {\n                out.write(TYPE_FLOAT);\n                out.writeFloat(((Float) (val)));\n            } else if (clazz == Double.class) {\n                out.write(TYPE_DOUBLE);\n                out.writeDouble(((Double) (val)));\n            } else if (clazz == byte[].class) {\n                out.write(TYPE_BYTES);\n                byte[] bytes = ((byte[]) (val));\n                out.writeInt(bytes.length);\n                out.write(bytes);\n            } else if (clazz == Boolean.class) {\n                out.write(TYPE_BOOLEAN);\n                out.writeBoolean(((Boolean) (val)));\n            } else {\n                throw new IllegalArgumentException(\"Unrecognized type. This method is deprecated and might not work\" + \" for all supported types.\");\n            }\n        }\n    }\n}"
      ],
      "imports": [
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.ExecutionConfig.GlobalJobParameters",
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.types.StringValue",
        "org.apache.flink.util.CollectionUtil",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class Configuration_YamlParserUtilsmethod_DumpSettingsBuildersetDefaultFlowStyleFikaTest {\n\n    @Test\n    public void testToMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.Configuration.toMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.Configuration.toMap()",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Creates a new empty configuration.\n */\npublic Configuration() {\n    this.confData = new HashMap<>();\n}",
        "/**\n * Creates a new configuration with the copy of the given configuration.\n *\n * @param other\n * \t\tThe configuration to copy the entries from.\n */\npublic Configuration(Configuration other) {\n    this.confData = new HashMap<>(other.confData);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private static final byte TYPE_STRING = 0;",
        "private static final byte TYPE_INT = 1;",
        "private static final byte TYPE_LONG = 2;",
        "private static final byte TYPE_BOOLEAN = 3;",
        "private static final byte TYPE_FLOAT = 4;",
        "private static final byte TYPE_DOUBLE = 5;",
        "private static final byte TYPE_BYTES = 6;",
        "/**\n * The log object used for debugging.\n */\nprivate static final Logger LOG = LoggerFactory.getLogger(Configuration.class);",
        "/**\n * Stores the concrete key/value pairs of this configuration object.\n *\n * <p>NOTE: This map stores the values that are actually used, and does not include any escaping\n * that is required by the standard YAML syntax.\n */\nprotected final HashMap<String, Object> confData;"
      ],
      "setters": [
        "public void addAll(Configuration other) {\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            this.confData.putAll(other.confData);\n        }\n    }\n}",
        "/**\n * Adds all entries from the given configuration into this configuration. The keys are prepended\n * with the given prefix.\n *\n * @param other\n * \t\tThe configuration whose entries are added to this configuration.\n * @param prefix\n * \t\tThe prefix to prepend.\n */\npublic void addAll(Configuration other, String prefix) {\n    final StringBuilder bld = new StringBuilder();\n    bld.append(prefix);\n    final int pl = bld.length();\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            for (Map.Entry<String, Object> entry : other.confData.entrySet()) {\n                bld.setLength(pl);\n                bld.append(entry.getKey());\n                this.confData.put(bld.toString(), entry.getValue());\n            }\n        }\n    }\n}",
        "/**\n * Adds all entries in this {@code Configuration} to the given {@link Properties}.\n */\npublic void addAllToProperties(Properties props) {\n    synchronized(this.confData) {\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            props.put(entry.getKey(), entry.getValue());\n        }\n    }\n}",
        "private void loggingFallback(FallbackKey fallbackKey, ConfigOption<?> configOption) {\n    if (fallbackKey.isDeprecated()) {\n        LOG.warn(\"Config uses deprecated configuration key '{}' instead of proper key '{}'\", fallbackKey.getKey(), configOption.key());\n    } else {\n        LOG.info(\"Config uses fallback configuration key '{}' instead of key '{}'\", fallbackKey.getKey(), configOption.key());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// Serialization\n// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    synchronized(this.confData) {\n        final int numberOfProperties = in.readInt();\n        for (int i = 0; i < numberOfProperties; i++) {\n            String key = StringValue.readString(in);\n            Object value;\n            byte type = in.readByte();\n            switch (type) {\n                case TYPE_STRING :\n                    value = StringValue.readString(in);\n                    break;\n                case TYPE_INT :\n                    value = in.readInt();\n                    break;\n                case TYPE_LONG :\n                    value = in.readLong();\n                    break;\n                case TYPE_FLOAT :\n                    value = in.readFloat();\n                    break;\n                case TYPE_DOUBLE :\n                    value = in.readDouble();\n                    break;\n                case TYPE_BOOLEAN :\n                    value = in.readBoolean();\n                    break;\n                case TYPE_BYTES :\n                    byte[] bytes = new byte[in.readInt()];\n                    in.readFully(bytes);\n                    value = bytes;\n                    break;\n                default :\n                    throw new IOException(String.format(\"Unrecognized type: %s. This method is deprecated and\" + \" might not work for all supported types.\", type));\n            }\n            this.confData.put(key, value);\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n<T> void setValueInternal(String key, T value, boolean canBePrefixMap) {\n    if (key == null) {\n        throw new NullPointerException(\"Key must not be null.\");\n    }\n    if (value == null) {\n        throw new NullPointerException(\"Value must not be null.\");\n    }\n    synchronized(this.confData) {\n        if (canBePrefixMap) {\n            removePrefixMap(this.confData, key);\n        }\n        this.confData.put(key, value);\n    }\n}",
        "@Override\npublic void write(final DataOutputView out) throws IOException {\n    synchronized(this.confData) {\n        out.writeInt(this.confData.size());\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            String key = entry.getKey();\n            Object val = entry.getValue();\n            StringValue.writeString(key, out);\n            Class<?> clazz = val.getClass();\n            if (clazz == String.class) {\n                out.write(TYPE_STRING);\n                StringValue.writeString(((String) (val)), out);\n            } else if (clazz == Integer.class) {\n                out.write(TYPE_INT);\n                out.writeInt(((Integer) (val)));\n            } else if (clazz == Long.class) {\n                out.write(TYPE_LONG);\n                out.writeLong(((Long) (val)));\n            } else if (clazz == Float.class) {\n                out.write(TYPE_FLOAT);\n                out.writeFloat(((Float) (val)));\n            } else if (clazz == Double.class) {\n                out.write(TYPE_DOUBLE);\n                out.writeDouble(((Double) (val)));\n            } else if (clazz == byte[].class) {\n                out.write(TYPE_BYTES);\n                byte[] bytes = ((byte[]) (val));\n                out.writeInt(bytes.length);\n                out.write(bytes);\n            } else if (clazz == Boolean.class) {\n                out.write(TYPE_BOOLEAN);\n                out.writeBoolean(((Boolean) (val)));\n            } else {\n                throw new IllegalArgumentException(\"Unrecognized type. This method is deprecated and might not work\" + \" for all supported types.\");\n            }\n        }\n    }\n}"
      ],
      "imports": [
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.ExecutionConfig.GlobalJobParameters",
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.types.StringValue",
        "org.apache.flink.util.CollectionUtil",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class Configuration_YamlParserUtilsmethod_DumpSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testToMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.of(java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.utils.Sets.newHashSet(java.lang.Object[])",
      "directCaller": "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<init>(java.lang.String, int, java.lang.String, java.util.Set)",
      "path": [
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.of(java.lang.String)",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<clinit>()",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<init>(java.lang.String, int, java.lang.String, java.util.Set)",
        "org.apache.commons.compress.utils.Sets.newHashSet(java.lang.Object[])"
      ],
      "methodSources": [
        "/**\n * Return the corresponding RestartStrategyType based on the displayed value.\n */\npublic static RestartStrategyType of(String value) {\n    for (RestartStrategyType restartStrategyType : RestartStrategyType.values()) {\n        if (restartStrategyType.getAllAvailableValues().contains(value)) {\n            return restartStrategyType;\n        }\n    }\n    throw new IllegalArgumentException(String.format(\"%s is an unknown value of RestartStrategyType.\", value));\n}",
        "org.apache.flink.configuration.RestartStrategyOptions$RestartStrategyType\n\n// Static field initializations\nNO_RESTART_STRATEGY(\"disable\", Sets.newHashSet(\"none\", \"off\"))\nFIXED_DELAY(\"fixed-delay\", Sets.newHashSet(\"fixeddelay\"))\nFAILURE_RATE(\"failure-rate\", Sets.newHashSet(\"failurerate\"))\nEXPONENTIAL_DELAY(\"exponential-delay\", Sets.newHashSet(\"exponentialdelay\"))\n",
        "(String mainValue, Set<String> otherAvailableValues) {\n    this.mainValue = mainValue;\n    // PATH: Test should invoke the next Sets.newHashSet(...) [step in execution path]\n    this.allAvailableValues = Sets.newHashSet(mainValue);\n    allAvailableValues.addAll(otherAvailableValues);\n}"
      ],
      "constructors": [
        "RestartStrategyType(String mainValue, Set<String> otherAvailableValues) {\n    this.mainValue = mainValue;\n    this.allAvailableValues = Sets.newHashSet(mainValue);\n    allAvailableValues.addAll(otherAvailableValues);\n}",
        "/**\n * Return the corresponding RestartStrategyType based on the displayed value.\n */\npublic static RestartStrategyType of(String value) {\n    for (RestartStrategyType restartStrategyType : RestartStrategyType.values()) {\n        if (restartStrategyType.getAllAvailableValues().contains(value)) {\n            return restartStrategyType;\n        }\n    }\n    throw new IllegalArgumentException(String.format(\"%s is an unknown value of RestartStrategyType.\", value));\n}"
      ],
      "fieldDeclarations": [
        "NO_RESTART_STRATEGY(\"disable\", Sets.newHashSet(\"none\", \"off\"))",
        "FIXED_DELAY(\"fixed-delay\", Sets.newHashSet(\"fixeddelay\"))",
        "FAILURE_RATE(\"failure-rate\", Sets.newHashSet(\"failurerate\"))",
        "EXPONENTIAL_DELAY(\"exponential-delay\", Sets.newHashSet(\"exponentialdelay\"))",
        "private final String mainValue;",
        "private final Set<String> allAvailableValues;"
      ],
      "setters": [],
      "imports": [
        "org.apache.commons.compress.utils.Sets",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType",
        "org.apache.flink.configuration.description.TextElement"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class RestartStrategyTypemethod_SetsnewHashSetFikaTest {\n\n    @Test\n    public void testOf() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.Configuration.toMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.Configuration.toMap()",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Creates a new empty configuration.\n */\npublic Configuration() {\n    this.confData = new HashMap<>();\n}",
        "/**\n * Creates a new configuration with the copy of the given configuration.\n *\n * @param other\n * \t\tThe configuration to copy the entries from.\n */\npublic Configuration(Configuration other) {\n    this.confData = new HashMap<>(other.confData);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private static final byte TYPE_STRING = 0;",
        "private static final byte TYPE_INT = 1;",
        "private static final byte TYPE_LONG = 2;",
        "private static final byte TYPE_BOOLEAN = 3;",
        "private static final byte TYPE_FLOAT = 4;",
        "private static final byte TYPE_DOUBLE = 5;",
        "private static final byte TYPE_BYTES = 6;",
        "/**\n * The log object used for debugging.\n */\nprivate static final Logger LOG = LoggerFactory.getLogger(Configuration.class);",
        "/**\n * Stores the concrete key/value pairs of this configuration object.\n *\n * <p>NOTE: This map stores the values that are actually used, and does not include any escaping\n * that is required by the standard YAML syntax.\n */\nprotected final HashMap<String, Object> confData;"
      ],
      "setters": [
        "public void addAll(Configuration other) {\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            this.confData.putAll(other.confData);\n        }\n    }\n}",
        "/**\n * Adds all entries from the given configuration into this configuration. The keys are prepended\n * with the given prefix.\n *\n * @param other\n * \t\tThe configuration whose entries are added to this configuration.\n * @param prefix\n * \t\tThe prefix to prepend.\n */\npublic void addAll(Configuration other, String prefix) {\n    final StringBuilder bld = new StringBuilder();\n    bld.append(prefix);\n    final int pl = bld.length();\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            for (Map.Entry<String, Object> entry : other.confData.entrySet()) {\n                bld.setLength(pl);\n                bld.append(entry.getKey());\n                this.confData.put(bld.toString(), entry.getValue());\n            }\n        }\n    }\n}",
        "/**\n * Adds all entries in this {@code Configuration} to the given {@link Properties}.\n */\npublic void addAllToProperties(Properties props) {\n    synchronized(this.confData) {\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            props.put(entry.getKey(), entry.getValue());\n        }\n    }\n}",
        "private void loggingFallback(FallbackKey fallbackKey, ConfigOption<?> configOption) {\n    if (fallbackKey.isDeprecated()) {\n        LOG.warn(\"Config uses deprecated configuration key '{}' instead of proper key '{}'\", fallbackKey.getKey(), configOption.key());\n    } else {\n        LOG.info(\"Config uses fallback configuration key '{}' instead of key '{}'\", fallbackKey.getKey(), configOption.key());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// Serialization\n// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    synchronized(this.confData) {\n        final int numberOfProperties = in.readInt();\n        for (int i = 0; i < numberOfProperties; i++) {\n            String key = StringValue.readString(in);\n            Object value;\n            byte type = in.readByte();\n            switch (type) {\n                case TYPE_STRING :\n                    value = StringValue.readString(in);\n                    break;\n                case TYPE_INT :\n                    value = in.readInt();\n                    break;\n                case TYPE_LONG :\n                    value = in.readLong();\n                    break;\n                case TYPE_FLOAT :\n                    value = in.readFloat();\n                    break;\n                case TYPE_DOUBLE :\n                    value = in.readDouble();\n                    break;\n                case TYPE_BOOLEAN :\n                    value = in.readBoolean();\n                    break;\n                case TYPE_BYTES :\n                    byte[] bytes = new byte[in.readInt()];\n                    in.readFully(bytes);\n                    value = bytes;\n                    break;\n                default :\n                    throw new IOException(String.format(\"Unrecognized type: %s. This method is deprecated and\" + \" might not work for all supported types.\", type));\n            }\n            this.confData.put(key, value);\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n<T> void setValueInternal(String key, T value, boolean canBePrefixMap) {\n    if (key == null) {\n        throw new NullPointerException(\"Key must not be null.\");\n    }\n    if (value == null) {\n        throw new NullPointerException(\"Value must not be null.\");\n    }\n    synchronized(this.confData) {\n        if (canBePrefixMap) {\n            removePrefixMap(this.confData, key);\n        }\n        this.confData.put(key, value);\n    }\n}",
        "@Override\npublic void write(final DataOutputView out) throws IOException {\n    synchronized(this.confData) {\n        out.writeInt(this.confData.size());\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            String key = entry.getKey();\n            Object val = entry.getValue();\n            StringValue.writeString(key, out);\n            Class<?> clazz = val.getClass();\n            if (clazz == String.class) {\n                out.write(TYPE_STRING);\n                StringValue.writeString(((String) (val)), out);\n            } else if (clazz == Integer.class) {\n                out.write(TYPE_INT);\n                out.writeInt(((Integer) (val)));\n            } else if (clazz == Long.class) {\n                out.write(TYPE_LONG);\n                out.writeLong(((Long) (val)));\n            } else if (clazz == Float.class) {\n                out.write(TYPE_FLOAT);\n                out.writeFloat(((Float) (val)));\n            } else if (clazz == Double.class) {\n                out.write(TYPE_DOUBLE);\n                out.writeDouble(((Double) (val)));\n            } else if (clazz == byte[].class) {\n                out.write(TYPE_BYTES);\n                byte[] bytes = ((byte[]) (val));\n                out.writeInt(bytes.length);\n                out.write(bytes);\n            } else if (clazz == Boolean.class) {\n                out.write(TYPE_BOOLEAN);\n                out.writeBoolean(((Boolean) (val)));\n            } else {\n                throw new IllegalArgumentException(\"Unrecognized type. This method is deprecated and might not work\" + \" for all supported types.\");\n            }\n        }\n    }\n}"
      ],
      "imports": [
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.ExecutionConfig.GlobalJobParameters",
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.types.StringValue",
        "org.apache.flink.util.CollectionUtil",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class Configuration_YamlParserUtilsmethod_DumpmethodFikaTest {\n\n    @Test\n    public void testToMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.GenericTypeInfo.createSerializer(org.apache.flink.api.common.serialization.SerializerConfig)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.TRACE()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.GenericTypeInfo.createSerializer(org.apache.flink.api.common.serialization.SerializerConfig)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.TRACE()"
      ],
      "methodSources": [
        "@Override\n@PublicEvolving\npublic TypeSerializer<T> createSerializer(SerializerConfig config) {\n    if (config.hasGenericTypesDisabled()) {\n        throw new UnsupportedOperationException((\"Generic types have been disabled in the ExecutionConfig and type \" + this.typeClass.getName()) + \" is treated as a generic type.\");\n    }\n    return new KryoSerializer<T>(this.typeClass, config);\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.TRACE(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "@PublicEvolving\npublic GenericTypeInfo(Class<T> typeClass) {\n    this.typeClass = checkNotNull(typeClass);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = -7959114120287706504L;",
        "private final Class<T> typeClass;"
      ],
      "setters": [],
      "imports": [
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.kryo.ChillSerializerRegistrar",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerDebugInitHelper",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder",
        "org.apache.flink.util.Preconditions",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils;\n\npublic class GenericTypeInfo_KryoSerializerconfigureKryoLogging_LogTRACEFikaTest {\n\n    @Test\n    public void testCreateSerializer() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.Configuration.toMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.Configuration.toMap()",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Creates a new empty configuration.\n */\npublic Configuration() {\n    this.confData = new HashMap<>();\n}",
        "/**\n * Creates a new configuration with the copy of the given configuration.\n *\n * @param other\n * \t\tThe configuration to copy the entries from.\n */\npublic Configuration(Configuration other) {\n    this.confData = new HashMap<>(other.confData);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private static final byte TYPE_STRING = 0;",
        "private static final byte TYPE_INT = 1;",
        "private static final byte TYPE_LONG = 2;",
        "private static final byte TYPE_BOOLEAN = 3;",
        "private static final byte TYPE_FLOAT = 4;",
        "private static final byte TYPE_DOUBLE = 5;",
        "private static final byte TYPE_BYTES = 6;",
        "/**\n * The log object used for debugging.\n */\nprivate static final Logger LOG = LoggerFactory.getLogger(Configuration.class);",
        "/**\n * Stores the concrete key/value pairs of this configuration object.\n *\n * <p>NOTE: This map stores the values that are actually used, and does not include any escaping\n * that is required by the standard YAML syntax.\n */\nprotected final HashMap<String, Object> confData;"
      ],
      "setters": [
        "public void addAll(Configuration other) {\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            this.confData.putAll(other.confData);\n        }\n    }\n}",
        "/**\n * Adds all entries from the given configuration into this configuration. The keys are prepended\n * with the given prefix.\n *\n * @param other\n * \t\tThe configuration whose entries are added to this configuration.\n * @param prefix\n * \t\tThe prefix to prepend.\n */\npublic void addAll(Configuration other, String prefix) {\n    final StringBuilder bld = new StringBuilder();\n    bld.append(prefix);\n    final int pl = bld.length();\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            for (Map.Entry<String, Object> entry : other.confData.entrySet()) {\n                bld.setLength(pl);\n                bld.append(entry.getKey());\n                this.confData.put(bld.toString(), entry.getValue());\n            }\n        }\n    }\n}",
        "/**\n * Adds all entries in this {@code Configuration} to the given {@link Properties}.\n */\npublic void addAllToProperties(Properties props) {\n    synchronized(this.confData) {\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            props.put(entry.getKey(), entry.getValue());\n        }\n    }\n}",
        "private void loggingFallback(FallbackKey fallbackKey, ConfigOption<?> configOption) {\n    if (fallbackKey.isDeprecated()) {\n        LOG.warn(\"Config uses deprecated configuration key '{}' instead of proper key '{}'\", fallbackKey.getKey(), configOption.key());\n    } else {\n        LOG.info(\"Config uses fallback configuration key '{}' instead of key '{}'\", fallbackKey.getKey(), configOption.key());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// Serialization\n// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    synchronized(this.confData) {\n        final int numberOfProperties = in.readInt();\n        for (int i = 0; i < numberOfProperties; i++) {\n            String key = StringValue.readString(in);\n            Object value;\n            byte type = in.readByte();\n            switch (type) {\n                case TYPE_STRING :\n                    value = StringValue.readString(in);\n                    break;\n                case TYPE_INT :\n                    value = in.readInt();\n                    break;\n                case TYPE_LONG :\n                    value = in.readLong();\n                    break;\n                case TYPE_FLOAT :\n                    value = in.readFloat();\n                    break;\n                case TYPE_DOUBLE :\n                    value = in.readDouble();\n                    break;\n                case TYPE_BOOLEAN :\n                    value = in.readBoolean();\n                    break;\n                case TYPE_BYTES :\n                    byte[] bytes = new byte[in.readInt()];\n                    in.readFully(bytes);\n                    value = bytes;\n                    break;\n                default :\n                    throw new IOException(String.format(\"Unrecognized type: %s. This method is deprecated and\" + \" might not work for all supported types.\", type));\n            }\n            this.confData.put(key, value);\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n<T> void setValueInternal(String key, T value, boolean canBePrefixMap) {\n    if (key == null) {\n        throw new NullPointerException(\"Key must not be null.\");\n    }\n    if (value == null) {\n        throw new NullPointerException(\"Value must not be null.\");\n    }\n    synchronized(this.confData) {\n        if (canBePrefixMap) {\n            removePrefixMap(this.confData, key);\n        }\n        this.confData.put(key, value);\n    }\n}",
        "@Override\npublic void write(final DataOutputView out) throws IOException {\n    synchronized(this.confData) {\n        out.writeInt(this.confData.size());\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            String key = entry.getKey();\n            Object val = entry.getValue();\n            StringValue.writeString(key, out);\n            Class<?> clazz = val.getClass();\n            if (clazz == String.class) {\n                out.write(TYPE_STRING);\n                StringValue.writeString(((String) (val)), out);\n            } else if (clazz == Integer.class) {\n                out.write(TYPE_INT);\n                out.writeInt(((Integer) (val)));\n            } else if (clazz == Long.class) {\n                out.write(TYPE_LONG);\n                out.writeLong(((Long) (val)));\n            } else if (clazz == Float.class) {\n                out.write(TYPE_FLOAT);\n                out.writeFloat(((Float) (val)));\n            } else if (clazz == Double.class) {\n                out.write(TYPE_DOUBLE);\n                out.writeDouble(((Double) (val)));\n            } else if (clazz == byte[].class) {\n                out.write(TYPE_BYTES);\n                byte[] bytes = ((byte[]) (val));\n                out.writeInt(bytes.length);\n                out.write(bytes);\n            } else if (clazz == Boolean.class) {\n                out.write(TYPE_BOOLEAN);\n                out.writeBoolean(((Boolean) (val)));\n            } else {\n                throw new IllegalArgumentException(\"Unrecognized type. This method is deprecated and might not work\" + \" for all supported types.\");\n            }\n        }\n    }\n}"
      ],
      "imports": [
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.ExecutionConfig.GlobalJobParameters",
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.types.StringValue",
        "org.apache.flink.util.CollectionUtil",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class Configuration_YamlParserUtilsmethod_LoadSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testToMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.GenericTypeInfo.createSerializer(org.apache.flink.api.common.serialization.SerializerConfig)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.GenericTypeInfo.createSerializer(org.apache.flink.api.common.serialization.SerializerConfig)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)"
      ],
      "methodSources": [
        "@Override\n@PublicEvolving\npublic TypeSerializer<T> createSerializer(SerializerConfig config) {\n    if (config.hasGenericTypesDisabled()) {\n        throw new UnsupportedOperationException((\"Generic types have been disabled in the ExecutionConfig and type \" + this.typeClass.getName()) + \" is treated as a generic type.\");\n    }\n    return new KryoSerializer<T>(this.typeClass, config);\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.setLogger(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "@PublicEvolving\npublic GenericTypeInfo(Class<T> typeClass) {\n    this.typeClass = checkNotNull(typeClass);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = -7959114120287706504L;",
        "private final Class<T> typeClass;"
      ],
      "setters": [],
      "imports": [
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.kryo.ChillSerializerRegistrar",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerDebugInitHelper",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder",
        "org.apache.flink.util.Preconditions",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils;\n\npublic class GenericTypeInfo_KryoSerializerconfigureKryoLogging_LogsetLoggerFikaTest {\n\n    @Test\n    public void testCreateSerializer() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.Configuration.toMap()",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.Configuration.toMap()",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Creates a new empty configuration.\n */\npublic Configuration() {\n    this.confData = new HashMap<>();\n}",
        "/**\n * Creates a new configuration with the copy of the given configuration.\n *\n * @param other\n * \t\tThe configuration to copy the entries from.\n */\npublic Configuration(Configuration other) {\n    this.confData = new HashMap<>(other.confData);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private static final byte TYPE_STRING = 0;",
        "private static final byte TYPE_INT = 1;",
        "private static final byte TYPE_LONG = 2;",
        "private static final byte TYPE_BOOLEAN = 3;",
        "private static final byte TYPE_FLOAT = 4;",
        "private static final byte TYPE_DOUBLE = 5;",
        "private static final byte TYPE_BYTES = 6;",
        "/**\n * The log object used for debugging.\n */\nprivate static final Logger LOG = LoggerFactory.getLogger(Configuration.class);",
        "/**\n * Stores the concrete key/value pairs of this configuration object.\n *\n * <p>NOTE: This map stores the values that are actually used, and does not include any escaping\n * that is required by the standard YAML syntax.\n */\nprotected final HashMap<String, Object> confData;"
      ],
      "setters": [
        "public void addAll(Configuration other) {\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            this.confData.putAll(other.confData);\n        }\n    }\n}",
        "/**\n * Adds all entries from the given configuration into this configuration. The keys are prepended\n * with the given prefix.\n *\n * @param other\n * \t\tThe configuration whose entries are added to this configuration.\n * @param prefix\n * \t\tThe prefix to prepend.\n */\npublic void addAll(Configuration other, String prefix) {\n    final StringBuilder bld = new StringBuilder();\n    bld.append(prefix);\n    final int pl = bld.length();\n    synchronized(this.confData) {\n        synchronized(other.confData) {\n            for (Map.Entry<String, Object> entry : other.confData.entrySet()) {\n                bld.setLength(pl);\n                bld.append(entry.getKey());\n                this.confData.put(bld.toString(), entry.getValue());\n            }\n        }\n    }\n}",
        "/**\n * Adds all entries in this {@code Configuration} to the given {@link Properties}.\n */\npublic void addAllToProperties(Properties props) {\n    synchronized(this.confData) {\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            props.put(entry.getKey(), entry.getValue());\n        }\n    }\n}",
        "private void loggingFallback(FallbackKey fallbackKey, ConfigOption<?> configOption) {\n    if (fallbackKey.isDeprecated()) {\n        LOG.warn(\"Config uses deprecated configuration key '{}' instead of proper key '{}'\", fallbackKey.getKey(), configOption.key());\n    } else {\n        LOG.info(\"Config uses fallback configuration key '{}' instead of key '{}'\", fallbackKey.getKey(), configOption.key());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// Serialization\n// --------------------------------------------------------------------------------------------\n@Override\npublic void read(DataInputView in) throws IOException {\n    synchronized(this.confData) {\n        final int numberOfProperties = in.readInt();\n        for (int i = 0; i < numberOfProperties; i++) {\n            String key = StringValue.readString(in);\n            Object value;\n            byte type = in.readByte();\n            switch (type) {\n                case TYPE_STRING :\n                    value = StringValue.readString(in);\n                    break;\n                case TYPE_INT :\n                    value = in.readInt();\n                    break;\n                case TYPE_LONG :\n                    value = in.readLong();\n                    break;\n                case TYPE_FLOAT :\n                    value = in.readFloat();\n                    break;\n                case TYPE_DOUBLE :\n                    value = in.readDouble();\n                    break;\n                case TYPE_BOOLEAN :\n                    value = in.readBoolean();\n                    break;\n                case TYPE_BYTES :\n                    byte[] bytes = new byte[in.readInt()];\n                    in.readFully(bytes);\n                    value = bytes;\n                    break;\n                default :\n                    throw new IOException(String.format(\"Unrecognized type: %s. This method is deprecated and\" + \" might not work for all supported types.\", type));\n            }\n            this.confData.put(key, value);\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n<T> void setValueInternal(String key, T value, boolean canBePrefixMap) {\n    if (key == null) {\n        throw new NullPointerException(\"Key must not be null.\");\n    }\n    if (value == null) {\n        throw new NullPointerException(\"Value must not be null.\");\n    }\n    synchronized(this.confData) {\n        if (canBePrefixMap) {\n            removePrefixMap(this.confData, key);\n        }\n        this.confData.put(key, value);\n    }\n}",
        "@Override\npublic void write(final DataOutputView out) throws IOException {\n    synchronized(this.confData) {\n        out.writeInt(this.confData.size());\n        for (Map.Entry<String, Object> entry : this.confData.entrySet()) {\n            String key = entry.getKey();\n            Object val = entry.getValue();\n            StringValue.writeString(key, out);\n            Class<?> clazz = val.getClass();\n            if (clazz == String.class) {\n                out.write(TYPE_STRING);\n                StringValue.writeString(((String) (val)), out);\n            } else if (clazz == Integer.class) {\n                out.write(TYPE_INT);\n                out.writeInt(((Integer) (val)));\n            } else if (clazz == Long.class) {\n                out.write(TYPE_LONG);\n                out.writeLong(((Long) (val)));\n            } else if (clazz == Float.class) {\n                out.write(TYPE_FLOAT);\n                out.writeFloat(((Float) (val)));\n            } else if (clazz == Double.class) {\n                out.write(TYPE_DOUBLE);\n                out.writeDouble(((Double) (val)));\n            } else if (clazz == byte[].class) {\n                out.write(TYPE_BYTES);\n                byte[] bytes = ((byte[]) (val));\n                out.writeInt(bytes.length);\n                out.write(bytes);\n            } else if (clazz == Boolean.class) {\n                out.write(TYPE_BOOLEAN);\n                out.writeBoolean(((Boolean) (val)));\n            } else {\n                throw new IllegalArgumentException(\"Unrecognized type. This method is deprecated and might not work\" + \" for all supported types.\");\n            }\n        }\n    }\n}"
      ],
      "imports": [
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.ExecutionConfig.GlobalJobParameters",
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.types.StringValue",
        "org.apache.flink.util.CollectionUtil",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class Configuration_YamlParserUtilsmethod_LoadmethodFikaTest {\n\n    @Test\n    public void testToMap() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.PojoTypeInfo.createSerializer(org.apache.flink.api.common.serialization.SerializerConfig)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.TRACE()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.PojoTypeInfo.createSerializer(org.apache.flink.api.common.serialization.SerializerConfig)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.TRACE()"
      ],
      "methodSources": [
        "@Override\n@PublicEvolving\n@SuppressWarnings(\"unchecked\")\npublic TypeSerializer<T> createSerializer(SerializerConfig config) {\n    if (config.isForceKryoEnabled()) {\n        return new KryoSerializer<>(getTypeClass(), config);\n    }\n    if (config.isForceAvroEnabled()) {\n        return AvroUtils.getAvroUtils().createAvroSerializer(getTypeClass());\n    }\n    return createPojoSerializer(config);\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.TRACE(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "@PublicEvolving\npublic PojoTypeInfo(Class<T> typeClass, List<PojoField> fields) {\n    super(typeClass);\n    checkArgument(Modifier.isPublic(typeClass.getModifiers()), \"POJO %s is not public\", typeClass);\n    this.fields = fields.toArray(new PojoField[fields.size()]);\n    Arrays.sort(this.fields, new Comparator<PojoField>() {\n        @Override\n        public int compare(PojoField o1, PojoField o2) {\n            return o1.getField().getName().compareTo(o2.getField().getName());\n        }\n    });\n    int counterFields = 0;\n    for (PojoField field : fields) {\n        counterFields += field.getTypeInformation().getTotalFields();\n    }\n    totalFields = counterFields;\n}",
        "1() {\n}",
        "public PojoTypeComparatorBuilder() {\n    fieldComparators = new ArrayList<TypeComparator>();\n    keyFields = new ArrayList<Field>();\n}",
        "public NamedFlatFieldDescriptor(String name, int keyPosition, TypeInformation<?> type) {\n    super(keyPosition, type);\n    this.fieldName = name;\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private static final String REGEX_FIELD = \"[\\\\p{L}_\\\\$][\\\\p{L}\\\\p{Digit}_\\\\$]*\";",
        "private static final String REGEX_NESTED_FIELDS = (\"(\" + REGEX_FIELD) + \")(\\\\.(.+))?\";",
        "private static final String REGEX_NESTED_FIELDS_WILDCARD = (((REGEX_NESTED_FIELDS + \"|\\\\\") + ExpressionKeys.SELECT_ALL_CHAR) + \"|\\\\\") + ExpressionKeys.SELECT_ALL_CHAR_SCALA;",
        "private static final Pattern PATTERN_NESTED_FIELDS = Pattern.compile(REGEX_NESTED_FIELDS);",
        "private static final Pattern PATTERN_NESTED_FIELDS_WILDCARD = Pattern.compile(REGEX_NESTED_FIELDS_WILDCARD);",
        "private final PojoField[] fields;",
        "private final int totalFields;"
      ],
      "setters": [
        "@Override\n@PublicEvolving\npublic void getFlatFields(String fieldExpression, int offset, List<FlatFieldDescriptor> result) {\n    Matcher matcher = PATTERN_NESTED_FIELDS_WILDCARD.matcher(fieldExpression);\n    if (!matcher.matches()) {\n        throw new InvalidFieldReferenceException((\"Invalid POJO field reference \\\"\" + fieldExpression) + \"\\\".\");\n    }\n    String field = matcher.group(0);\n    if (field.equals(ExpressionKeys.SELECT_ALL_CHAR) || field.equals(ExpressionKeys.SELECT_ALL_CHAR_SCALA)) {\n        // handle select all\n        int keyPosition = 0;\n        for (PojoField pField : fields) {\n            if (pField.getTypeInformation() instanceof CompositeType) {\n                CompositeType<?> cType = ((CompositeType<?>) (pField.getTypeInformation()));\n                cType.getFlatFields(String.valueOf(ExpressionKeys.SELECT_ALL_CHAR), offset + keyPosition, result);\n                keyPosition += cType.getTotalFields() - 1;\n            } else {\n                result.add(new NamedFlatFieldDescriptor(pField.getField().getName(), offset + keyPosition, pField.getTypeInformation()));\n            }\n            keyPosition++;\n        }\n        return;\n    } else {\n        field = matcher.group(1);\n    }\n    // get field\n    int fieldPos = -1;\n    TypeInformation<?> fieldType = null;\n    for (int i = 0; i < fields.length; i++) {\n        if (fields[i].getField().getName().equals(field)) {\n            fieldPos = i;\n            fieldType = fields[i].getTypeInformation();\n            break;\n        }\n    }\n    if (fieldPos == (-1)) {\n        throw new InvalidFieldReferenceException((((\"Unable to find field \\\"\" + field) + \"\\\" in type \") + this) + \".\");\n    }\n    String tail = matcher.group(3);\n    if (tail == null) {\n        if (fieldType instanceof CompositeType) {\n            // forward offset\n            for (int i = 0; i < fieldPos; i++) {\n                offset += this.getTypeAt(i).getTotalFields();\n            }\n            // add all fields of composite type\n            ((CompositeType<?>) (fieldType)).getFlatFields(\"*\", offset, result);\n        } else {\n            // we found the field to add\n            // compute flat field position by adding skipped fields\n            int flatFieldPos = offset;\n            for (int i = 0; i < fieldPos; i++) {\n                flatFieldPos += this.getTypeAt(i).getTotalFields();\n            }\n            result.add(new FlatFieldDescriptor(flatFieldPos, fieldType));\n        }\n    } else if (fieldType instanceof CompositeType<?>) {\n        // forward offset\n        for (int i = 0; i < fieldPos; i++) {\n            offset += this.getTypeAt(i).getTotalFields();\n        }\n        ((CompositeType<?>) (fieldType)).getFlatFields(tail, offset, result);\n    } else {\n        throw new InvalidFieldReferenceException((((\"Nested field expression \\\"\" + tail) + \"\\\" not possible on atomic type \") + fieldType) + \".\");\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.operators.Keys",
        "org.apache.flink.api.common.operators.Keys.ExpressionKeys",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.CompositeType.FlatFieldDescriptor",
        "org.apache.flink.api.common.typeutils.CompositeType.InvalidFieldReferenceException",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.PojoTypeInfo.NamedFlatFieldDescriptor",
        "org.apache.flink.api.java.typeutils.PojoTypeInfo.PojoTypeComparatorBuilder",
        "org.apache.flink.api.java.typeutils.runtime.PojoSerializer",
        "org.apache.flink.api.java.typeutils.runtime.kryo.ChillSerializerRegistrar",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerDebugInitHelper",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder",
        "org.apache.flink.util.Preconditions",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils;\n\npublic class PojoTypeInfo_KryoSerializerconfigureKryoLogging_LogTRACEFikaTest {\n\n    @Test\n    public void testCreateSerializer() {\n    }\n}",
      "conditionCount": 3,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.PojoTypeInfo.createSerializer(org.apache.flink.api.common.serialization.SerializerConfig)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.PojoTypeInfo.createSerializer(org.apache.flink.api.common.serialization.SerializerConfig)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)"
      ],
      "methodSources": [
        "@Override\n@PublicEvolving\n@SuppressWarnings(\"unchecked\")\npublic TypeSerializer<T> createSerializer(SerializerConfig config) {\n    if (config.isForceKryoEnabled()) {\n        return new KryoSerializer<>(getTypeClass(), config);\n    }\n    if (config.isForceAvroEnabled()) {\n        return AvroUtils.getAvroUtils().createAvroSerializer(getTypeClass());\n    }\n    return createPojoSerializer(config);\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.setLogger(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "@PublicEvolving\npublic PojoTypeInfo(Class<T> typeClass, List<PojoField> fields) {\n    super(typeClass);\n    checkArgument(Modifier.isPublic(typeClass.getModifiers()), \"POJO %s is not public\", typeClass);\n    this.fields = fields.toArray(new PojoField[fields.size()]);\n    Arrays.sort(this.fields, new Comparator<PojoField>() {\n        @Override\n        public int compare(PojoField o1, PojoField o2) {\n            return o1.getField().getName().compareTo(o2.getField().getName());\n        }\n    });\n    int counterFields = 0;\n    for (PojoField field : fields) {\n        counterFields += field.getTypeInformation().getTotalFields();\n    }\n    totalFields = counterFields;\n}",
        "1() {\n}",
        "public PojoTypeComparatorBuilder() {\n    fieldComparators = new ArrayList<TypeComparator>();\n    keyFields = new ArrayList<Field>();\n}",
        "public NamedFlatFieldDescriptor(String name, int keyPosition, TypeInformation<?> type) {\n    super(keyPosition, type);\n    this.fieldName = name;\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private static final String REGEX_FIELD = \"[\\\\p{L}_\\\\$][\\\\p{L}\\\\p{Digit}_\\\\$]*\";",
        "private static final String REGEX_NESTED_FIELDS = (\"(\" + REGEX_FIELD) + \")(\\\\.(.+))?\";",
        "private static final String REGEX_NESTED_FIELDS_WILDCARD = (((REGEX_NESTED_FIELDS + \"|\\\\\") + ExpressionKeys.SELECT_ALL_CHAR) + \"|\\\\\") + ExpressionKeys.SELECT_ALL_CHAR_SCALA;",
        "private static final Pattern PATTERN_NESTED_FIELDS = Pattern.compile(REGEX_NESTED_FIELDS);",
        "private static final Pattern PATTERN_NESTED_FIELDS_WILDCARD = Pattern.compile(REGEX_NESTED_FIELDS_WILDCARD);",
        "private final PojoField[] fields;",
        "private final int totalFields;"
      ],
      "setters": [
        "@Override\n@PublicEvolving\npublic void getFlatFields(String fieldExpression, int offset, List<FlatFieldDescriptor> result) {\n    Matcher matcher = PATTERN_NESTED_FIELDS_WILDCARD.matcher(fieldExpression);\n    if (!matcher.matches()) {\n        throw new InvalidFieldReferenceException((\"Invalid POJO field reference \\\"\" + fieldExpression) + \"\\\".\");\n    }\n    String field = matcher.group(0);\n    if (field.equals(ExpressionKeys.SELECT_ALL_CHAR) || field.equals(ExpressionKeys.SELECT_ALL_CHAR_SCALA)) {\n        // handle select all\n        int keyPosition = 0;\n        for (PojoField pField : fields) {\n            if (pField.getTypeInformation() instanceof CompositeType) {\n                CompositeType<?> cType = ((CompositeType<?>) (pField.getTypeInformation()));\n                cType.getFlatFields(String.valueOf(ExpressionKeys.SELECT_ALL_CHAR), offset + keyPosition, result);\n                keyPosition += cType.getTotalFields() - 1;\n            } else {\n                result.add(new NamedFlatFieldDescriptor(pField.getField().getName(), offset + keyPosition, pField.getTypeInformation()));\n            }\n            keyPosition++;\n        }\n        return;\n    } else {\n        field = matcher.group(1);\n    }\n    // get field\n    int fieldPos = -1;\n    TypeInformation<?> fieldType = null;\n    for (int i = 0; i < fields.length; i++) {\n        if (fields[i].getField().getName().equals(field)) {\n            fieldPos = i;\n            fieldType = fields[i].getTypeInformation();\n            break;\n        }\n    }\n    if (fieldPos == (-1)) {\n        throw new InvalidFieldReferenceException((((\"Unable to find field \\\"\" + field) + \"\\\" in type \") + this) + \".\");\n    }\n    String tail = matcher.group(3);\n    if (tail == null) {\n        if (fieldType instanceof CompositeType) {\n            // forward offset\n            for (int i = 0; i < fieldPos; i++) {\n                offset += this.getTypeAt(i).getTotalFields();\n            }\n            // add all fields of composite type\n            ((CompositeType<?>) (fieldType)).getFlatFields(\"*\", offset, result);\n        } else {\n            // we found the field to add\n            // compute flat field position by adding skipped fields\n            int flatFieldPos = offset;\n            for (int i = 0; i < fieldPos; i++) {\n                flatFieldPos += this.getTypeAt(i).getTotalFields();\n            }\n            result.add(new FlatFieldDescriptor(flatFieldPos, fieldType));\n        }\n    } else if (fieldType instanceof CompositeType<?>) {\n        // forward offset\n        for (int i = 0; i < fieldPos; i++) {\n            offset += this.getTypeAt(i).getTotalFields();\n        }\n        ((CompositeType<?>) (fieldType)).getFlatFields(tail, offset, result);\n    } else {\n        throw new InvalidFieldReferenceException((((\"Nested field expression \\\"\" + tail) + \"\\\" not possible on atomic type \") + fieldType) + \".\");\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.operators.Keys",
        "org.apache.flink.api.common.operators.Keys.ExpressionKeys",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.CompositeType.FlatFieldDescriptor",
        "org.apache.flink.api.common.typeutils.CompositeType.InvalidFieldReferenceException",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.PojoTypeInfo.NamedFlatFieldDescriptor",
        "org.apache.flink.api.java.typeutils.PojoTypeInfo.PojoTypeComparatorBuilder",
        "org.apache.flink.api.java.typeutils.runtime.PojoSerializer",
        "org.apache.flink.api.java.typeutils.runtime.kryo.ChillSerializerRegistrar",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerDebugInitHelper",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder",
        "org.apache.flink.util.Preconditions",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils;\n\npublic class PojoTypeInfo_KryoSerializerconfigureKryoLogging_LogsetLoggerFikaTest {\n\n    @Test\n    public void testCreateSerializer() {\n    }\n}",
      "conditionCount": 3,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration(java.lang.String, org.apache.flink.configuration.Configuration)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration(java.lang.String, org.apache.flink.configuration.Configuration)",
        "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource(java.io.File)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)"
      ],
      "methodSources": [
        "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        // PATH: Test should invoke the next GlobalConfiguration.loadYAMLResource(...) [step in execution path]\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}",
        "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\nprivate GlobalConfiguration() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(GlobalConfiguration.class);",
        "public static final String FLINK_CONF_FILENAME = \"config.yaml\";",
        "// key separator character\nprivate static final String KEY_SEPARATOR = \".\";",
        "// the keys whose values should be hidden\nprivate static final String[] SENSITIVE_KEYS = new String[]{ \"password\", \"secret\", \"fs.azure.account.key\", \"apikey\", \"api-key\", \"auth-params\", \"service-key\", \"token\", \"basic-auth\", \"jaas.config\", \"http-headers\" };",
        "// the hidden content to be displayed\npublic static final String HIDDEN_CONTENT = \"******\";"
      ],
      "setters": [
        "private static void logConfiguration(String prefix, Configuration config) {\n    config.confData.forEach((key, value) -> LOG.info(\"{} configuration property: {}, {}\", prefix, key, isSensitive(key) ? HIDDEN_CONTENT : value));\n}"
      ],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class GlobalConfiguration_YamlParserUtilsmethod_DumpSettingsBuildersetDefaultFlowStyleFikaTest {\n\n    @Test\n    public void testLoadConfiguration() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration(java.lang.String, org.apache.flink.configuration.Configuration)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration(java.lang.String, org.apache.flink.configuration.Configuration)",
        "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource(java.io.File)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        // PATH: Test should invoke the next GlobalConfiguration.loadYAMLResource(...) [step in execution path]\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}",
        "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\nprivate GlobalConfiguration() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(GlobalConfiguration.class);",
        "public static final String FLINK_CONF_FILENAME = \"config.yaml\";",
        "// key separator character\nprivate static final String KEY_SEPARATOR = \".\";",
        "// the keys whose values should be hidden\nprivate static final String[] SENSITIVE_KEYS = new String[]{ \"password\", \"secret\", \"fs.azure.account.key\", \"apikey\", \"api-key\", \"auth-params\", \"service-key\", \"token\", \"basic-auth\", \"jaas.config\", \"http-headers\" };",
        "// the hidden content to be displayed\npublic static final String HIDDEN_CONTENT = \"******\";"
      ],
      "setters": [
        "private static void logConfiguration(String prefix, Configuration config) {\n    config.confData.forEach((key, value) -> LOG.info(\"{} configuration property: {}, {}\", prefix, key, isSensitive(key) ? HIDDEN_CONTENT : value));\n}"
      ],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class GlobalConfiguration_YamlParserUtilsmethod_DumpSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testLoadConfiguration() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate()",
      "thirdPartyMethod": "org.apache.commons.lang3.exception.CloneFailedException.<init>(java.lang.String, java.lang.Throwable)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deepCopySerializer(org.apache.flink.api.common.SerializableSerializer)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<init>(org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deepCopySerializer(org.apache.flink.api.common.SerializableSerializer)",
        "org.apache.commons.lang3.exception.CloneFailedException.<init>(java.lang.String, java.lang.Throwable)"
      ],
      "methodSources": [
        "@Override\npublic KryoSerializer<T> duplicate() {\n    return new KryoSerializer<>(this);\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected (KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        // PATH: Test should invoke the next KryoSerializer.deepCopySerializer(...) [step in execution path]\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "private SerializableSerializer<? extends Serializer<?>> deepCopySerializer(SerializableSerializer<? extends Serializer<?>> original) {\n    try {\n        return InstantiationUtil.clone(original, Thread.currentThread().getContextClassLoader());\n    } catch (IOException | ClassNotFoundException ex) {\n        throw new CloneFailedException(\"Could not clone serializer instance of class \" + original.getClass(), ex);\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.commons.lang3.exception.CloneFailedException",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.InstantiationUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerdeepCopySerializer_CloneFailedExceptionmethodFikaTest {\n\n    @Test\n    public void testDuplicate() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration(java.lang.String, org.apache.flink.configuration.Configuration)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration(java.lang.String, org.apache.flink.configuration.Configuration)",
        "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource(java.io.File)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)"
      ],
      "methodSources": [
        "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        // PATH: Test should invoke the next GlobalConfiguration.loadYAMLResource(...) [step in execution path]\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}",
        "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\nprivate GlobalConfiguration() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(GlobalConfiguration.class);",
        "public static final String FLINK_CONF_FILENAME = \"config.yaml\";",
        "// key separator character\nprivate static final String KEY_SEPARATOR = \".\";",
        "// the keys whose values should be hidden\nprivate static final String[] SENSITIVE_KEYS = new String[]{ \"password\", \"secret\", \"fs.azure.account.key\", \"apikey\", \"api-key\", \"auth-params\", \"service-key\", \"token\", \"basic-auth\", \"jaas.config\", \"http-headers\" };",
        "// the hidden content to be displayed\npublic static final String HIDDEN_CONTENT = \"******\";"
      ],
      "setters": [
        "private static void logConfiguration(String prefix, Configuration config) {\n    config.confData.forEach((key, value) -> LOG.info(\"{} configuration property: {}, {}\", prefix, key, isSensitive(key) ? HIDDEN_CONTENT : value));\n}"
      ],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class GlobalConfiguration_YamlParserUtilsmethod_DumpmethodFikaTest {\n\n    @Test\n    public void testLoadConfiguration() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.TRACE()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.TRACE()"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.TRACE(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogTRACEFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(org.apache.flink.core.memory.DataInputView, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.TRACE()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(org.apache.flink.core.memory.DataInputView, org.apache.flink.core.memory.DataOutputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.TRACE()"
      ],
      "methodSources": [
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.TRACE(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogTRACEFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration(java.lang.String, org.apache.flink.configuration.Configuration)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration(java.lang.String, org.apache.flink.configuration.Configuration)",
        "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource(java.io.File)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        // PATH: Test should invoke the next GlobalConfiguration.loadYAMLResource(...) [step in execution path]\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}",
        "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\nprivate GlobalConfiguration() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(GlobalConfiguration.class);",
        "public static final String FLINK_CONF_FILENAME = \"config.yaml\";",
        "// key separator character\nprivate static final String KEY_SEPARATOR = \".\";",
        "// the keys whose values should be hidden\nprivate static final String[] SENSITIVE_KEYS = new String[]{ \"password\", \"secret\", \"fs.azure.account.key\", \"apikey\", \"api-key\", \"auth-params\", \"service-key\", \"token\", \"basic-auth\", \"jaas.config\", \"http-headers\" };",
        "// the hidden content to be displayed\npublic static final String HIDDEN_CONTENT = \"******\";"
      ],
      "setters": [
        "private static void logConfiguration(String prefix, Configuration config) {\n    config.confData.forEach((key, value) -> LOG.info(\"{} configuration property: {}, {}\", prefix, key, isSensitive(key) ? HIDDEN_CONTENT : value));\n}"
      ],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class GlobalConfiguration_YamlParserUtilsmethod_LoadSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testLoadConfiguration() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.setLogger(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogsetLoggerFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(org.apache.flink.core.memory.DataInputView, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(org.apache.flink.core.memory.DataInputView, org.apache.flink.core.memory.DataOutputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)"
      ],
      "methodSources": [
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.setLogger(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogsetLoggerFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration(java.lang.String, org.apache.flink.configuration.Configuration)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration(java.lang.String, org.apache.flink.configuration.Configuration)",
        "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource(java.io.File)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)"
      ],
      "methodSources": [
        "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        // PATH: Test should invoke the next GlobalConfiguration.loadYAMLResource(...) [step in execution path]\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}",
        "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\nprivate GlobalConfiguration() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(GlobalConfiguration.class);",
        "public static final String FLINK_CONF_FILENAME = \"config.yaml\";",
        "// key separator character\nprivate static final String KEY_SEPARATOR = \".\";",
        "// the keys whose values should be hidden\nprivate static final String[] SENSITIVE_KEYS = new String[]{ \"password\", \"secret\", \"fs.azure.account.key\", \"apikey\", \"api-key\", \"auth-params\", \"service-key\", \"token\", \"basic-auth\", \"jaas.config\", \"http-headers\" };",
        "// the hidden content to be displayed\npublic static final String HIDDEN_CONTENT = \"******\";"
      ],
      "setters": [
        "private static void logConfiguration(String prefix, Configuration config) {\n    config.confData.forEach((key, value) -> LOG.info(\"{} configuration property: {}, {}\", prefix, key, isSensitive(key) ? HIDDEN_CONTENT : value));\n}"
      ],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class GlobalConfiguration_YamlParserUtilsmethod_LoadmethodFikaTest {\n\n    @Test\n    public void testLoadConfiguration() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.TRACE()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.TRACE()"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.TRACE(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogTRACEFikaTest {\n\n    @Test\n    public void testDeserialize() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.setLogger(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogsetLoggerFikaTest {\n\n    @Test\n    public void testDeserialize() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.TRACE()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.TRACE()"
      ],
      "methodSources": [
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.TRACE(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogTRACEFikaTest {\n\n    @Test\n    public void testSerialize() {\n    }\n}",
      "conditionCount": 6,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)"
      ],
      "methodSources": [
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.setLogger(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogsetLoggerFikaTest {\n\n    @Test\n    public void testSerialize() {\n    }\n}",
      "conditionCount": 6,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.util.Utils.getSerializerTree(org.apache.flink.api.common.typeinfo.TypeInformation)",
      "thirdPartyMethod": "org.apache.commons.lang3.StringUtils.repeat(char, int)",
      "directCaller": "org.apache.flink.util.Utils.getGenericTypeTree(java.lang.Class, int)",
      "path": [
        "org.apache.flink.util.Utils.getSerializerTree(org.apache.flink.api.common.typeinfo.TypeInformation)",
        "org.apache.flink.util.Utils.getSerializerTree(org.apache.flink.api.common.typeinfo.TypeInformation, int)",
        "org.apache.flink.util.Utils.getGenericTypeTree(java.lang.Class, int)",
        "org.apache.commons.lang3.StringUtils.repeat(char, int)"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n/**\n * Debugging utility to understand the hierarchy of serializers created by the Java API. Tested\n * in GroupReduceITCase.testGroupByGenericType()\n */\npublic static <T> String getSerializerTree(TypeInformation<T> ti) {\n    // PATH: Test should invoke the next Utils.getSerializerTree(...) [step in execution path]\n    return Utils.getSerializerTree(ti, 0);\n}",
        "private static <T> String getSerializerTree(TypeInformation<T> ti, int indent) {\n    String ret = \"\";\n    if (ti instanceof CompositeType) {\n        ret += (StringUtils.repeat(' ', indent) + ti.getClass().getSimpleName()) + \"\\n\";\n        CompositeType<T> cti = ((CompositeType<T>) (ti));\n        String[] fieldNames = cti.getFieldNames();\n        for (int i = 0; i < cti.getArity(); i++) {\n            TypeInformation<?> fieldType = cti.getTypeAt(i);\n            ret += ((StringUtils.repeat(' ', indent + 2) + fieldNames[i]) + \":\") + Utils.getSerializerTree(fieldType, indent);\n        }\n    } else if (ti instanceof GenericTypeInfo) {\n        ret += ((StringUtils.repeat(' ', indent) + \"GenericTypeInfo (\") + ti.getTypeClass().getSimpleName()) + \")\\n\";\n        // PATH: Test should invoke the next Utils.getGenericTypeTree(...) [step in execution path]\n        ret += getGenericTypeTree(ti.getTypeClass(), indent + 4);\n    } else {\n        ret += (StringUtils.repeat(' ', indent) + ti.toString()) + \"\\n\";\n    }\n    return ret;\n}",
        "private static String getGenericTypeTree(Class<?> type, int indent) {\n    String ret = \"\";\n    for (Field field : type.getDeclaredFields()) {\n        if (Modifier.isStatic(field.getModifiers()) || Modifier.isTransient(field.getModifiers())) {\n            continue;\n        }\n        // PATH: Test should invoke the next StringUtils.repeat(...) [step in execution path]\n        ret += ((((StringUtils.repeat(' ', indent) + field.getName()) + \":\") + field.getType().getName()) + (field.getType().isEnum() ? \" (is enum)\" : \"\")) + \"\\n\";\n        if (!field.getType().isPrimitive()) {\n            ret += Utils.getGenericTypeTree(field.getType(), indent + 4);\n        }\n    }\n    return ret;\n}"
      ],
      "constructors": [
        "public CountHelper(String id) {\n    this.id = id;\n    this.counter = 0L;\n}",
        "public CollectHelper(String id, TypeSerializer<T> serializer) {\n    this.id = id;\n    this.serializer = serializer;\n}",
        "public ChecksumHashCode() {\n}",
        "public ChecksumHashCode(long count, long checksum) {\n    this.count = count;\n    this.checksum = checksum;\n}",
        "public ChecksumHashCodeHelper(String id) {\n    this.id = id;\n    this.counter = 0L;\n    this.checksum = 0L;\n}",
        "/**\n * Private constructor to prevent instantiation.\n */\nprivate Utils() {\n    throw new RuntimeException();\n}"
      ],
      "fieldDeclarations": [
        "public static final Random RNG = new Random();"
      ],
      "setters": [],
      "imports": [
        "org.apache.commons.lang3.StringUtils",
        "org.apache.flink.api.common.io.RichOutputFormat",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.GenericTypeInfo",
        "org.apache.flink.util.Utils.ChecksumHashCode",
        "org.apache.flink.util.Utils.ChecksumHashCodeHelper",
        "org.apache.flink.util.Utils.CollectHelper",
        "org.apache.flink.util.Utils.CountHelper"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class UtilsgetGenericTypeTree_StringUtilsrepeatFikaTest {\n\n    @Test\n    public void testGetSerializerTree() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo()",
      "thirdPartyMethod": "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@VisibleForTesting\npublic Kryo getKryo() // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n{\n    checkKryoInitialized();\n    return this.kryo;\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        // PATH: Test should invoke the next KryoSerializer.getKryoInstance(...) [step in execution path]\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) // PATH: Test should invoke the next DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(...) [step in execution path]\n    {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializergetKryoInstance_DefaultInstantiatorStrategysetFallbackInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testGetKryo() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo()",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
        "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@VisibleForTesting\npublic Kryo getKryo() // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n{\n    checkKryoInitialized();\n    return this.kryo;\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        // PATH: Test should invoke the next KryoSerializer.getKryoInstance(...) [step in execution path]\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) // PATH: Test should invoke the next Kryo.setInstantiatorStrategy(...) [step in execution path]\n    {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializergetKryoInstance_KryosetInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testGetKryo() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
      "thirdPartyMethod": "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        // PATH: Test should invoke the next KryoSerializer.getKryoInstance(...) [step in execution path]\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) // PATH: Test should invoke the next DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(...) [step in execution path]\n    {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializergetKryoInstance_DefaultInstantiatorStrategysetFallbackInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testCreateInstance() {\n    }\n}",
      "conditionCount": 8,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
        "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        // PATH: Test should invoke the next KryoSerializer.getKryoInstance(...) [step in execution path]\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) // PATH: Test should invoke the next Kryo.setInstantiatorStrategy(...) [step in execution path]\n    {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializergetKryoInstance_KryosetInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testCreateInstance() {\n    }\n}",
      "conditionCount": 8,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        // PATH: Test should invoke the next KryoSerializer.getKryoInstance(...) [step in execution path]\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) // PATH: Test should invoke the next DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(...) [step in execution path]\n    {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializergetKryoInstance_DefaultInstantiatorStrategysetFallbackInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 10,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(org.apache.flink.core.memory.DataInputView, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(org.apache.flink.core.memory.DataInputView, org.apache.flink.core.memory.DataOutputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        // PATH: Test should invoke the next KryoSerializer.getKryoInstance(...) [step in execution path]\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) // PATH: Test should invoke the next DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(...) [step in execution path]\n    {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializergetKryoInstance_DefaultInstantiatorStrategysetFallbackInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 10,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
        "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        // PATH: Test should invoke the next KryoSerializer.getKryoInstance(...) [step in execution path]\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) // PATH: Test should invoke the next Kryo.setInstantiatorStrategy(...) [step in execution path]\n    {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializergetKryoInstance_KryosetInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 10,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(org.apache.flink.core.memory.DataInputView, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(org.apache.flink.core.memory.DataInputView, org.apache.flink.core.memory.DataOutputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
        "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        // PATH: Test should invoke the next KryoSerializer.getKryoInstance(...) [step in execution path]\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) // PATH: Test should invoke the next Kryo.setInstantiatorStrategy(...) [step in execution path]\n    {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializergetKryoInstance_KryosetInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 10,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertValue(java.lang.Object, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertValue(java.lang.Object, java.lang.Class)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle(org.snakeyaml.engine.v2.common.FlowStyle)"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpSettingsBuildersetDefaultFlowStyleFikaTest {\n\n    @Test\n    public void testConvertValue() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertValue(java.lang.Object, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertValue(java.lang.Object, java.lang.Class)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testConvertValue() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        // PATH: Test should invoke the next KryoSerializer.getKryoInstance(...) [step in execution path]\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) // PATH: Test should invoke the next DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(...) [step in execution path]\n    {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializergetKryoInstance_DefaultInstantiatorStrategysetFallbackInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testDeserialize() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
        "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        // PATH: Test should invoke the next KryoSerializer.getKryoInstance(...) [step in execution path]\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) // PATH: Test should invoke the next Kryo.setInstantiatorStrategy(...) [step in execution path]\n    {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializergetKryoInstance_KryosetInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testDeserialize() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertValue(java.lang.Object, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertValue(java.lang.Object, java.lang.Class)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Dump.<init>(org.snakeyaml.engine.v2.api.DumpSettings, org.snakeyaml.engine.v2.representer.BaseRepresenter)"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_DumpmethodFikaTest {\n\n    @Test\n    public void testConvertValue() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertValue(java.lang.Object, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertValue(java.lang.Object, java.lang.Class)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema(org.snakeyaml.engine.v2.schema.Schema)"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_LoadSettingsBuildersetSchemaFikaTest {\n\n    @Test\n    public void testConvertValue() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.configuration.ConfigurationUtils.convertValue(java.lang.Object, java.lang.Class)",
      "thirdPartyMethod": "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)",
      "directCaller": "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
      "path": [
        "org.apache.flink.configuration.ConfigurationUtils.convertValue(java.lang.Object, java.lang.Class)",
        "org.apache.flink.configuration.ConfigurationUtils.convertToString(java.lang.Object)",
        "org.apache.flink.configuration.YamlParserUtils.<clinit>()",
        "org.snakeyaml.engine.v2.api.Load.<init>(org.snakeyaml.engine.v2.api.LoadSettings)"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        // PATH: Test should invoke the next ConfigurationUtils.convertToString(...) [step in execution path]\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}",
        "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}",
        "org.apache.flink.configuration.YamlParserUtils\n\n// Static field initializations\nprivate static final Logger LOG = LoggerFactory.getLogger(YamlParserUtils.class);\nprivate static final DumpSettings blockerDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.BLOCK).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final DumpSettings flowDumperSettings = // Disable split long lines to avoid add unexpected line breaks\nDumpSettings.builder().setDefaultFlowStyle(FlowStyle.FLOW).setSplitLines(false).setSchema(new CoreSchema()).build();\nprivate static final Dump blockerDumper = new Dump(blockerDumperSettings, new FlinkConfigRepresenter(blockerDumperSettings));\nprivate static final Dump flowDumper = new Dump(flowDumperSettings, new FlinkConfigRepresenter(flowDumperSettings));\nprivate static final Load loader = new Load(LoadSettings.builder().setSchema(new CoreSchema()).build());\n"
      ],
      "constructors": [
        "// Make sure that we cannot instantiate this class\nprivate ConfigurationUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final String[] EMPTY = new String[0];"
      ],
      "setters": [],
      "imports": [
        "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory",
        "org.snakeyaml.engine.v2.api.Dump",
        "org.snakeyaml.engine.v2.api.DumpSettings",
        "org.snakeyaml.engine.v2.api.DumpSettingsBuilder",
        "org.snakeyaml.engine.v2.api.Load",
        "org.snakeyaml.engine.v2.api.LoadSettings",
        "org.snakeyaml.engine.v2.api.LoadSettingsBuilder",
        "org.snakeyaml.engine.v2.common.FlowStyle",
        "org.snakeyaml.engine.v2.representer.BaseRepresenter",
        "org.snakeyaml.engine.v2.schema.CoreSchema",
        "org.snakeyaml.engine.v2.schema.Schema"
      ],
      "testTemplate": "package org.apache.flink.configuration;\n\npublic class ConfigurationUtils_YamlParserUtilsmethod_LoadmethodFikaTest {\n\n    @Test\n    public void testConvertValue() {\n    }\n}",
      "conditionCount": 11,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        // PATH: Test should invoke the next KryoSerializer.getKryoInstance(...) [step in execution path]\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) // PATH: Test should invoke the next DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy(...) [step in execution path]\n    {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializergetKryoInstance_DefaultInstantiatorStrategysetFallbackInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testSerialize() {\n    }\n}",
      "conditionCount": 12,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance()",
        "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(org.objenesis.strategy.InstantiatorStrategy)"
      ],
      "methodSources": [
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        // PATH: Test should invoke the next KryoSerializer.getKryoInstance(...) [step in execution path]\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) // PATH: Test should invoke the next Kryo.setInstantiatorStrategy(...) [step in execution path]\n    {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializergetKryoInstance_KryosetInstantiatorStrategyFikaTest {\n\n    @Test\n    public void testSerialize() {\n    }\n}",
      "conditionCount": 12,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.tar.TarArchiveEntry.getName()",
      "directCaller": "org.apache.flink.util.CompressionUtils.unpackEntry(org.apache.commons.compress.archivers.tar.TarArchiveInputStream, org.apache.commons.compress.archivers.tar.TarArchiveEntry, java.io.File)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
        "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava(java.lang.String, java.lang.String, boolean)",
        "org.apache.flink.util.CompressionUtils.unpackEntry(org.apache.commons.compress.archivers.tar.TarArchiveInputStream, org.apache.commons.compress.archivers.tar.TarArchiveEntry, java.io.File)",
        "org.apache.commons.compress.archivers.tar.TarArchiveEntry.getName()"
      ],
      "methodSources": [
        "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else // PATH: Test should invoke the next CompressionUtils.extractTarFileUsingJava(...) [step in execution path]\n    {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}",
        "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) // PATH: Test should invoke the next CompressionUtils.unpackEntry(...) [step in execution path]\n        {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}",
        "private static void unpackEntry(TarArchiveInputStream tis, TarArchiveEntry entry, File targetDir) throws IOException {\n    String targetDirPath = targetDir.getCanonicalPath() + File.separator;\n    File outputFile = // PATH: Test should invoke the next TarArchiveEntry.getName(...) [step in execution path]\n    new File(targetDir, entry.getName());\n    if (!outputFile.getCanonicalPath().startsWith(targetDirPath)) {\n        throw new IOException(((\"expanding \" + entry.getName()) + \" would create entry outside of \") + targetDir);\n    }\n    if (entry.isDirectory()) {\n        if ((!outputFile.mkdirs()) && (!outputFile.isDirectory())) {\n            throw new IOException(\"Failed to create directory \" + outputFile);\n        }\n        for (TarArchiveEntry e : entry.getDirectoryEntries()) {\n            CompressionUtils.unpackEntry(tis, e, outputFile);\n        }\n        return;\n    }\n    if (entry.isSymbolicLink()) {\n        // create symbolic link relative to tar parent dir\n        Files.createSymbolicLink(Paths.get(new File(targetDir, entry.getName()).getCanonicalPath()), Paths.get(entry.getLinkName()));\n        return;\n    }\n    if (!outputFile.getParentFile().exists()) {\n        if (!outputFile.getParentFile().mkdirs()) {\n            throw new IOException(\"Mkdirs failed to create tar internal dir \" + targetDir);\n        }\n    }\n    try (OutputStream o = Files.newOutputStream(Paths.get(outputFile.getCanonicalPath()))) {\n        IOUtils.copyBytes(tis, o, false);\n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.tar.TarArchiveEntry",
        "org.apache.commons.compress.archivers.tar.TarArchiveInputStream",
        "org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsunpackEntry_TarArchiveEntrygetNameFikaTest {\n\n    @Test\n    public void testExtractTarFile() {\n    }\n}",
      "conditionCount": 12,
      "callCount": 3,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.tar.TarArchiveEntry.isSymbolicLink()",
      "directCaller": "org.apache.flink.util.CompressionUtils.unpackEntry(org.apache.commons.compress.archivers.tar.TarArchiveInputStream, org.apache.commons.compress.archivers.tar.TarArchiveEntry, java.io.File)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
        "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava(java.lang.String, java.lang.String, boolean)",
        "org.apache.flink.util.CompressionUtils.unpackEntry(org.apache.commons.compress.archivers.tar.TarArchiveInputStream, org.apache.commons.compress.archivers.tar.TarArchiveEntry, java.io.File)",
        "org.apache.commons.compress.archivers.tar.TarArchiveEntry.isSymbolicLink()"
      ],
      "methodSources": [
        "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else // PATH: Test should invoke the next CompressionUtils.extractTarFileUsingJava(...) [step in execution path]\n    {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}",
        "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) // PATH: Test should invoke the next CompressionUtils.unpackEntry(...) [step in execution path]\n        {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}",
        "private static void unpackEntry(TarArchiveInputStream tis, TarArchiveEntry entry, File targetDir) throws IOException {\n    String targetDirPath = targetDir.getCanonicalPath() + File.separator;\n    File outputFile = new File(targetDir, entry.getName());\n    if (!outputFile.getCanonicalPath().startsWith(targetDirPath)) {\n        throw new IOException(((\"expanding \" + entry.getName()) + \" would create entry outside of \") + targetDir);\n    }\n    if (entry.isDirectory()) {\n        if ((!outputFile.mkdirs()) && (!outputFile.isDirectory())) {\n            throw new IOException(\"Failed to create directory \" + outputFile);\n        }\n        for (TarArchiveEntry e : entry.getDirectoryEntries()) {\n            CompressionUtils.unpackEntry(tis, e, outputFile);\n        }\n        return;\n    }\n    // PATH: Test should invoke the next TarArchiveEntry.isSymbolicLink(...) [step in execution path]\n    if (entry.isSymbolicLink()) {\n        // create symbolic link relative to tar parent dir\n        Files.createSymbolicLink(Paths.get(new File(targetDir, entry.getName()).getCanonicalPath()), Paths.get(entry.getLinkName()));\n        return;\n    }\n    if (!outputFile.getParentFile().exists()) {\n        if (!outputFile.getParentFile().mkdirs()) {\n            throw new IOException(\"Mkdirs failed to create tar internal dir \" + targetDir);\n        }\n    }\n    try (OutputStream o = Files.newOutputStream(Paths.get(outputFile.getCanonicalPath()))) {\n        IOUtils.copyBytes(tis, o, false);\n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.tar.TarArchiveEntry",
        "org.apache.commons.compress.archivers.tar.TarArchiveInputStream",
        "org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsunpackEntry_TarArchiveEntryisSymbolicLinkFikaTest {\n\n    @Test\n    public void testExtractTarFile() {\n    }\n}",
      "conditionCount": 12,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.tar.TarArchiveEntry.getLinkName()",
      "directCaller": "org.apache.flink.util.CompressionUtils.unpackEntry(org.apache.commons.compress.archivers.tar.TarArchiveInputStream, org.apache.commons.compress.archivers.tar.TarArchiveEntry, java.io.File)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
        "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava(java.lang.String, java.lang.String, boolean)",
        "org.apache.flink.util.CompressionUtils.unpackEntry(org.apache.commons.compress.archivers.tar.TarArchiveInputStream, org.apache.commons.compress.archivers.tar.TarArchiveEntry, java.io.File)",
        "org.apache.commons.compress.archivers.tar.TarArchiveEntry.getLinkName()"
      ],
      "methodSources": [
        "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else // PATH: Test should invoke the next CompressionUtils.extractTarFileUsingJava(...) [step in execution path]\n    {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}",
        "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) // PATH: Test should invoke the next CompressionUtils.unpackEntry(...) [step in execution path]\n        {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}",
        "private static void unpackEntry(TarArchiveInputStream tis, TarArchiveEntry entry, File targetDir) throws IOException {\n    String targetDirPath = targetDir.getCanonicalPath() + File.separator;\n    File outputFile = new File(targetDir, entry.getName());\n    if (!outputFile.getCanonicalPath().startsWith(targetDirPath)) {\n        throw new IOException(((\"expanding \" + entry.getName()) + \" would create entry outside of \") + targetDir);\n    }\n    if (entry.isDirectory()) {\n        if ((!outputFile.mkdirs()) && (!outputFile.isDirectory())) {\n            throw new IOException(\"Failed to create directory \" + outputFile);\n        }\n        for (TarArchiveEntry e : entry.getDirectoryEntries()) {\n            CompressionUtils.unpackEntry(tis, e, outputFile);\n        }\n        return;\n    }\n    if (entry.isSymbolicLink()) {\n        // create symbolic link relative to tar parent dir\n        Files.createSymbolicLink(Paths.get(new File(targetDir, entry.getName()).getCanonicalPath()), // PATH: Test should invoke the next TarArchiveEntry.getLinkName(...) [step in execution path]\n        Paths.get(entry.getLinkName()));\n        return;\n    }\n    if (!outputFile.getParentFile().exists()) {\n        if (!outputFile.getParentFile().mkdirs()) {\n            throw new IOException(\"Mkdirs failed to create tar internal dir \" + targetDir);\n        }\n    }\n    try (OutputStream o = Files.newOutputStream(Paths.get(outputFile.getCanonicalPath()))) {\n        IOUtils.copyBytes(tis, o, false);\n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.tar.TarArchiveEntry",
        "org.apache.commons.compress.archivers.tar.TarArchiveInputStream",
        "org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsunpackEntry_TarArchiveEntrygetLinkNameFikaTest {\n\n    @Test\n    public void testExtractTarFile() {\n    }\n}",
      "conditionCount": 12,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.tar.TarArchiveEntry.isDirectory()",
      "directCaller": "org.apache.flink.util.CompressionUtils.unpackEntry(org.apache.commons.compress.archivers.tar.TarArchiveInputStream, org.apache.commons.compress.archivers.tar.TarArchiveEntry, java.io.File)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
        "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava(java.lang.String, java.lang.String, boolean)",
        "org.apache.flink.util.CompressionUtils.unpackEntry(org.apache.commons.compress.archivers.tar.TarArchiveInputStream, org.apache.commons.compress.archivers.tar.TarArchiveEntry, java.io.File)",
        "org.apache.commons.compress.archivers.tar.TarArchiveEntry.isDirectory()"
      ],
      "methodSources": [
        "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else // PATH: Test should invoke the next CompressionUtils.extractTarFileUsingJava(...) [step in execution path]\n    {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}",
        "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) // PATH: Test should invoke the next CompressionUtils.unpackEntry(...) [step in execution path]\n        {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}",
        "private static void unpackEntry(TarArchiveInputStream tis, TarArchiveEntry entry, File targetDir) throws IOException {\n    String targetDirPath = targetDir.getCanonicalPath() + File.separator;\n    File outputFile = new File(targetDir, entry.getName());\n    if (!outputFile.getCanonicalPath().startsWith(targetDirPath)) {\n        throw new IOException(((\"expanding \" + entry.getName()) + \" would create entry outside of \") + targetDir);\n    }\n    // PATH: Test should invoke the next TarArchiveEntry.isDirectory(...) [step in execution path]\n    if (entry.isDirectory()) {\n        if ((!outputFile.mkdirs()) && (!outputFile.isDirectory())) {\n            throw new IOException(\"Failed to create directory \" + outputFile);\n        }\n        for (TarArchiveEntry e : entry.getDirectoryEntries()) {\n            CompressionUtils.unpackEntry(tis, e, outputFile);\n        }\n        return;\n    }\n    if (entry.isSymbolicLink()) {\n        // create symbolic link relative to tar parent dir\n        Files.createSymbolicLink(Paths.get(new File(targetDir, entry.getName()).getCanonicalPath()), Paths.get(entry.getLinkName()));\n        return;\n    }\n    if (!outputFile.getParentFile().exists()) {\n        if (!outputFile.getParentFile().mkdirs()) {\n            throw new IOException(\"Mkdirs failed to create tar internal dir \" + targetDir);\n        }\n    }\n    try (OutputStream o = Files.newOutputStream(Paths.get(outputFile.getCanonicalPath()))) {\n        IOUtils.copyBytes(tis, o, false);\n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.tar.TarArchiveEntry",
        "org.apache.commons.compress.archivers.tar.TarArchiveInputStream",
        "org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsunpackEntry_TarArchiveEntryisDirectoryFikaTest {\n\n    @Test\n    public void testExtractTarFile() {\n    }\n}",
      "conditionCount": 12,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
      "thirdPartyMethod": "org.apache.commons.compress.archivers.tar.TarArchiveEntry.getDirectoryEntries()",
      "directCaller": "org.apache.flink.util.CompressionUtils.unpackEntry(org.apache.commons.compress.archivers.tar.TarArchiveInputStream, org.apache.commons.compress.archivers.tar.TarArchiveEntry, java.io.File)",
      "path": [
        "org.apache.flink.util.CompressionUtils.extractTarFile(java.lang.String, java.lang.String)",
        "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava(java.lang.String, java.lang.String, boolean)",
        "org.apache.flink.util.CompressionUtils.unpackEntry(org.apache.commons.compress.archivers.tar.TarArchiveInputStream, org.apache.commons.compress.archivers.tar.TarArchiveEntry, java.io.File)",
        "org.apache.commons.compress.archivers.tar.TarArchiveEntry.getDirectoryEntries()"
      ],
      "methodSources": [
        "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else // PATH: Test should invoke the next CompressionUtils.extractTarFileUsingJava(...) [step in execution path]\n    {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}",
        "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) // PATH: Test should invoke the next CompressionUtils.unpackEntry(...) [step in execution path]\n        {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}",
        "private static void unpackEntry(TarArchiveInputStream tis, TarArchiveEntry entry, File targetDir) throws IOException {\n    String targetDirPath = targetDir.getCanonicalPath() + File.separator;\n    File outputFile = new File(targetDir, entry.getName());\n    if (!outputFile.getCanonicalPath().startsWith(targetDirPath)) {\n        throw new IOException(((\"expanding \" + entry.getName()) + \" would create entry outside of \") + targetDir);\n    }\n    if (entry.isDirectory()) {\n        if ((!outputFile.mkdirs()) && (!outputFile.isDirectory())) {\n            throw new IOException(\"Failed to create directory \" + outputFile);\n        }\n        // PATH: Test should invoke the next TarArchiveEntry.getDirectoryEntries(...) [step in execution path]\n        for (TarArchiveEntry e : entry.getDirectoryEntries()) {\n            CompressionUtils.unpackEntry(tis, e, outputFile);\n        }\n        return;\n    }\n    if (entry.isSymbolicLink()) {\n        // create symbolic link relative to tar parent dir\n        Files.createSymbolicLink(Paths.get(new File(targetDir, entry.getName()).getCanonicalPath()), Paths.get(entry.getLinkName()));\n        return;\n    }\n    if (!outputFile.getParentFile().exists()) {\n        if (!outputFile.getParentFile().mkdirs()) {\n            throw new IOException(\"Mkdirs failed to create tar internal dir \" + targetDir);\n        }\n    }\n    try (OutputStream o = Files.newOutputStream(Paths.get(outputFile.getCanonicalPath()))) {\n        IOUtils.copyBytes(tis, o, false);\n    }\n}"
      ],
      "constructors": [
        "CompressionUtils() {\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(CompressionUtils.class);"
      ],
      "setters": [
        "public static void extractFile(String srcFilePath, String targetDirPath, String originalFileName) throws IOException {\n    if (hasOneOfSuffixes(originalFileName, \".zip\", \".jar\")) {\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    } else if (hasOneOfSuffixes(originalFileName, \".tar\", \".tar.gz\", \".tgz\")) {\n        extractTarFile(srcFilePath, targetDirPath);\n    } else {\n        LOG.warn(\"Only zip, jar, tar, tgz and tar.gz suffixes are supported, found {}. Trying to extract it as zip file.\", originalFileName);\n        extractZipFileWithPermissions(srcFilePath, targetDirPath);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.compress.archivers.tar.TarArchiveEntry",
        "org.apache.commons.compress.archivers.tar.TarArchiveInputStream",
        "org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream",
        "org.slf4j.Logger"
      ],
      "testTemplate": "package org.apache.flink.util;\n\npublic class CompressionUtilsunpackEntry_TarArchiveEntrygetDirectoryEntriesFikaTest {\n\n    @Test\n    public void testExtractTarFile() {\n    }\n}",
      "conditionCount": 12,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.ExecutionConfig.configure(org.apache.flink.configuration.ReadableConfig, java.lang.ClassLoader)",
      "thirdPartyMethod": "org.apache.commons.compress.utils.Sets.newHashSet(java.lang.Object[])",
      "directCaller": "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<init>(java.lang.String, int, java.lang.String, java.util.Set)",
      "path": [
        "org.apache.flink.api.common.ExecutionConfig.configure(org.apache.flink.configuration.ReadableConfig, java.lang.ClassLoader)",
        "org.apache.flink.configuration.RestartStrategyOptions.<clinit>()",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<clinit>()",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<init>(java.lang.String, int, java.lang.String, java.util.Set)",
        "org.apache.commons.compress.utils.Sets.newHashSet(java.lang.Object[])"
      ],
      "methodSources": [
        "/**\n * Sets all relevant options contained in the {@link ReadableConfig} such as e.g. {@link PipelineOptions#CLOSURE_CLEANER_LEVEL}.\n *\n * <p>It will change the value of a setting only if a corresponding option was set in the {@code configuration}. If a key is not present, the current value of a field will remain untouched.\n *\n * @param configuration\n * \t\ta configuration to read the values from\n * @param classLoader\n * \t\ta class loader to use when loading classes\n */\npublic void configure(ReadableConfig configuration, ClassLoader classLoader) {\n    configuration.getOptional(PipelineOptions.AUTO_GENERATE_UIDS).ifPresent(this::setAutoGeneratedUids);\n    configuration.getOptional(PipelineOptions.AUTO_WATERMARK_INTERVAL).ifPresent(this::setAutoWatermarkInterval);\n    configuration.getOptional(PipelineOptions.CLOSURE_CLEANER_LEVEL).ifPresent(this::setClosureCleanerLevel);\n    configuration.getOptional(PipelineOptions.GLOBAL_JOB_PARAMETERS).ifPresent(this::setGlobalJobParameters);\n    configuration.getOptional(MetricOptions.LATENCY_INTERVAL).ifPresent(interval -> setLatencyTrackingInterval(interval.toMillis()));\n    configuration.getOptional(StateChangelogOptions.PERIODIC_MATERIALIZATION_ENABLED).ifPresent(this::enablePeriodicMaterialize);\n    configuration.getOptional(StateChangelogOptions.PERIODIC_MATERIALIZATION_INTERVAL).ifPresent(this::setPeriodicMaterializeIntervalMillis);\n    configuration.getOptional(StateChangelogOptions.MATERIALIZATION_MAX_FAILURES_ALLOWED).ifPresent(this::setMaterializationMaxAllowedFailures);\n    configuration.getOptional(PipelineOptions.MAX_PARALLELISM).ifPresent(this::setMaxParallelism);\n    configuration.getOptional(CoreOptions.DEFAULT_PARALLELISM).ifPresent(this::setParallelism);\n    configuration.getOptional(PipelineOptions.OBJECT_REUSE).ifPresent(this::setObjectReuse);\n    configuration.getOptional(TaskManagerOptions.TASK_CANCELLATION_INTERVAL).ifPresent(interval -> setTaskCancellationInterval(interval.toMillis()));\n    configuration.getOptional(TaskManagerOptions.TASK_CANCELLATION_TIMEOUT).ifPresent(timeout -> setTaskCancellationTimeout(timeout.toMillis()));\n    configuration.getOptional(ExecutionOptions.SNAPSHOT_COMPRESSION).ifPresent(this::setUseSnapshotCompression);\n    configuration.getOptional(RestartStrategyOptions.RESTART_STRATEGY).ifPresent(s -> this.setRestartStrategy(configuration));\n    configuration.getOptional(JobManagerOptions.SCHEDULER).ifPresent(t -> this.configuration.set(JobManagerOptions.SCHEDULER, t));\n    serializerConfig.configure(configuration, classLoader);\n}",
        "org.apache.flink.configuration.RestartStrategyOptions\n\n// Static field initializations\n@Internal\npublic static final String RESTART_STRATEGY_CONFIG_PREFIX = \"restart-strategy\";\npublic static final ConfigOption<String> RESTART_STRATEGY = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".type\").stringType().noDefaultValue().withDeprecatedKeys(RESTART_STRATEGY_CONFIG_PREFIX).withDescription(Description.builder().text(\"Defines the restart strategy to use in case of job failures.\").linebreak().text(\"Accepted values are:\").list(text(\"%s, %s, %s: No restart strategy.\", NO_RESTART_STRATEGY.getAllTextElement()), text(\"%s, %s: Fixed delay restart strategy. More details can be found %s.\", concat(FIXED_DELAY.getAllTextElement(), link(\"{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#fixed-delay-restart-strategy\", \"here\"))), text(\"%s, %s: Failure rate restart strategy. More details can be found %s.\", concat(FAILURE_RATE.getAllTextElement(), link(\"{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#failure-rate-restart-strategy\", \"here\"))), text(\"%s, %s: Exponential delay restart strategy. More details can be found %s.\", concat(EXPONENTIAL_DELAY.getAllTextElement(), link(\"{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#exponential-delay-restart-strategy\", \"here\")))).text(\"If checkpointing is disabled, the default value is %s. \" + \"If checkpointing is enabled, the default value is %s, and the default values of %s related config options will be used.\", code(NO_RESTART_STRATEGY.getMainValue()), code(EXPONENTIAL_DELAY.getMainValue()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Integer> RESTART_STRATEGY_FIXED_DELAY_ATTEMPTS = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".fixed-delay.attempts\").intType().defaultValue(1).withDescription(Description.builder().text(\"The number of times that Flink retries the execution before the job is declared as failed if %s has been set to %s.\", code(RESTART_STRATEGY.key()), code(FIXED_DELAY.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_FIXED_DELAY_DELAY = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".fixed-delay.delay\").durationType().defaultValue(Duration.ofSeconds(1)).withDescription(Description.builder().text(((\"Delay between two consecutive restart attempts if %s has been set to %s. \" + \"Delaying the retries can be helpful when the program interacts with external systems where \") + \"for example connections or pending transactions should reach a timeout before re-execution \") + \"is attempted. It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(FIXED_DELAY.getMainValue())).build());\npublic static final ConfigOption<Integer> RESTART_STRATEGY_FAILURE_RATE_MAX_FAILURES_PER_INTERVAL = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".failure-rate.max-failures-per-interval\").intType().defaultValue(1).withDescription(Description.builder().text(\"Maximum number of restarts in given time interval before failing a job if %s has been set to %s.\", code(RESTART_STRATEGY.key()), code(FAILURE_RATE.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_FAILURE_RATE_FAILURE_RATE_INTERVAL = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".failure-rate.failure-rate-interval\").durationType().defaultValue(Duration.ofMinutes(1)).withDescription(Description.builder().text(\"Time interval for measuring failure rate if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(FAILURE_RATE.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_FAILURE_RATE_DELAY = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".failure-rate.delay\").durationType().defaultValue(Duration.ofSeconds(1)).withDescription(Description.builder().text(\"Delay between two consecutive restart attempts if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(FAILURE_RATE.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_EXPONENTIAL_DELAY_INITIAL_BACKOFF = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.initial-backoff\").durationType().defaultValue(Duration.ofSeconds(1)).withDescription(Description.builder().text(\"Starting duration between restarts if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_EXPONENTIAL_DELAY_MAX_BACKOFF = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.max-backoff\").durationType().defaultValue(Duration.ofMinutes(1)).withDescription(Description.builder().text(\"The highest possible duration between restarts if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Double> RESTART_STRATEGY_EXPONENTIAL_DELAY_BACKOFF_MULTIPLIER = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.backoff-multiplier\").doubleType().defaultValue(1.5).withDescription(Description.builder().text(\"Backoff value is multiplied by this value after every failure,\" + \"until max backoff is reached if %s has been set to %s.\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_EXPONENTIAL_DELAY_RESET_BACKOFF_THRESHOLD = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.reset-backoff-threshold\").durationType().defaultValue(Duration.ofHours(1)).withDescription(Description.builder().text(((\"Threshold when the backoff is reset to its initial value if %s has been set to %s. \" + \"It specifies how long the job must be running without failure \") + \"to reset the exponentially increasing backoff to its initial value. \") + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Double> RESTART_STRATEGY_EXPONENTIAL_DELAY_JITTER_FACTOR = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.jitter-factor\").doubleType().defaultValue(0.1).withDescription(Description.builder().text((\"Jitter specified as a portion of the backoff if %s has been set to %s. \" + \"It represents how large random value will be added or subtracted to the backoff. \") + \"Useful when you want to avoid restarting multiple jobs at the same time.\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\n@Documentation.OverrideDefault(\"infinite\")\npublic static final ConfigOption<Integer> RESTART_STRATEGY_EXPONENTIAL_DELAY_ATTEMPTS = ConfigOptions.key(\"restart-strategy.exponential-delay.attempts-before-reset-backoff\").intType().defaultValue(Integer.MAX_VALUE).withDescription(Description.builder().text(\"The number of times that Flink retries the execution before failing the job if %s has been set to %s. \" + \"The number will be reset once the backoff is reset to its initial value.\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\n",
        "org.apache.flink.configuration.RestartStrategyOptions$RestartStrategyType\n\n// Static field initializations\nNO_RESTART_STRATEGY(\"disable\", Sets.newHashSet(\"none\", \"off\"))\nFIXED_DELAY(\"fixed-delay\", Sets.newHashSet(\"fixeddelay\"))\nFAILURE_RATE(\"failure-rate\", Sets.newHashSet(\"failurerate\"))\nEXPONENTIAL_DELAY(\"exponential-delay\", Sets.newHashSet(\"exponentialdelay\"))\n",
        "(String mainValue, Set<String> otherAvailableValues) {\n    this.mainValue = mainValue;\n    // PATH: Test should invoke the next Sets.newHashSet(...) [step in execution path]\n    this.allAvailableValues = Sets.newHashSet(mainValue);\n    allAvailableValues.addAll(otherAvailableValues);\n}"
      ],
      "constructors": [
        "public ExecutionConfig() {\n    this(new Configuration());\n}",
        "@Internal\npublic ExecutionConfig(Configuration configuration) {\n    this.configuration = configuration;\n    this.serializerConfig = new SerializerConfigImpl(configuration);\n}",
        "GlobalJobParameters() {\n}",
        "ClosureCleanerLevel(InlineElement description) {\n    this.description = description;\n}",
        "private MapBasedJobParameters(Map<String, String> properties) {\n    this.properties = properties;\n}"
      ],
      "fieldDeclarations": [
        "// NOTE TO IMPLEMENTERS:\n// Please do not add further fields to this class. Use the ConfigOption stack instead!\n// It is currently very tricky to keep this kind of POJO classes in sync with instances of\n// org.apache.flink.configuration.Configuration. Instances of Configuration are way easier to\n// pass, layer, merge, restrict, copy, filter, etc.\n// See ExecutionOptions.RUNTIME_MODE for a reference implementation. If the option is very\n// crucial for the API, we can add a dedicated setter to StreamExecutionEnvironment. Otherwise,\n// introducing a ConfigOption should be enough.\nprivate static final long serialVersionUID = 1L;",
        "/**\n * The flag value indicating use of the default parallelism. This value can be used to reset the\n * parallelism back to the default state.\n */\npublic static final int PARALLELISM_DEFAULT = -1;",
        "/**\n * The flag value indicating an unknown or unset parallelism. This value is not a valid\n * parallelism and indicates that the parallelism should remain unchanged.\n */\npublic static final int PARALLELISM_UNKNOWN = -2;",
        "// --------------------------------------------------------------------------------------------\n/**\n * In the long run, this field should be somehow merged with the {@link Configuration} from\n * StreamExecutionEnvironment.\n */\nprivate final Configuration configuration;",
        "private final SerializerConfig serializerConfig;"
      ],
      "setters": [
        "/**\n * Sets all relevant options contained in the {@link ReadableConfig} such as e.g. {@link PipelineOptions#CLOSURE_CLEANER_LEVEL}.\n *\n * <p>It will change the value of a setting only if a corresponding option was set in the {@code configuration}. If a key is not present, the current value of a field will remain untouched.\n *\n * @param configuration\n * \t\ta configuration to read the values from\n * @param classLoader\n * \t\ta class loader to use when loading classes\n */\npublic void configure(ReadableConfig configuration, ClassLoader classLoader) {\n    configuration.getOptional(PipelineOptions.AUTO_GENERATE_UIDS).ifPresent(this::setAutoGeneratedUids);\n    configuration.getOptional(PipelineOptions.AUTO_WATERMARK_INTERVAL).ifPresent(this::setAutoWatermarkInterval);\n    configuration.getOptional(PipelineOptions.CLOSURE_CLEANER_LEVEL).ifPresent(this::setClosureCleanerLevel);\n    configuration.getOptional(PipelineOptions.GLOBAL_JOB_PARAMETERS).ifPresent(this::setGlobalJobParameters);\n    configuration.getOptional(MetricOptions.LATENCY_INTERVAL).ifPresent(interval -> setLatencyTrackingInterval(interval.toMillis()));\n    configuration.getOptional(StateChangelogOptions.PERIODIC_MATERIALIZATION_ENABLED).ifPresent(this::enablePeriodicMaterialize);\n    configuration.getOptional(StateChangelogOptions.PERIODIC_MATERIALIZATION_INTERVAL).ifPresent(this::setPeriodicMaterializeIntervalMillis);\n    configuration.getOptional(StateChangelogOptions.MATERIALIZATION_MAX_FAILURES_ALLOWED).ifPresent(this::setMaterializationMaxAllowedFailures);\n    configuration.getOptional(PipelineOptions.MAX_PARALLELISM).ifPresent(this::setMaxParallelism);\n    configuration.getOptional(CoreOptions.DEFAULT_PARALLELISM).ifPresent(this::setParallelism);\n    configuration.getOptional(PipelineOptions.OBJECT_REUSE).ifPresent(this::setObjectReuse);\n    configuration.getOptional(TaskManagerOptions.TASK_CANCELLATION_INTERVAL).ifPresent(interval -> setTaskCancellationInterval(interval.toMillis()));\n    configuration.getOptional(TaskManagerOptions.TASK_CANCELLATION_TIMEOUT).ifPresent(timeout -> setTaskCancellationTimeout(timeout.toMillis()));\n    configuration.getOptional(ExecutionOptions.SNAPSHOT_COMPRESSION).ifPresent(this::setUseSnapshotCompression);\n    configuration.getOptional(RestartStrategyOptions.RESTART_STRATEGY).ifPresent(s -> this.setRestartStrategy(configuration));\n    configuration.getOptional(JobManagerOptions.SCHEDULER).ifPresent(t -> this.configuration.set(JobManagerOptions.SCHEDULER, t));\n    serializerConfig.configure(configuration, classLoader);\n}",
        "@Internal\npublic void enablePeriodicMaterialize(boolean enabled) {\n    configuration.set(StateChangelogOptions.PERIODIC_MATERIALIZATION_ENABLED, enabled);\n}",
        "@Internal\npublic void resetParallelism() {\n    configuration.removeConfig(CoreOptions.DEFAULT_PARALLELISM);\n}",
        "private void setAutoGeneratedUids(boolean autoGeneratedUids) {\n    configuration.set(PipelineOptions.AUTO_GENERATE_UIDS, autoGeneratedUids);\n}",
        "private void setGlobalJobParameters(Map<String, String> parameters) {\n    configuration.set(PipelineOptions.GLOBAL_JOB_PARAMETERS, parameters);\n}",
        "@Internal\npublic void setMaterializationMaxAllowedFailures(int materializationMaxAllowedFailures) {\n    configuration.set(StateChangelogOptions.MATERIALIZATION_MAX_FAILURES_ALLOWED, materializationMaxAllowedFailures);\n}",
        "/**\n * Sets the maximum degree of parallelism defined for the program.\n *\n * <p>The maximum degree of parallelism specifies the upper limit for dynamic scaling. It also\n * defines the number of key groups used for partitioned state.\n *\n * @param maxParallelism\n * \t\tMaximum degree of parallelism to be used for the program.\n */\n@PublicEvolving\npublic void setMaxParallelism(int maxParallelism) {\n    checkArgument(maxParallelism > 0, \"The maximum parallelism must be greater than 0.\");\n    configuration.set(PipelineOptions.MAX_PARALLELISM, maxParallelism);\n}",
        "@Internal\npublic void setPeriodicMaterializeIntervalMillis(Duration periodicMaterializeInterval) {\n    configuration.set(StateChangelogOptions.PERIODIC_MATERIALIZATION_INTERVAL, periodicMaterializeInterval);\n}",
        "private void setRestartStrategy(ReadableConfig configuration) {\n    Map<String, String> map = configuration.toMap();\n    Map<String, String> restartStrategyEntries = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(RestartStrategyOptions.RESTART_STRATEGY_CONFIG_PREFIX)) {\n            restartStrategyEntries.put(entry.getKey(), entry.getValue());\n        }\n    }\n    this.configuration.addAll(Configuration.fromMap(restartStrategyEntries));\n}",
        "public void setUseSnapshotCompression(boolean useSnapshotCompression) {\n    configuration.set(ExecutionOptions.SNAPSHOT_COMPRESSION, useSnapshotCompression);\n}"
      ],
      "imports": [
        "org.apache.commons.compress.utils.Sets",
        "org.apache.flink.api.common.ExecutionConfig.ClosureCleanerLevel",
        "org.apache.flink.api.common.ExecutionConfig.GlobalJobParameters",
        "org.apache.flink.api.common.ExecutionConfig.MapBasedJobParameters",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.configuration.ConfigOption",
        "org.apache.flink.configuration.ConfigOptions",
        "org.apache.flink.configuration.ConfigOptions.OptionBuilder",
        "org.apache.flink.configuration.ConfigOptions.TypedConfigOptionBuilder",
        "org.apache.flink.configuration.Configuration",
        "org.apache.flink.configuration.CoreOptions",
        "org.apache.flink.configuration.ExecutionOptions",
        "org.apache.flink.configuration.JobManagerOptions",
        "org.apache.flink.configuration.JobManagerOptions.SchedulerType",
        "org.apache.flink.configuration.MetricOptions",
        "org.apache.flink.configuration.PipelineOptions",
        "org.apache.flink.configuration.ReadableConfig",
        "org.apache.flink.configuration.RestartStrategyOptions",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType",
        "org.apache.flink.configuration.StateChangelogOptions",
        "org.apache.flink.configuration.TaskManagerOptions",
        "org.apache.flink.configuration.description.Description",
        "org.apache.flink.configuration.description.Description.DescriptionBuilder",
        "org.apache.flink.configuration.description.InlineElement",
        "org.apache.flink.configuration.description.LinkElement",
        "org.apache.flink.configuration.description.TextElement",
        "org.apache.flink.util.Preconditions"
      ],
      "testTemplate": "package org.apache.flink.api.common;\n\npublic class ExecutionConfig_RestartStrategyTypemethod_SetsnewHashSetFikaTest {\n\n    @Test\n    public void testConfigure() {\n    }\n}",
      "conditionCount": 0,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.restoreSerializer()",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.Logger.<init>()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.restoreSerializer()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
        "com.esotericsoftware.minlog.Log.Logger.<init>()"
      ],
      "methodSources": [
        "@Override\npublic TypeSerializer<T> restoreSerializer() {\n    return new KryoSerializer<>(snapshotData.getTypeClass(), snapshotData.getDefaultKryoSerializers().unwrapOptionals(), snapshotData.getDefaultKryoSerializerClasses().unwrapOptionals(), snapshotData.getKryoRegistrations().unwrapOptionals());\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "(Logger log) // PATH: Test should invoke the next new Log$Logger(...) [step in execution path]\n{\n    this.log = checkNotNull(log);\n}"
      ],
      "constructors": [
        "@SuppressWarnings(\"unused\")\npublic KryoSerializerSnapshot() {\n}",
        "KryoSerializerSnapshot(Class<T> typeClass, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultKryoSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultKryoSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.snapshotData = createFrom(typeClass, defaultKryoSerializers, defaultKryoSerializerClasses, kryoRegistrations);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializerSnapshot.class);",
        "private static final int VERSION = 2;",
        "private KryoSerializerSnapshotData<T> snapshotData;"
      ],
      "setters": [
        "private void logMissingKeys(MergeResult<?, ?> mergeResult) {\n    mergeResult.missingKeys().forEach(key -> LOG.warn((\"The Kryo registration for a previously registered class {} does not have a \" + \"proper serializer, because its previous serializer cannot be loaded or is no \") + \"longer valid but a new serializer is not available\", key));\n}",
        "@Override\npublic void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException {\n    this.snapshotData = createFrom(in, userCodeClassLoader);\n}",
        "@Override\npublic void writeSnapshot(DataOutputView out) throws IOException {\n    snapshotData.writeSnapshotData(out);\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.common.typeutils.TypeSerializerSchemaCompatibility",
        "org.apache.flink.api.common.typeutils.TypeSerializerSnapshot",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.LinkedOptionalMap",
        "org.apache.flink.util.LinkedOptionalMap.MergeResult",
        "org.apache.flink.util.Preconditions",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerSnapshot_MinlogForwardermethod_LoggermethodFikaTest {\n\n    @Test\n    public void testRestoreSerializer() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<init>(java.lang.Class, org.apache.flink.api.common.serialization.SerializerConfig)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.Logger.<init>()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<init>(java.lang.Class, org.apache.flink.api.common.serialization.SerializerConfig)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
        "com.esotericsoftware.minlog.Log.Logger.<init>()"
      ],
      "methodSources": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "(Logger log) // PATH: Test should invoke the next new Log$Logger(...) [step in execution path]\n{\n    this.log = checkNotNull(log);\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializer_MinlogForwardermethod_LoggermethodFikaTest {\n\n    @Test\n    public void testMethod() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate()",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.Logger.<init>()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
        "com.esotericsoftware.minlog.Log.Logger.<init>()"
      ],
      "methodSources": [
        "@Override\npublic KryoSerializer<T> duplicate() {\n    return new KryoSerializer<>(this);\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "(Logger log) // PATH: Test should invoke the next new Log$Logger(...) [step in execution path]\n{\n    this.log = checkNotNull(log);\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializer_MinlogForwardermethod_LoggermethodFikaTest {\n\n    @Test\n    public void testDuplicate() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.RestartStrategyDescriptionUtils.getRestartStrategyDescription(org.apache.flink.configuration.Configuration)",
      "thirdPartyMethod": "org.apache.commons.compress.utils.Sets.newHashSet(java.lang.Object[])",
      "directCaller": "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<init>(java.lang.String, int, java.lang.String, java.util.Set)",
      "path": [
        "org.apache.flink.api.common.RestartStrategyDescriptionUtils.getRestartStrategyDescription(org.apache.flink.configuration.Configuration)",
        "org.apache.flink.configuration.RestartStrategyOptions.<clinit>()",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<clinit>()",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<init>(java.lang.String, int, java.lang.String, java.util.Set)",
        "org.apache.commons.compress.utils.Sets.newHashSet(java.lang.Object[])"
      ],
      "methodSources": [
        "/**\n * Returns a descriptive string of the restart strategy configured in the given Configuration\n * object.\n *\n * @param configuration\n * \t\tthe Configuration to extract the restart strategy from\n * @return a description of the restart strategy\n */\npublic static String getRestartStrategyDescription(Configuration configuration) {\n    final Optional<String> restartStrategyNameOptional = configuration.getOptional(RestartStrategyOptions.RESTART_STRATEGY);\n    return restartStrategyNameOptional.map(restartStrategyName -> {\n        switch (RestartStrategyOptions.RestartStrategyType.of(restartStrategyName.toLowerCase())) {\n            case NO_RESTART_STRATEGY :\n                return \"Restart deactivated.\";\n            case FIXED_DELAY :\n                return getFixedDelayDescription(configuration);\n            case FAILURE_RATE :\n                return getFailureRateDescription(configuration);\n            case EXPONENTIAL_DELAY :\n                return getExponentialDelayDescription(configuration);\n            default :\n                throw new IllegalArgumentException((\"Unknown restart strategy \" + restartStrategyName) + \".\");\n        }\n    }).orElse(\"Cluster level default restart strategy\");\n}",
        "org.apache.flink.configuration.RestartStrategyOptions\n\n// Static field initializations\n@Internal\npublic static final String RESTART_STRATEGY_CONFIG_PREFIX = \"restart-strategy\";\npublic static final ConfigOption<String> RESTART_STRATEGY = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".type\").stringType().noDefaultValue().withDeprecatedKeys(RESTART_STRATEGY_CONFIG_PREFIX).withDescription(Description.builder().text(\"Defines the restart strategy to use in case of job failures.\").linebreak().text(\"Accepted values are:\").list(text(\"%s, %s, %s: No restart strategy.\", NO_RESTART_STRATEGY.getAllTextElement()), text(\"%s, %s: Fixed delay restart strategy. More details can be found %s.\", concat(FIXED_DELAY.getAllTextElement(), link(\"{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#fixed-delay-restart-strategy\", \"here\"))), text(\"%s, %s: Failure rate restart strategy. More details can be found %s.\", concat(FAILURE_RATE.getAllTextElement(), link(\"{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#failure-rate-restart-strategy\", \"here\"))), text(\"%s, %s: Exponential delay restart strategy. More details can be found %s.\", concat(EXPONENTIAL_DELAY.getAllTextElement(), link(\"{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery#exponential-delay-restart-strategy\", \"here\")))).text(\"If checkpointing is disabled, the default value is %s. \" + \"If checkpointing is enabled, the default value is %s, and the default values of %s related config options will be used.\", code(NO_RESTART_STRATEGY.getMainValue()), code(EXPONENTIAL_DELAY.getMainValue()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Integer> RESTART_STRATEGY_FIXED_DELAY_ATTEMPTS = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".fixed-delay.attempts\").intType().defaultValue(1).withDescription(Description.builder().text(\"The number of times that Flink retries the execution before the job is declared as failed if %s has been set to %s.\", code(RESTART_STRATEGY.key()), code(FIXED_DELAY.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_FIXED_DELAY_DELAY = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".fixed-delay.delay\").durationType().defaultValue(Duration.ofSeconds(1)).withDescription(Description.builder().text(((\"Delay between two consecutive restart attempts if %s has been set to %s. \" + \"Delaying the retries can be helpful when the program interacts with external systems where \") + \"for example connections or pending transactions should reach a timeout before re-execution \") + \"is attempted. It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(FIXED_DELAY.getMainValue())).build());\npublic static final ConfigOption<Integer> RESTART_STRATEGY_FAILURE_RATE_MAX_FAILURES_PER_INTERVAL = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".failure-rate.max-failures-per-interval\").intType().defaultValue(1).withDescription(Description.builder().text(\"Maximum number of restarts in given time interval before failing a job if %s has been set to %s.\", code(RESTART_STRATEGY.key()), code(FAILURE_RATE.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_FAILURE_RATE_FAILURE_RATE_INTERVAL = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".failure-rate.failure-rate-interval\").durationType().defaultValue(Duration.ofMinutes(1)).withDescription(Description.builder().text(\"Time interval for measuring failure rate if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(FAILURE_RATE.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_FAILURE_RATE_DELAY = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".failure-rate.delay\").durationType().defaultValue(Duration.ofSeconds(1)).withDescription(Description.builder().text(\"Delay between two consecutive restart attempts if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(FAILURE_RATE.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_EXPONENTIAL_DELAY_INITIAL_BACKOFF = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.initial-backoff\").durationType().defaultValue(Duration.ofSeconds(1)).withDescription(Description.builder().text(\"Starting duration between restarts if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_EXPONENTIAL_DELAY_MAX_BACKOFF = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.max-backoff\").durationType().defaultValue(Duration.ofMinutes(1)).withDescription(Description.builder().text(\"The highest possible duration between restarts if %s has been set to %s. \" + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Double> RESTART_STRATEGY_EXPONENTIAL_DELAY_BACKOFF_MULTIPLIER = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.backoff-multiplier\").doubleType().defaultValue(1.5).withDescription(Description.builder().text(\"Backoff value is multiplied by this value after every failure,\" + \"until max backoff is reached if %s has been set to %s.\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Duration> RESTART_STRATEGY_EXPONENTIAL_DELAY_RESET_BACKOFF_THRESHOLD = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.reset-backoff-threshold\").durationType().defaultValue(Duration.ofHours(1)).withDescription(Description.builder().text(((\"Threshold when the backoff is reset to its initial value if %s has been set to %s. \" + \"It specifies how long the job must be running without failure \") + \"to reset the exponentially increasing backoff to its initial value. \") + \"It can be specified using notation: \\\"1 min\\\", \\\"20 s\\\"\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\npublic static final ConfigOption<Double> RESTART_STRATEGY_EXPONENTIAL_DELAY_JITTER_FACTOR = ConfigOptions.key(RESTART_STRATEGY_CONFIG_PREFIX + \".exponential-delay.jitter-factor\").doubleType().defaultValue(0.1).withDescription(Description.builder().text((\"Jitter specified as a portion of the backoff if %s has been set to %s. \" + \"It represents how large random value will be added or subtracted to the backoff. \") + \"Useful when you want to avoid restarting multiple jobs at the same time.\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\n@Documentation.OverrideDefault(\"infinite\")\npublic static final ConfigOption<Integer> RESTART_STRATEGY_EXPONENTIAL_DELAY_ATTEMPTS = ConfigOptions.key(\"restart-strategy.exponential-delay.attempts-before-reset-backoff\").intType().defaultValue(Integer.MAX_VALUE).withDescription(Description.builder().text(\"The number of times that Flink retries the execution before failing the job if %s has been set to %s. \" + \"The number will be reset once the backoff is reset to its initial value.\", code(RESTART_STRATEGY.key()), code(EXPONENTIAL_DELAY.getMainValue())).build());\n",
        "org.apache.flink.configuration.RestartStrategyOptions$RestartStrategyType\n\n// Static field initializations\nNO_RESTART_STRATEGY(\"disable\", Sets.newHashSet(\"none\", \"off\"))\nFIXED_DELAY(\"fixed-delay\", Sets.newHashSet(\"fixeddelay\"))\nFAILURE_RATE(\"failure-rate\", Sets.newHashSet(\"failurerate\"))\nEXPONENTIAL_DELAY(\"exponential-delay\", Sets.newHashSet(\"exponentialdelay\"))\n",
        "(String mainValue, Set<String> otherAvailableValues) {\n    this.mainValue = mainValue;\n    // PATH: Test should invoke the next Sets.newHashSet(...) [step in execution path]\n    this.allAvailableValues = Sets.newHashSet(mainValue);\n    allAvailableValues.addAll(otherAvailableValues);\n}"
      ],
      "constructors": [
        "RestartStrategyDescriptionUtils() {\n}"
      ],
      "fieldDeclarations": [],
      "setters": [],
      "imports": [
        "org.apache.commons.compress.utils.Sets",
        "org.apache.flink.configuration.ConfigOption",
        "org.apache.flink.configuration.ConfigOptions",
        "org.apache.flink.configuration.ConfigOptions.OptionBuilder",
        "org.apache.flink.configuration.ConfigOptions.TypedConfigOptionBuilder",
        "org.apache.flink.configuration.Configuration",
        "org.apache.flink.configuration.RestartStrategyOptions",
        "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType",
        "org.apache.flink.configuration.description.Description",
        "org.apache.flink.configuration.description.Description.DescriptionBuilder",
        "org.apache.flink.configuration.description.InlineElement",
        "org.apache.flink.configuration.description.LinkElement",
        "org.apache.flink.configuration.description.TextElement"
      ],
      "testTemplate": "package org.apache.flink.api.common;\n\npublic class RestartStrategyDescriptionUtils_RestartStrategyTypemethod_SetsnewHashSetFikaTest {\n\n    @Test\n    public void testGetRestartStrategyDescription() {\n    }\n}",
      "conditionCount": 1,
      "callCount": 1,
      "covered": true
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.GenericTypeInfo.createSerializer(org.apache.flink.api.common.serialization.SerializerConfig)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.Logger.<init>()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
      "path": [
        "org.apache.flink.api.java.typeutils.GenericTypeInfo.createSerializer(org.apache.flink.api.common.serialization.SerializerConfig)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
        "com.esotericsoftware.minlog.Log.Logger.<init>()"
      ],
      "methodSources": [
        "@Override\n@PublicEvolving\npublic TypeSerializer<T> createSerializer(SerializerConfig config) {\n    if (config.hasGenericTypesDisabled()) {\n        throw new UnsupportedOperationException((\"Generic types have been disabled in the ExecutionConfig and type \" + this.typeClass.getName()) + \" is treated as a generic type.\");\n    }\n    return new KryoSerializer<T>(this.typeClass, config);\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "(Logger log) // PATH: Test should invoke the next new Log$Logger(...) [step in execution path]\n{\n    this.log = checkNotNull(log);\n}"
      ],
      "constructors": [
        "@PublicEvolving\npublic GenericTypeInfo(Class<T> typeClass) {\n    this.typeClass = checkNotNull(typeClass);\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = -7959114120287706504L;",
        "private final Class<T> typeClass;"
      ],
      "setters": [],
      "imports": [
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.kryo.ChillSerializerRegistrar",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerDebugInitHelper",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder",
        "org.apache.flink.util.Preconditions",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils;\n\npublic class GenericTypeInfo_MinlogForwardermethod_LoggermethodFikaTest {\n\n    @Test\n    public void testCreateSerializer() {\n    }\n}",
      "conditionCount": 2,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.PojoTypeInfo.createSerializer(org.apache.flink.api.common.serialization.SerializerConfig)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.Logger.<init>()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
      "path": [
        "org.apache.flink.api.java.typeutils.PojoTypeInfo.createSerializer(org.apache.flink.api.common.serialization.SerializerConfig)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
        "com.esotericsoftware.minlog.Log.Logger.<init>()"
      ],
      "methodSources": [
        "@Override\n@PublicEvolving\n@SuppressWarnings(\"unchecked\")\npublic TypeSerializer<T> createSerializer(SerializerConfig config) {\n    if (config.isForceKryoEnabled()) {\n        return new KryoSerializer<>(getTypeClass(), config);\n    }\n    if (config.isForceAvroEnabled()) {\n        return AvroUtils.getAvroUtils().createAvroSerializer(getTypeClass());\n    }\n    return createPojoSerializer(config);\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "(Logger log) // PATH: Test should invoke the next new Log$Logger(...) [step in execution path]\n{\n    this.log = checkNotNull(log);\n}"
      ],
      "constructors": [
        "@PublicEvolving\npublic PojoTypeInfo(Class<T> typeClass, List<PojoField> fields) {\n    super(typeClass);\n    checkArgument(Modifier.isPublic(typeClass.getModifiers()), \"POJO %s is not public\", typeClass);\n    this.fields = fields.toArray(new PojoField[fields.size()]);\n    Arrays.sort(this.fields, new Comparator<PojoField>() {\n        @Override\n        public int compare(PojoField o1, PojoField o2) {\n            return o1.getField().getName().compareTo(o2.getField().getName());\n        }\n    });\n    int counterFields = 0;\n    for (PojoField field : fields) {\n        counterFields += field.getTypeInformation().getTotalFields();\n    }\n    totalFields = counterFields;\n}",
        "1() {\n}",
        "public PojoTypeComparatorBuilder() {\n    fieldComparators = new ArrayList<TypeComparator>();\n    keyFields = new ArrayList<Field>();\n}",
        "public NamedFlatFieldDescriptor(String name, int keyPosition, TypeInformation<?> type) {\n    super(keyPosition, type);\n    this.fieldName = name;\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 1L;",
        "private static final String REGEX_FIELD = \"[\\\\p{L}_\\\\$][\\\\p{L}\\\\p{Digit}_\\\\$]*\";",
        "private static final String REGEX_NESTED_FIELDS = (\"(\" + REGEX_FIELD) + \")(\\\\.(.+))?\";",
        "private static final String REGEX_NESTED_FIELDS_WILDCARD = (((REGEX_NESTED_FIELDS + \"|\\\\\") + ExpressionKeys.SELECT_ALL_CHAR) + \"|\\\\\") + ExpressionKeys.SELECT_ALL_CHAR_SCALA;",
        "private static final Pattern PATTERN_NESTED_FIELDS = Pattern.compile(REGEX_NESTED_FIELDS);",
        "private static final Pattern PATTERN_NESTED_FIELDS_WILDCARD = Pattern.compile(REGEX_NESTED_FIELDS_WILDCARD);",
        "private final PojoField[] fields;",
        "private final int totalFields;"
      ],
      "setters": [
        "@Override\n@PublicEvolving\npublic void getFlatFields(String fieldExpression, int offset, List<FlatFieldDescriptor> result) {\n    Matcher matcher = PATTERN_NESTED_FIELDS_WILDCARD.matcher(fieldExpression);\n    if (!matcher.matches()) {\n        throw new InvalidFieldReferenceException((\"Invalid POJO field reference \\\"\" + fieldExpression) + \"\\\".\");\n    }\n    String field = matcher.group(0);\n    if (field.equals(ExpressionKeys.SELECT_ALL_CHAR) || field.equals(ExpressionKeys.SELECT_ALL_CHAR_SCALA)) {\n        // handle select all\n        int keyPosition = 0;\n        for (PojoField pField : fields) {\n            if (pField.getTypeInformation() instanceof CompositeType) {\n                CompositeType<?> cType = ((CompositeType<?>) (pField.getTypeInformation()));\n                cType.getFlatFields(String.valueOf(ExpressionKeys.SELECT_ALL_CHAR), offset + keyPosition, result);\n                keyPosition += cType.getTotalFields() - 1;\n            } else {\n                result.add(new NamedFlatFieldDescriptor(pField.getField().getName(), offset + keyPosition, pField.getTypeInformation()));\n            }\n            keyPosition++;\n        }\n        return;\n    } else {\n        field = matcher.group(1);\n    }\n    // get field\n    int fieldPos = -1;\n    TypeInformation<?> fieldType = null;\n    for (int i = 0; i < fields.length; i++) {\n        if (fields[i].getField().getName().equals(field)) {\n            fieldPos = i;\n            fieldType = fields[i].getTypeInformation();\n            break;\n        }\n    }\n    if (fieldPos == (-1)) {\n        throw new InvalidFieldReferenceException((((\"Unable to find field \\\"\" + field) + \"\\\" in type \") + this) + \".\");\n    }\n    String tail = matcher.group(3);\n    if (tail == null) {\n        if (fieldType instanceof CompositeType) {\n            // forward offset\n            for (int i = 0; i < fieldPos; i++) {\n                offset += this.getTypeAt(i).getTotalFields();\n            }\n            // add all fields of composite type\n            ((CompositeType<?>) (fieldType)).getFlatFields(\"*\", offset, result);\n        } else {\n            // we found the field to add\n            // compute flat field position by adding skipped fields\n            int flatFieldPos = offset;\n            for (int i = 0; i < fieldPos; i++) {\n                flatFieldPos += this.getTypeAt(i).getTotalFields();\n            }\n            result.add(new FlatFieldDescriptor(flatFieldPos, fieldType));\n        }\n    } else if (fieldType instanceof CompositeType<?>) {\n        // forward offset\n        for (int i = 0; i < fieldPos; i++) {\n            offset += this.getTypeAt(i).getTotalFields();\n        }\n        ((CompositeType<?>) (fieldType)).getFlatFields(tail, offset, result);\n    } else {\n        throw new InvalidFieldReferenceException((((\"Nested field expression \\\"\" + tail) + \"\\\" not possible on atomic type \") + fieldType) + \".\");\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.operators.Keys",
        "org.apache.flink.api.common.operators.Keys.ExpressionKeys",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.CompositeType.FlatFieldDescriptor",
        "org.apache.flink.api.common.typeutils.CompositeType.InvalidFieldReferenceException",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.PojoTypeInfo.NamedFlatFieldDescriptor",
        "org.apache.flink.api.java.typeutils.PojoTypeInfo.PojoTypeComparatorBuilder",
        "org.apache.flink.api.java.typeutils.runtime.PojoSerializer",
        "org.apache.flink.api.java.typeutils.runtime.kryo.ChillSerializerRegistrar",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerDebugInitHelper",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder",
        "org.apache.flink.util.Preconditions",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils;\n\npublic class PojoTypeInfo_MinlogForwardermethod_LoggermethodFikaTest {\n\n    @Test\n    public void testCreateSerializer() {\n    }\n}",
      "conditionCount": 3,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.Logger.<init>()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(java.lang.Object)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
        "com.esotericsoftware.minlog.Log.Logger.<init>()"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "(Logger log) // PATH: Test should invoke the next new Log$Logger(...) [step in execution path]\n{\n    this.log = checkNotNull(log);\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializer_MinlogForwardermethod_LoggermethodFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(org.apache.flink.core.memory.DataInputView, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.Logger.<init>()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(org.apache.flink.core.memory.DataInputView, org.apache.flink.core.memory.DataOutputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
        "com.esotericsoftware.minlog.Log.Logger.<init>()"
      ],
      "methodSources": [
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "(Logger log) // PATH: Test should invoke the next new Log$Logger(...) [step in execution path]\n{\n    this.log = checkNotNull(log);\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializer_MinlogForwardermethod_LoggermethodFikaTest {\n\n    @Test\n    public void testCopy() {\n    }\n}",
      "conditionCount": 4,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.Logger.<init>()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(org.apache.flink.core.memory.DataInputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
        "com.esotericsoftware.minlog.Log.Logger.<init>()"
      ],
      "methodSources": [
        "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "(Logger log) // PATH: Test should invoke the next new Log$Logger(...) [step in execution path]\n{\n    this.log = checkNotNull(log);\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializer_MinlogForwardermethod_LoggermethodFikaTest {\n\n    @Test\n    public void testDeserialize() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo()",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.TRACE()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.TRACE()"
      ],
      "methodSources": [
        "@VisibleForTesting\npublic Kryo getKryo() // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n{\n    checkKryoInitialized();\n    return this.kryo;\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.TRACE(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogTRACEFikaTest {\n\n    @Test\n    public void testGetKryo() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo()",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)"
      ],
      "methodSources": [
        "@VisibleForTesting\npublic Kryo getKryo() // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n{\n    checkKryoInitialized();\n    return this.kryo;\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.setLogger(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogsetLoggerFikaTest {\n\n    @Test\n    public void testGetKryo() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.Logger.<init>()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(java.lang.Object, org.apache.flink.core.memory.DataOutputView)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
        "com.esotericsoftware.minlog.Log.Logger.<init>()"
      ],
      "methodSources": [
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "(Logger log) // PATH: Test should invoke the next new Log$Logger(...) [step in execution path]\n{\n    this.log = checkNotNull(log);\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializer_MinlogForwardermethod_LoggermethodFikaTest {\n\n    @Test\n    public void testSerialize() {\n    }\n}",
      "conditionCount": 6,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.TRACE()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.TRACE()"
      ],
      "methodSources": [
        "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.TRACE(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogTRACEFikaTest {\n\n    @Test\n    public void testCreateInstance() {\n    }\n}",
      "conditionCount": 6,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)"
      ],
      "methodSources": [
        "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.setLogger(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerconfigureKryoLogging_LogsetLoggerFikaTest {\n\n    @Test\n    public void testCreateInstance() {\n    }\n}",
      "conditionCount": 6,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo()",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.Logger.<init>()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
        "com.esotericsoftware.minlog.Log.Logger.<init>()"
      ],
      "methodSources": [
        "@VisibleForTesting\npublic Kryo getKryo() // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n{\n    checkKryoInitialized();\n    return this.kryo;\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "(Logger log) // PATH: Test should invoke the next new Log$Logger(...) [step in execution path]\n{\n    this.log = checkNotNull(log);\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializer_MinlogForwardermethod_LoggermethodFikaTest {\n\n    @Test\n    public void testGetKryo() {\n    }\n}",
      "conditionCount": 5,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.Logger.<init>()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
        "com.esotericsoftware.minlog.Log.Logger.<init>()"
      ],
      "methodSources": [
        "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else // PATH: Test should invoke the next KryoSerializer.checkKryoInitialized(...) [step in execution path]\n    {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}",
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "(Logger log) // PATH: Test should invoke the next new Log$Logger(...) [step in execution path]\n{\n    this.log = checkNotNull(log);\n}"
      ],
      "constructors": [
        "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}",
        "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}",
        "// for KryoSerializerSnapshot\n// ------------------------------------------------------------------------\nKryoSerializer(Class<T> type, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.type = checkNotNull(type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = checkNotNull(defaultSerializerClasses, \"Default serializer classes cannot be null.\");\n    this.defaultSerializers = checkNotNull(defaultSerializers, \"Default serializers cannot be null.\");\n    this.kryoRegistrations = checkNotNull(kryoRegistrations, \"Kryo registrations cannot be null.\");\n}"
      ],
      "fieldDeclarations": [
        "private static final long serialVersionUID = 3L;",
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);",
        "/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;",
        "@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();",
        "// ------------------------------------------------------------------------\nprivate final LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultSerializers;",
        "private final LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultSerializerClasses;",
        "/**\n * Map of class tag (using classname as tag) to their Kryo registration.\n *\n * <p>This map serves as a preview of the final registration result of the Kryo instance, taking\n * into account registration overwrites.\n */\nprivate LinkedHashMap<String, KryoRegistration> kryoRegistrations;",
        "private final Class<T> type;",
        "// ------------------------------------------------------------------------\n// The fields below are lazily initialized after duplication or deserialization.\nprivate transient Kryo kryo;",
        "private transient T copyInstance;",
        "private transient DataOutputView previousOut;",
        "private transient DataInputView previousIn;",
        "private transient Input input;",
        "private transient Output output;",
        "// ------------------------------------------------------------------------\n// legacy fields; these fields cannot yet be removed to retain backwards compatibility\nprivate LinkedHashMap<Class<?>, SerializableSerializer<?>> registeredTypesWithSerializers;",
        "private LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> registeredTypesWithSerializerClasses;",
        "private LinkedHashSet<Class<?>> registeredTypes;",
        "// for debugging purposes\nprivate volatile transient Thread currentThread;"
      ],
      "setters": [
        "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}",
        "// --------------------------------------------------------------------------------------------\n// For testing\n// --------------------------------------------------------------------------------------------\nprivate void enterExclusiveThread() {\n    // we use simple get, check, set here, rather than CAS\n    // we don't need lock-style correctness, this is only a sanity-check and we thus\n    // favor speed at the cost of some false negatives in this check\n    Thread previous = currentThread;\n    Thread thisThread = Thread.currentThread();\n    if (previous == null) {\n        currentThread = thisThread;\n    } else if (previous != thisThread) {\n        throw new IllegalStateException(((\"Concurrent access to KryoSerializer. Thread 1: \" + thisThread.getName()) + \" , Thread 2: \") + previous.getName());\n    }\n}",
        "private void exitExclusiveThread() {\n    currentThread = null;\n}",
        "// --------------------------------------------------------------------------------------------\nprivate void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n    in.defaultReadObject();\n    // kryoRegistrations may be null if this Kryo serializer is deserialized from an old version\n    if (kryoRegistrations == null) {\n        this.kryoRegistrations = buildKryoRegistrations(type, registeredTypes, registeredTypesWithSerializerClasses, registeredTypesWithSerializers, TernaryBoolean.UNDEFINED);\n    }\n}",
        "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Kryo",
        "com.esotericsoftware.kryo.KryoException",
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.kryo.io.Input",
        "com.esotericsoftware.kryo.io.KryoBufferUnderflowException",
        "com.esotericsoftware.kryo.io.Output",
        "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.serialization.SerializerConfigImpl",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.java.typeutils.runtime.DataInputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.SerializerDefinitionType",
        "org.apache.flink.api.java.typeutils.runtime.KryoUtils",
        "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.CollectionUtil",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.TernaryBoolean",
        "org.objenesis.strategy.InstantiatorStrategy",
        "org.objenesis.strategy.StdInstantiatorStrategy",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializer_MinlogForwardermethod_LoggermethodFikaTest {\n\n    @Test\n    public void testCreateInstance() {\n    }\n}",
      "conditionCount": 6,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility(org.apache.flink.api.common.typeutils.TypeSerializerSnapshot)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.TRACE()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility(org.apache.flink.api.common.typeutils.TypeSerializerSnapshot)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility(org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility(org.apache.flink.util.LinkedOptionalMap.MergeResult, org.apache.flink.util.LinkedOptionalMap.MergeResult, org.apache.flink.util.LinkedOptionalMap.MergeResult)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.TRACE()"
      ],
      "methodSources": [
        "@Override\npublic TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(TypeSerializerSnapshot<T> oldSerializerSnapshot) {\n    if (!(oldSerializerSnapshot instanceof KryoSerializerSnapshot)) {\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    KryoSerializerSnapshot<T> oldKryoSerializerSnapshot = ((KryoSerializerSnapshot<T>) (oldSerializerSnapshot));\n    if (snapshotData.getTypeClass() != oldKryoSerializerSnapshot.snapshotData.getTypeClass()) {\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // PATH: Test should invoke the next KryoSerializerSnapshot.resolveSchemaCompatibility(...) [step in execution path]\n    return resolveSchemaCompatibility(oldKryoSerializerSnapshot);\n}",
        "private TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(KryoSerializerSnapshot<T> oldKryoSerializerSnapshot) {\n    // merge the default serializers\n    final MergeResult<Class<?>, SerializableSerializer<?>> reconfiguredDefaultKryoSerializers = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getDefaultKryoSerializers(), snapshotData.getDefaultKryoSerializers());\n    if (reconfiguredDefaultKryoSerializers.hasMissingKeys()) {\n        logMissingKeys(reconfiguredDefaultKryoSerializers);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // merge default serializer classes\n    final MergeResult<Class<?>, Class<? extends Serializer<?>>> reconfiguredDefaultKryoSerializerClasses = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getDefaultKryoSerializerClasses(), snapshotData.getDefaultKryoSerializerClasses());\n    if (reconfiguredDefaultKryoSerializerClasses.hasMissingKeys()) {\n        logMissingKeys(reconfiguredDefaultKryoSerializerClasses);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // merge registration\n    final MergeResult<String, KryoRegistration> reconfiguredRegistrations = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getKryoRegistrations(), snapshotData.getKryoRegistrations());\n    if (reconfiguredRegistrations.hasMissingKeys()) {\n        logMissingKeys(reconfiguredRegistrations);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // there are no missing keys, now we have to decide whether we are compatible as-is or we\n    // require reconfiguration.\n    // PATH: Test should invoke the next KryoSerializerSnapshot.resolveSchemaCompatibility(...) [step in execution path]\n    return resolveSchemaCompatibility(reconfiguredDefaultKryoSerializers, reconfiguredDefaultKryoSerializerClasses, reconfiguredRegistrations);\n}",
        "private TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(MergeResult<Class<?>, SerializableSerializer<?>> reconfiguredDefaultKryoSerializers, MergeResult<Class<?>, Class<? extends Serializer<?>>> reconfiguredDefaultKryoSerializerClasses, MergeResult<String, KryoRegistration> reconfiguredRegistrations) {\n    if ((reconfiguredDefaultKryoSerializers.isOrderedSubset() && reconfiguredDefaultKryoSerializerClasses.isOrderedSubset()) && reconfiguredRegistrations.isOrderedSubset()) {\n        return TypeSerializerSchemaCompatibility.compatibleAsIs();\n    }\n    // reconfigure a new KryoSerializer\n    KryoSerializer<T> reconfiguredSerializer = new KryoSerializer<>(snapshotData.getTypeClass(), reconfiguredDefaultKryoSerializers.getMerged(), reconfiguredDefaultKryoSerializerClasses.getMerged(), reconfiguredRegistrations.getMerged());\n    return TypeSerializerSchemaCompatibility.compatibleWithReconfiguredSerializer(reconfiguredSerializer);\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.TRACE(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "@SuppressWarnings(\"unused\")\npublic KryoSerializerSnapshot() {\n}",
        "KryoSerializerSnapshot(Class<T> typeClass, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultKryoSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultKryoSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.snapshotData = createFrom(typeClass, defaultKryoSerializers, defaultKryoSerializerClasses, kryoRegistrations);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializerSnapshot.class);",
        "private static final int VERSION = 2;",
        "private KryoSerializerSnapshotData<T> snapshotData;"
      ],
      "setters": [
        "private void logMissingKeys(MergeResult<?, ?> mergeResult) {\n    mergeResult.missingKeys().forEach(key -> LOG.warn((\"The Kryo registration for a previously registered class {} does not have a \" + \"proper serializer, because its previous serializer cannot be loaded or is no \") + \"longer valid but a new serializer is not available\", key));\n}",
        "@Override\npublic void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException {\n    this.snapshotData = createFrom(in, userCodeClassLoader);\n}",
        "@Override\npublic void writeSnapshot(DataOutputView out) throws IOException {\n    snapshotData.writeSnapshotData(out);\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.common.typeutils.TypeSerializerSchemaCompatibility",
        "org.apache.flink.api.common.typeutils.TypeSerializerSnapshot",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.LinkedOptionalMap",
        "org.apache.flink.util.LinkedOptionalMap.MergeResult",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerSnapshot_KryoSerializerconfigureKryoLogging_LogTRACEFikaTest {\n\n    @Test\n    public void testResolveSchemaCompatibility() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility(org.apache.flink.api.common.typeutils.TypeSerializerSnapshot)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility(org.apache.flink.api.common.typeutils.TypeSerializerSnapshot)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility(org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility(org.apache.flink.util.LinkedOptionalMap.MergeResult, org.apache.flink.util.LinkedOptionalMap.MergeResult, org.apache.flink.util.LinkedOptionalMap.MergeResult)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "com.esotericsoftware.minlog.Log.setLogger(com.esotericsoftware.minlog.Log.Logger)"
      ],
      "methodSources": [
        "@Override\npublic TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(TypeSerializerSnapshot<T> oldSerializerSnapshot) {\n    if (!(oldSerializerSnapshot instanceof KryoSerializerSnapshot)) {\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    KryoSerializerSnapshot<T> oldKryoSerializerSnapshot = ((KryoSerializerSnapshot<T>) (oldSerializerSnapshot));\n    if (snapshotData.getTypeClass() != oldKryoSerializerSnapshot.snapshotData.getTypeClass()) {\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // PATH: Test should invoke the next KryoSerializerSnapshot.resolveSchemaCompatibility(...) [step in execution path]\n    return resolveSchemaCompatibility(oldKryoSerializerSnapshot);\n}",
        "private TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(KryoSerializerSnapshot<T> oldKryoSerializerSnapshot) {\n    // merge the default serializers\n    final MergeResult<Class<?>, SerializableSerializer<?>> reconfiguredDefaultKryoSerializers = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getDefaultKryoSerializers(), snapshotData.getDefaultKryoSerializers());\n    if (reconfiguredDefaultKryoSerializers.hasMissingKeys()) {\n        logMissingKeys(reconfiguredDefaultKryoSerializers);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // merge default serializer classes\n    final MergeResult<Class<?>, Class<? extends Serializer<?>>> reconfiguredDefaultKryoSerializerClasses = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getDefaultKryoSerializerClasses(), snapshotData.getDefaultKryoSerializerClasses());\n    if (reconfiguredDefaultKryoSerializerClasses.hasMissingKeys()) {\n        logMissingKeys(reconfiguredDefaultKryoSerializerClasses);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // merge registration\n    final MergeResult<String, KryoRegistration> reconfiguredRegistrations = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getKryoRegistrations(), snapshotData.getKryoRegistrations());\n    if (reconfiguredRegistrations.hasMissingKeys()) {\n        logMissingKeys(reconfiguredRegistrations);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // there are no missing keys, now we have to decide whether we are compatible as-is or we\n    // require reconfiguration.\n    // PATH: Test should invoke the next KryoSerializerSnapshot.resolveSchemaCompatibility(...) [step in execution path]\n    return resolveSchemaCompatibility(reconfiguredDefaultKryoSerializers, reconfiguredDefaultKryoSerializerClasses, reconfiguredRegistrations);\n}",
        "private TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(MergeResult<Class<?>, SerializableSerializer<?>> reconfiguredDefaultKryoSerializers, MergeResult<Class<?>, Class<? extends Serializer<?>>> reconfiguredDefaultKryoSerializerClasses, MergeResult<String, KryoRegistration> reconfiguredRegistrations) {\n    if ((reconfiguredDefaultKryoSerializers.isOrderedSubset() && reconfiguredDefaultKryoSerializerClasses.isOrderedSubset()) && reconfiguredRegistrations.isOrderedSubset()) {\n        return TypeSerializerSchemaCompatibility.compatibleAsIs();\n    }\n    // reconfigure a new KryoSerializer\n    KryoSerializer<T> reconfiguredSerializer = new KryoSerializer<>(snapshotData.getTypeClass(), reconfiguredDefaultKryoSerializers.getMerged(), reconfiguredDefaultKryoSerializerClasses.getMerged(), reconfiguredRegistrations.getMerged());\n    return TypeSerializerSchemaCompatibility.compatibleWithReconfiguredSerializer(reconfiguredSerializer);\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) // PATH: Test should invoke the next Log.setLogger(...) [step in execution path]\n    {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}"
      ],
      "constructors": [
        "@SuppressWarnings(\"unused\")\npublic KryoSerializerSnapshot() {\n}",
        "KryoSerializerSnapshot(Class<T> typeClass, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultKryoSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultKryoSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.snapshotData = createFrom(typeClass, defaultKryoSerializers, defaultKryoSerializerClasses, kryoRegistrations);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializerSnapshot.class);",
        "private static final int VERSION = 2;",
        "private KryoSerializerSnapshotData<T> snapshotData;"
      ],
      "setters": [
        "private void logMissingKeys(MergeResult<?, ?> mergeResult) {\n    mergeResult.missingKeys().forEach(key -> LOG.warn((\"The Kryo registration for a previously registered class {} does not have a \" + \"proper serializer, because its previous serializer cannot be loaded or is no \") + \"longer valid but a new serializer is not available\", key));\n}",
        "@Override\npublic void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException {\n    this.snapshotData = createFrom(in, userCodeClassLoader);\n}",
        "@Override\npublic void writeSnapshot(DataOutputView out) throws IOException {\n    snapshotData.writeSnapshotData(out);\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.common.typeutils.TypeSerializerSchemaCompatibility",
        "org.apache.flink.api.common.typeutils.TypeSerializerSnapshot",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.LinkedOptionalMap",
        "org.apache.flink.util.LinkedOptionalMap.MergeResult",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerSnapshot_KryoSerializerconfigureKryoLogging_LogsetLoggerFikaTest {\n\n    @Test\n    public void testResolveSchemaCompatibility() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
      "thirdPartyMethod": "org.apache.commons.lang3.ArrayUtils.addAll(int[], int[])",
      "directCaller": "org.apache.flink.api.common.operators.base.GroupCombineOperatorBase.executeOnCollections(java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
      "path": [
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.executeUnaryOperator(org.apache.flink.api.common.operators.SingleInputOperator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.base.GroupCombineOperatorBase.executeOnCollections(java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
        "org.apache.commons.lang3.ArrayUtils.addAll(int[], int[])"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// General execution methods\n// --------------------------------------------------------------------------------------------\npublic JobExecutionResult execute(Plan program) throws Exception {\n    long startTime = System.currentTimeMillis();\n    JobID jobID = (program.getJobId() == null) ? new JobID() : program.getJobId();\n    JobInfo jobInfo = new JobInfoImpl(jobID, program.getJobName());\n    initCache(program.getCachedFiles());\n    Collection<? extends GenericDataSinkBase<?>> sinks = program.getDataSinks();\n    for (Operator<?> sink : sinks) // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    {\n        execute(sink, jobInfo);\n    }\n    long endTime = System.currentTimeMillis();\n    Map<String, OptionalFailure<Object>> accumulatorResults = AccumulatorHelper.toResultMap(accumulators);\n    return new JobExecutionResult(null, endTime - startTime, accumulatorResults);\n}",
        "private List<?> execute(Operator<?> operator, JobInfo jobInfo) throws Exception {\n    // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    return execute(operator, 0, jobInfo);\n}",
        "private List<?> execute(Operator<?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    List<?> result = this.intermediateResults.get(operator);\n    // if it has already been computed, use the cached variant\n    if (result != null) {\n        return result;\n    }\n    if (operator instanceof BulkIterationBase) {\n        result = executeBulkIteration(((BulkIterationBase<?>) (operator)), jobInfo);\n    } else if (operator instanceof DeltaIterationBase) {\n        result = executeDeltaIteration(((DeltaIterationBase<?, ?>) (operator)), jobInfo);\n    } else if (operator instanceof SingleInputOperator) {\n        // PATH: Test should invoke the next CollectionExecutor.executeUnaryOperator(...) [step in execution path]\n        result = executeUnaryOperator(((SingleInputOperator<?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof DualInputOperator) {\n        result = executeBinaryOperator(((DualInputOperator<?, ?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSourceBase) {\n        result = executeDataSource(((GenericDataSourceBase<?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSinkBase) {\n        executeDataSink(((GenericDataSinkBase<?>) (operator)), superStep, jobInfo);\n        result = Collections.emptyList();\n    } else {\n        throw new RuntimeException(\"Cannot execute operator \" + operator.getClass().getName());\n    }\n    this.intermediateResults.put(operator, result);\n    return result;\n}",
        "private <IN, OUT> List<OUT> executeUnaryOperator(SingleInputOperator<?, ?, ?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    Operator<?> inputOp = operator.getInput();\n    if (inputOp == null) {\n        throw new InvalidProgramException((\"The unary operation \" + operator.getName()) + \" has no input.\");\n    }\n    @SuppressWarnings(\"unchecked\")\n    List<IN> inputData = ((List<IN>) (execute(inputOp, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    SingleInputOperator<IN, OUT, ?> typedOp = ((SingleInputOperator<IN, OUT, ?>) (operator));\n    // build the runtime context and compute broadcast variables, if necessary\n    TaskInfo taskInfo = new TaskInfoImpl(typedOp.getName(), 1, 0, 1, 0);\n    RuntimeUDFContext ctx;\n    if (RichFunction.class.isAssignableFrom(typedOp.getUserCodeWrapper().getUserCodeClass())) {\n        ctx = createContext(superStep, taskInfo, jobInfo);\n        for (Map.Entry<String, Operator<?>> bcInputs : operator.getBroadcastInputs().entrySet()) {\n            List<?> bcData = execute(bcInputs.getValue(), jobInfo);\n            ctx.setBroadcastVariable(bcInputs.getKey(), bcData);\n        }\n    } else {\n        ctx = null;\n    }\n    // PATH: Test should invoke the next GroupCombineOperatorBase.executeOnCollections(...) [step in execution path]\n    return typedOp.executeOnCollections(inputData, ctx, executionConfig);\n}",
        "// --------------------------------------------------------------------------------------------\n@Override\nprotected List<OUT> executeOnCollections(List<IN> inputData, RuntimeContext ctx, ExecutionConfig executionConfig) throws Exception {\n    GroupCombineFunction<IN, OUT> function = this.userFunction.getUserCodeObject();\n    UnaryOperatorInformation<IN, OUT> operatorInfo = getOperatorInfo();\n    TypeInformation<IN> inputType = operatorInfo.getInputType();\n    int[] keyColumns = getKeyColumns(0);\n    int[] sortColumns = keyColumns;\n    boolean[] sortOrderings = new boolean[sortColumns.length];\n    if (groupOrder != null) {\n        // PATH: Test should invoke the next ArrayUtils.addAll(...) [step in execution path]\n        sortColumns = ArrayUtils.addAll(sortColumns, groupOrder.getFieldPositions());\n        sortOrderings = ArrayUtils.addAll(sortOrderings, groupOrder.getFieldSortDirections());\n    }\n    if (sortColumns.length == 0) {\n        // => all reduce. No comparator\n        checkArgument(sortOrderings.length == 0);\n    } else {\n        final TypeComparator<IN> sortComparator = getTypeComparator(inputType, sortColumns, sortOrderings, executionConfig);\n        Collections.sort(inputData, new Comparator<IN>() {\n            @Override\n            public int compare(IN o1, IN o2) {\n                return sortComparator.compare(o1, o2);\n            }\n        });\n    }\n    FunctionUtils.setFunctionRuntimeContext(function, ctx);\n    FunctionUtils.openFunction(function, DefaultOpenContext.INSTANCE);\n    ArrayList<OUT> result = new ArrayList<OUT>();\n    if (keyColumns.length == 0) {\n        final TypeSerializer<IN> inputSerializer = inputType.createSerializer(executionConfig.getSerializerConfig());\n        TypeSerializer<OUT> outSerializer = getOperatorInfo().getOutputType().createSerializer(executionConfig.getSerializerConfig());\n        List<IN> inputDataCopy = new ArrayList<IN>(inputData.size());\n        for (IN in : inputData) {\n            inputDataCopy.add(inputSerializer.copy(in));\n        }\n        CopyingListCollector<OUT> collector = new CopyingListCollector<OUT>(result, outSerializer);\n        function.combine(inputDataCopy, collector);\n    } else {\n        final TypeSerializer<IN> inputSerializer = inputType.createSerializer(executionConfig.getSerializerConfig());\n        boolean[] keyOrderings = new boolean[keyColumns.length];\n        final TypeComparator<IN> comparator = getTypeComparator(inputType, keyColumns, keyOrderings, executionConfig);\n        ListKeyGroupedIterator<IN> keyedIterator = new ListKeyGroupedIterator<IN>(inputData, inputSerializer, comparator);\n        TypeSerializer<OUT> outSerializer = getOperatorInfo().getOutputType().createSerializer(executionConfig.getSerializerConfig());\n        CopyingListCollector<OUT> collector = new CopyingListCollector<OUT>(result, outSerializer);\n        while (keyedIterator.nextKey()) {\n            function.combine(keyedIterator.getValues(), collector);\n        } \n    }\n    FunctionUtils.closeFunction(function);\n    return result;\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic CollectionExecutor(ExecutionConfig executionConfig) {\n    this.executionConfig = executionConfig;\n    this.intermediateResults = new HashMap<Operator<?>, List<?>>();\n    this.accumulators = new HashMap<String, Accumulator<?, ?>>();\n    this.previousAggregates = new HashMap<String, Value>();\n    this.aggregators = new HashMap<String, Aggregator<?>>();\n    this.cachedFiles = new HashMap<String, Future<Path>>();\n    this.userCodeClassLoader = Thread.currentThread().getContextClassLoader();\n}",
        "public DynamicPathCollector(Set<Operator<?>> dynamicPathOperations) {\n    this.dynamicPathOperations = dynamicPathOperations;\n}",
        "public IterationRuntimeUDFContext(JobInfo jobInfo, TaskInfo taskInfo, ClassLoader classloader, ExecutionConfig executionConfig, Map<String, Future<Path>> cpTasks, Map<String, Accumulator<?, ?>> accumulators, OperatorMetricGroup metrics) {\n    super(jobInfo, taskInfo, classloader, executionConfig, cpTasks, accumulators, metrics);\n}",
        "public CompletedFuture(Path entry) {\n    try {\n        LocalFileSystem fs = ((LocalFileSystem) (FileSystem.getUnguardedFileSystem(entry.toUri())));\n        result = (entry.isAbsolute()) ? new Path(entry.toUri().getPath()) : new Path(fs.getWorkingDirectory(), entry);\n    } catch (Exception e) {\n        throw new RuntimeException(\"DistributedCache supports only local files for Collection Environments\");\n    }\n}"
      ],
      "fieldDeclarations": [
        "private final Map<Operator<?>, List<?>> intermediateResults;",
        "private final Map<String, Accumulator<?, ?>> accumulators;",
        "private final Map<String, Future<Path>> cachedFiles;",
        "private final Map<String, Value> previousAggregates;",
        "private final Map<String, Aggregator<?>> aggregators;",
        "private final ClassLoader userCodeClassLoader;",
        "private final ExecutionConfig executionConfig;",
        "private int iterationSuperstep;"
      ],
      "setters": [
        "private void initCache(Set<Map.Entry<String, DistributedCache.DistributedCacheEntry>> files) {\n    for (Map.Entry<String, DistributedCache.DistributedCacheEntry> file : files) {\n        Future<Path> doNothing = new CompletedFuture(new Path(file.getValue().filePath));\n        cachedFiles.put(file.getKey(), doNothing);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.lang3.ArrayUtils",
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.InvalidProgramException",
        "org.apache.flink.api.common.JobExecutionResult",
        "org.apache.flink.api.common.JobID",
        "org.apache.flink.api.common.JobInfo",
        "org.apache.flink.api.common.JobInfoImpl",
        "org.apache.flink.api.common.Plan",
        "org.apache.flink.api.common.TaskInfo",
        "org.apache.flink.api.common.TaskInfoImpl",
        "org.apache.flink.api.common.accumulators.Accumulator",
        "org.apache.flink.api.common.accumulators.AccumulatorHelper",
        "org.apache.flink.api.common.aggregators.Aggregator",
        "org.apache.flink.api.common.aggregators.AggregatorRegistry",
        "org.apache.flink.api.common.aggregators.AggregatorWithName",
        "org.apache.flink.api.common.aggregators.ConvergenceCriterion",
        "org.apache.flink.api.common.cache.DistributedCache",
        "org.apache.flink.api.common.cache.DistributedCache.DistributedCacheEntry",
        "org.apache.flink.api.common.functions.DefaultOpenContext",
        "org.apache.flink.api.common.functions.Function",
        "org.apache.flink.api.common.functions.GroupCombineFunction",
        "org.apache.flink.api.common.functions.OpenContext",
        "org.apache.flink.api.common.functions.RichFunction",
        "org.apache.flink.api.common.functions.RuntimeContext",
        "org.apache.flink.api.common.functions.util.CopyingListCollector",
        "org.apache.flink.api.common.functions.util.FunctionUtils",
        "org.apache.flink.api.common.functions.util.RuntimeUDFContext",
        "org.apache.flink.api.common.operators.CollectionExecutor.CompletedFuture",
        "org.apache.flink.api.common.operators.CollectionExecutor.DynamicPathCollector",
        "org.apache.flink.api.common.operators.base.BulkIterationBase",
        "org.apache.flink.api.common.operators.base.DeltaIterationBase",
        "org.apache.flink.api.common.operators.base.GroupCombineOperatorBase",
        "org.apache.flink.api.common.operators.util.ListKeyGroupedIterator",
        "org.apache.flink.api.common.operators.util.ListKeyGroupedIterator.ValuesIterator",
        "org.apache.flink.api.common.operators.util.TypeComparable",
        "org.apache.flink.api.common.operators.util.UserCodeWrapper",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.fs.FileSystem",
        "org.apache.flink.core.fs.Path",
        "org.apache.flink.core.fs.local.LocalFileSystem",
        "org.apache.flink.metrics.groups.OperatorMetricGroup",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.Collector",
        "org.apache.flink.util.OptionalFailure",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.Visitable",
        "org.apache.flink.util.Visitor"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators;\n\npublic class CollectionExecutor_GroupCombineOperatorBaseexecuteOnCollections_ArrayUtilsaddAllFikaTest {\n\n    @Test\n    public void testExecute() {\n    }\n}",
      "conditionCount": 17,
      "callCount": 2,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
      "thirdPartyMethod": "org.apache.commons.lang3.ArrayUtils.addAll(boolean[], boolean[])",
      "directCaller": "org.apache.flink.api.common.operators.base.GroupCombineOperatorBase.executeOnCollections(java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
      "path": [
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.executeUnaryOperator(org.apache.flink.api.common.operators.SingleInputOperator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.base.GroupCombineOperatorBase.executeOnCollections(java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
        "org.apache.commons.lang3.ArrayUtils.addAll(boolean[], boolean[])"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// General execution methods\n// --------------------------------------------------------------------------------------------\npublic JobExecutionResult execute(Plan program) throws Exception {\n    long startTime = System.currentTimeMillis();\n    JobID jobID = (program.getJobId() == null) ? new JobID() : program.getJobId();\n    JobInfo jobInfo = new JobInfoImpl(jobID, program.getJobName());\n    initCache(program.getCachedFiles());\n    Collection<? extends GenericDataSinkBase<?>> sinks = program.getDataSinks();\n    for (Operator<?> sink : sinks) // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    {\n        execute(sink, jobInfo);\n    }\n    long endTime = System.currentTimeMillis();\n    Map<String, OptionalFailure<Object>> accumulatorResults = AccumulatorHelper.toResultMap(accumulators);\n    return new JobExecutionResult(null, endTime - startTime, accumulatorResults);\n}",
        "private List<?> execute(Operator<?> operator, JobInfo jobInfo) throws Exception {\n    // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    return execute(operator, 0, jobInfo);\n}",
        "private List<?> execute(Operator<?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    List<?> result = this.intermediateResults.get(operator);\n    // if it has already been computed, use the cached variant\n    if (result != null) {\n        return result;\n    }\n    if (operator instanceof BulkIterationBase) {\n        result = executeBulkIteration(((BulkIterationBase<?>) (operator)), jobInfo);\n    } else if (operator instanceof DeltaIterationBase) {\n        result = executeDeltaIteration(((DeltaIterationBase<?, ?>) (operator)), jobInfo);\n    } else if (operator instanceof SingleInputOperator) {\n        // PATH: Test should invoke the next CollectionExecutor.executeUnaryOperator(...) [step in execution path]\n        result = executeUnaryOperator(((SingleInputOperator<?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof DualInputOperator) {\n        result = executeBinaryOperator(((DualInputOperator<?, ?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSourceBase) {\n        result = executeDataSource(((GenericDataSourceBase<?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSinkBase) {\n        executeDataSink(((GenericDataSinkBase<?>) (operator)), superStep, jobInfo);\n        result = Collections.emptyList();\n    } else {\n        throw new RuntimeException(\"Cannot execute operator \" + operator.getClass().getName());\n    }\n    this.intermediateResults.put(operator, result);\n    return result;\n}",
        "private <IN, OUT> List<OUT> executeUnaryOperator(SingleInputOperator<?, ?, ?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    Operator<?> inputOp = operator.getInput();\n    if (inputOp == null) {\n        throw new InvalidProgramException((\"The unary operation \" + operator.getName()) + \" has no input.\");\n    }\n    @SuppressWarnings(\"unchecked\")\n    List<IN> inputData = ((List<IN>) (execute(inputOp, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    SingleInputOperator<IN, OUT, ?> typedOp = ((SingleInputOperator<IN, OUT, ?>) (operator));\n    // build the runtime context and compute broadcast variables, if necessary\n    TaskInfo taskInfo = new TaskInfoImpl(typedOp.getName(), 1, 0, 1, 0);\n    RuntimeUDFContext ctx;\n    if (RichFunction.class.isAssignableFrom(typedOp.getUserCodeWrapper().getUserCodeClass())) {\n        ctx = createContext(superStep, taskInfo, jobInfo);\n        for (Map.Entry<String, Operator<?>> bcInputs : operator.getBroadcastInputs().entrySet()) {\n            List<?> bcData = execute(bcInputs.getValue(), jobInfo);\n            ctx.setBroadcastVariable(bcInputs.getKey(), bcData);\n        }\n    } else {\n        ctx = null;\n    }\n    // PATH: Test should invoke the next GroupCombineOperatorBase.executeOnCollections(...) [step in execution path]\n    return typedOp.executeOnCollections(inputData, ctx, executionConfig);\n}",
        "// --------------------------------------------------------------------------------------------\n@Override\nprotected List<OUT> executeOnCollections(List<IN> inputData, RuntimeContext ctx, ExecutionConfig executionConfig) throws Exception {\n    GroupCombineFunction<IN, OUT> function = this.userFunction.getUserCodeObject();\n    UnaryOperatorInformation<IN, OUT> operatorInfo = getOperatorInfo();\n    TypeInformation<IN> inputType = operatorInfo.getInputType();\n    int[] keyColumns = getKeyColumns(0);\n    int[] sortColumns = keyColumns;\n    boolean[] sortOrderings = new boolean[sortColumns.length];\n    if (groupOrder != null) {\n        // PATH: Test should invoke the next ArrayUtils.addAll(...) [step in execution path]\n        sortColumns = ArrayUtils.addAll(sortColumns, groupOrder.getFieldPositions());\n        sortOrderings = ArrayUtils.addAll(sortOrderings, groupOrder.getFieldSortDirections());\n    }\n    if (sortColumns.length == 0) {\n        // => all reduce. No comparator\n        checkArgument(sortOrderings.length == 0);\n    } else {\n        final TypeComparator<IN> sortComparator = getTypeComparator(inputType, sortColumns, sortOrderings, executionConfig);\n        Collections.sort(inputData, new Comparator<IN>() {\n            @Override\n            public int compare(IN o1, IN o2) {\n                return sortComparator.compare(o1, o2);\n            }\n        });\n    }\n    FunctionUtils.setFunctionRuntimeContext(function, ctx);\n    FunctionUtils.openFunction(function, DefaultOpenContext.INSTANCE);\n    ArrayList<OUT> result = new ArrayList<OUT>();\n    if (keyColumns.length == 0) {\n        final TypeSerializer<IN> inputSerializer = inputType.createSerializer(executionConfig.getSerializerConfig());\n        TypeSerializer<OUT> outSerializer = getOperatorInfo().getOutputType().createSerializer(executionConfig.getSerializerConfig());\n        List<IN> inputDataCopy = new ArrayList<IN>(inputData.size());\n        for (IN in : inputData) {\n            inputDataCopy.add(inputSerializer.copy(in));\n        }\n        CopyingListCollector<OUT> collector = new CopyingListCollector<OUT>(result, outSerializer);\n        function.combine(inputDataCopy, collector);\n    } else {\n        final TypeSerializer<IN> inputSerializer = inputType.createSerializer(executionConfig.getSerializerConfig());\n        boolean[] keyOrderings = new boolean[keyColumns.length];\n        final TypeComparator<IN> comparator = getTypeComparator(inputType, keyColumns, keyOrderings, executionConfig);\n        ListKeyGroupedIterator<IN> keyedIterator = new ListKeyGroupedIterator<IN>(inputData, inputSerializer, comparator);\n        TypeSerializer<OUT> outSerializer = getOperatorInfo().getOutputType().createSerializer(executionConfig.getSerializerConfig());\n        CopyingListCollector<OUT> collector = new CopyingListCollector<OUT>(result, outSerializer);\n        while (keyedIterator.nextKey()) {\n            function.combine(keyedIterator.getValues(), collector);\n        } \n    }\n    FunctionUtils.closeFunction(function);\n    return result;\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic CollectionExecutor(ExecutionConfig executionConfig) {\n    this.executionConfig = executionConfig;\n    this.intermediateResults = new HashMap<Operator<?>, List<?>>();\n    this.accumulators = new HashMap<String, Accumulator<?, ?>>();\n    this.previousAggregates = new HashMap<String, Value>();\n    this.aggregators = new HashMap<String, Aggregator<?>>();\n    this.cachedFiles = new HashMap<String, Future<Path>>();\n    this.userCodeClassLoader = Thread.currentThread().getContextClassLoader();\n}",
        "public DynamicPathCollector(Set<Operator<?>> dynamicPathOperations) {\n    this.dynamicPathOperations = dynamicPathOperations;\n}",
        "public IterationRuntimeUDFContext(JobInfo jobInfo, TaskInfo taskInfo, ClassLoader classloader, ExecutionConfig executionConfig, Map<String, Future<Path>> cpTasks, Map<String, Accumulator<?, ?>> accumulators, OperatorMetricGroup metrics) {\n    super(jobInfo, taskInfo, classloader, executionConfig, cpTasks, accumulators, metrics);\n}",
        "public CompletedFuture(Path entry) {\n    try {\n        LocalFileSystem fs = ((LocalFileSystem) (FileSystem.getUnguardedFileSystem(entry.toUri())));\n        result = (entry.isAbsolute()) ? new Path(entry.toUri().getPath()) : new Path(fs.getWorkingDirectory(), entry);\n    } catch (Exception e) {\n        throw new RuntimeException(\"DistributedCache supports only local files for Collection Environments\");\n    }\n}"
      ],
      "fieldDeclarations": [
        "private final Map<Operator<?>, List<?>> intermediateResults;",
        "private final Map<String, Accumulator<?, ?>> accumulators;",
        "private final Map<String, Future<Path>> cachedFiles;",
        "private final Map<String, Value> previousAggregates;",
        "private final Map<String, Aggregator<?>> aggregators;",
        "private final ClassLoader userCodeClassLoader;",
        "private final ExecutionConfig executionConfig;",
        "private int iterationSuperstep;"
      ],
      "setters": [
        "private void initCache(Set<Map.Entry<String, DistributedCache.DistributedCacheEntry>> files) {\n    for (Map.Entry<String, DistributedCache.DistributedCacheEntry> file : files) {\n        Future<Path> doNothing = new CompletedFuture(new Path(file.getValue().filePath));\n        cachedFiles.put(file.getKey(), doNothing);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.lang3.ArrayUtils",
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.InvalidProgramException",
        "org.apache.flink.api.common.JobExecutionResult",
        "org.apache.flink.api.common.JobID",
        "org.apache.flink.api.common.JobInfo",
        "org.apache.flink.api.common.JobInfoImpl",
        "org.apache.flink.api.common.Plan",
        "org.apache.flink.api.common.TaskInfo",
        "org.apache.flink.api.common.TaskInfoImpl",
        "org.apache.flink.api.common.accumulators.Accumulator",
        "org.apache.flink.api.common.accumulators.AccumulatorHelper",
        "org.apache.flink.api.common.aggregators.Aggregator",
        "org.apache.flink.api.common.aggregators.AggregatorRegistry",
        "org.apache.flink.api.common.aggregators.AggregatorWithName",
        "org.apache.flink.api.common.aggregators.ConvergenceCriterion",
        "org.apache.flink.api.common.cache.DistributedCache",
        "org.apache.flink.api.common.cache.DistributedCache.DistributedCacheEntry",
        "org.apache.flink.api.common.functions.DefaultOpenContext",
        "org.apache.flink.api.common.functions.Function",
        "org.apache.flink.api.common.functions.GroupCombineFunction",
        "org.apache.flink.api.common.functions.OpenContext",
        "org.apache.flink.api.common.functions.RichFunction",
        "org.apache.flink.api.common.functions.RuntimeContext",
        "org.apache.flink.api.common.functions.util.CopyingListCollector",
        "org.apache.flink.api.common.functions.util.FunctionUtils",
        "org.apache.flink.api.common.functions.util.RuntimeUDFContext",
        "org.apache.flink.api.common.operators.CollectionExecutor.CompletedFuture",
        "org.apache.flink.api.common.operators.CollectionExecutor.DynamicPathCollector",
        "org.apache.flink.api.common.operators.base.BulkIterationBase",
        "org.apache.flink.api.common.operators.base.DeltaIterationBase",
        "org.apache.flink.api.common.operators.base.GroupCombineOperatorBase",
        "org.apache.flink.api.common.operators.util.ListKeyGroupedIterator",
        "org.apache.flink.api.common.operators.util.ListKeyGroupedIterator.ValuesIterator",
        "org.apache.flink.api.common.operators.util.TypeComparable",
        "org.apache.flink.api.common.operators.util.UserCodeWrapper",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.fs.FileSystem",
        "org.apache.flink.core.fs.Path",
        "org.apache.flink.core.fs.local.LocalFileSystem",
        "org.apache.flink.metrics.groups.OperatorMetricGroup",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.Collector",
        "org.apache.flink.util.OptionalFailure",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.Visitable",
        "org.apache.flink.util.Visitor"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators;\n\npublic class CollectionExecutor_GroupCombineOperatorBaseexecuteOnCollections_ArrayUtilsaddAllFikaTest {\n\n    @Test\n    public void testExecute() {\n    }\n}",
      "conditionCount": 17,
      "callCount": 2,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
      "thirdPartyMethod": "org.apache.commons.lang3.ArrayUtils.addAll(boolean[], boolean[])",
      "directCaller": "org.apache.flink.api.common.operators.base.GroupReduceOperatorBase.executeOnCollections(java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
      "path": [
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.executeUnaryOperator(org.apache.flink.api.common.operators.SingleInputOperator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.base.GroupReduceOperatorBase.executeOnCollections(java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
        "org.apache.commons.lang3.ArrayUtils.addAll(boolean[], boolean[])"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// General execution methods\n// --------------------------------------------------------------------------------------------\npublic JobExecutionResult execute(Plan program) throws Exception {\n    long startTime = System.currentTimeMillis();\n    JobID jobID = (program.getJobId() == null) ? new JobID() : program.getJobId();\n    JobInfo jobInfo = new JobInfoImpl(jobID, program.getJobName());\n    initCache(program.getCachedFiles());\n    Collection<? extends GenericDataSinkBase<?>> sinks = program.getDataSinks();\n    for (Operator<?> sink : sinks) // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    {\n        execute(sink, jobInfo);\n    }\n    long endTime = System.currentTimeMillis();\n    Map<String, OptionalFailure<Object>> accumulatorResults = AccumulatorHelper.toResultMap(accumulators);\n    return new JobExecutionResult(null, endTime - startTime, accumulatorResults);\n}",
        "private List<?> execute(Operator<?> operator, JobInfo jobInfo) throws Exception {\n    // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    return execute(operator, 0, jobInfo);\n}",
        "private List<?> execute(Operator<?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    List<?> result = this.intermediateResults.get(operator);\n    // if it has already been computed, use the cached variant\n    if (result != null) {\n        return result;\n    }\n    if (operator instanceof BulkIterationBase) {\n        result = executeBulkIteration(((BulkIterationBase<?>) (operator)), jobInfo);\n    } else if (operator instanceof DeltaIterationBase) {\n        result = executeDeltaIteration(((DeltaIterationBase<?, ?>) (operator)), jobInfo);\n    } else if (operator instanceof SingleInputOperator) {\n        // PATH: Test should invoke the next CollectionExecutor.executeUnaryOperator(...) [step in execution path]\n        result = executeUnaryOperator(((SingleInputOperator<?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof DualInputOperator) {\n        result = executeBinaryOperator(((DualInputOperator<?, ?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSourceBase) {\n        result = executeDataSource(((GenericDataSourceBase<?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSinkBase) {\n        executeDataSink(((GenericDataSinkBase<?>) (operator)), superStep, jobInfo);\n        result = Collections.emptyList();\n    } else {\n        throw new RuntimeException(\"Cannot execute operator \" + operator.getClass().getName());\n    }\n    this.intermediateResults.put(operator, result);\n    return result;\n}",
        "private <IN, OUT> List<OUT> executeUnaryOperator(SingleInputOperator<?, ?, ?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    Operator<?> inputOp = operator.getInput();\n    if (inputOp == null) {\n        throw new InvalidProgramException((\"The unary operation \" + operator.getName()) + \" has no input.\");\n    }\n    @SuppressWarnings(\"unchecked\")\n    List<IN> inputData = ((List<IN>) (execute(inputOp, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    SingleInputOperator<IN, OUT, ?> typedOp = ((SingleInputOperator<IN, OUT, ?>) (operator));\n    // build the runtime context and compute broadcast variables, if necessary\n    TaskInfo taskInfo = new TaskInfoImpl(typedOp.getName(), 1, 0, 1, 0);\n    RuntimeUDFContext ctx;\n    if (RichFunction.class.isAssignableFrom(typedOp.getUserCodeWrapper().getUserCodeClass())) {\n        ctx = createContext(superStep, taskInfo, jobInfo);\n        for (Map.Entry<String, Operator<?>> bcInputs : operator.getBroadcastInputs().entrySet()) {\n            List<?> bcData = execute(bcInputs.getValue(), jobInfo);\n            ctx.setBroadcastVariable(bcInputs.getKey(), bcData);\n        }\n    } else {\n        ctx = null;\n    }\n    // PATH: Test should invoke the next GroupReduceOperatorBase.executeOnCollections(...) [step in execution path]\n    return typedOp.executeOnCollections(inputData, ctx, executionConfig);\n}",
        "// --------------------------------------------------------------------------------------------\n@Override\nprotected List<OUT> executeOnCollections(List<IN> inputData, RuntimeContext ctx, ExecutionConfig executionConfig) throws Exception {\n    GroupReduceFunction<IN, OUT> function = this.userFunction.getUserCodeObject();\n    UnaryOperatorInformation<IN, OUT> operatorInfo = getOperatorInfo();\n    TypeInformation<IN> inputType = operatorInfo.getInputType();\n    int[] keyColumns = getKeyColumns(0);\n    int[] sortColumns = keyColumns;\n    boolean[] sortOrderings = new boolean[sortColumns.length];\n    if (groupOrder != null) {\n        // PATH: Test should invoke the next ArrayUtils.addAll(...) [step in execution path]\n        sortColumns = ArrayUtils.addAll(sortColumns, groupOrder.getFieldPositions());\n        sortOrderings = ArrayUtils.addAll(sortOrderings, groupOrder.getFieldSortDirections());\n    }\n    if (sortColumns.length == 0) {\n        // => all reduce. No comparator\n        checkArgument(sortOrderings.length == 0);\n    } else {\n        final TypeComparator<IN> sortComparator = getTypeComparator(inputType, sortColumns, sortOrderings, executionConfig);\n        Collections.sort(inputData, new Comparator<IN>() {\n            @Override\n            public int compare(IN o1, IN o2) {\n                return sortComparator.compare(o1, o2);\n            }\n        });\n    }\n    FunctionUtils.setFunctionRuntimeContext(function, ctx);\n    FunctionUtils.openFunction(function, DefaultOpenContext.INSTANCE);\n    ArrayList<OUT> result = new ArrayList<OUT>();\n    if (inputData.size() > 0) {\n        final TypeSerializer<IN> inputSerializer = inputType.createSerializer(executionConfig.getSerializerConfig());\n        if (keyColumns.length == 0) {\n            TypeSerializer<OUT> outSerializer = getOperatorInfo().getOutputType().createSerializer(executionConfig.getSerializerConfig());\n            List<IN> inputDataCopy = new ArrayList<IN>(inputData.size());\n            for (IN in : inputData) {\n                inputDataCopy.add(inputSerializer.copy(in));\n            }\n            CopyingListCollector<OUT> collector = new CopyingListCollector<OUT>(result, outSerializer);\n            function.reduce(inputDataCopy, collector);\n        } else {\n            boolean[] keyOrderings = new boolean[keyColumns.length];\n            final TypeComparator<IN> comparator = getTypeComparator(inputType, keyColumns, keyOrderings, executionConfig);\n            ListKeyGroupedIterator<IN> keyedIterator = new ListKeyGroupedIterator<IN>(inputData, inputSerializer, comparator);\n            TypeSerializer<OUT> outSerializer = getOperatorInfo().getOutputType().createSerializer(executionConfig.getSerializerConfig());\n            CopyingListCollector<OUT> collector = new CopyingListCollector<OUT>(result, outSerializer);\n            while (keyedIterator.nextKey()) {\n                function.reduce(keyedIterator.getValues(), collector);\n            } \n        }\n    }\n    FunctionUtils.closeFunction(function);\n    return result;\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic CollectionExecutor(ExecutionConfig executionConfig) {\n    this.executionConfig = executionConfig;\n    this.intermediateResults = new HashMap<Operator<?>, List<?>>();\n    this.accumulators = new HashMap<String, Accumulator<?, ?>>();\n    this.previousAggregates = new HashMap<String, Value>();\n    this.aggregators = new HashMap<String, Aggregator<?>>();\n    this.cachedFiles = new HashMap<String, Future<Path>>();\n    this.userCodeClassLoader = Thread.currentThread().getContextClassLoader();\n}",
        "public DynamicPathCollector(Set<Operator<?>> dynamicPathOperations) {\n    this.dynamicPathOperations = dynamicPathOperations;\n}",
        "public IterationRuntimeUDFContext(JobInfo jobInfo, TaskInfo taskInfo, ClassLoader classloader, ExecutionConfig executionConfig, Map<String, Future<Path>> cpTasks, Map<String, Accumulator<?, ?>> accumulators, OperatorMetricGroup metrics) {\n    super(jobInfo, taskInfo, classloader, executionConfig, cpTasks, accumulators, metrics);\n}",
        "public CompletedFuture(Path entry) {\n    try {\n        LocalFileSystem fs = ((LocalFileSystem) (FileSystem.getUnguardedFileSystem(entry.toUri())));\n        result = (entry.isAbsolute()) ? new Path(entry.toUri().getPath()) : new Path(fs.getWorkingDirectory(), entry);\n    } catch (Exception e) {\n        throw new RuntimeException(\"DistributedCache supports only local files for Collection Environments\");\n    }\n}"
      ],
      "fieldDeclarations": [
        "private final Map<Operator<?>, List<?>> intermediateResults;",
        "private final Map<String, Accumulator<?, ?>> accumulators;",
        "private final Map<String, Future<Path>> cachedFiles;",
        "private final Map<String, Value> previousAggregates;",
        "private final Map<String, Aggregator<?>> aggregators;",
        "private final ClassLoader userCodeClassLoader;",
        "private final ExecutionConfig executionConfig;",
        "private int iterationSuperstep;"
      ],
      "setters": [
        "private void initCache(Set<Map.Entry<String, DistributedCache.DistributedCacheEntry>> files) {\n    for (Map.Entry<String, DistributedCache.DistributedCacheEntry> file : files) {\n        Future<Path> doNothing = new CompletedFuture(new Path(file.getValue().filePath));\n        cachedFiles.put(file.getKey(), doNothing);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.lang3.ArrayUtils",
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.InvalidProgramException",
        "org.apache.flink.api.common.JobExecutionResult",
        "org.apache.flink.api.common.JobID",
        "org.apache.flink.api.common.JobInfo",
        "org.apache.flink.api.common.JobInfoImpl",
        "org.apache.flink.api.common.Plan",
        "org.apache.flink.api.common.TaskInfo",
        "org.apache.flink.api.common.TaskInfoImpl",
        "org.apache.flink.api.common.accumulators.Accumulator",
        "org.apache.flink.api.common.accumulators.AccumulatorHelper",
        "org.apache.flink.api.common.aggregators.Aggregator",
        "org.apache.flink.api.common.aggregators.AggregatorRegistry",
        "org.apache.flink.api.common.aggregators.AggregatorWithName",
        "org.apache.flink.api.common.aggregators.ConvergenceCriterion",
        "org.apache.flink.api.common.cache.DistributedCache",
        "org.apache.flink.api.common.cache.DistributedCache.DistributedCacheEntry",
        "org.apache.flink.api.common.functions.DefaultOpenContext",
        "org.apache.flink.api.common.functions.Function",
        "org.apache.flink.api.common.functions.GroupReduceFunction",
        "org.apache.flink.api.common.functions.OpenContext",
        "org.apache.flink.api.common.functions.RichFunction",
        "org.apache.flink.api.common.functions.RuntimeContext",
        "org.apache.flink.api.common.functions.util.CopyingListCollector",
        "org.apache.flink.api.common.functions.util.FunctionUtils",
        "org.apache.flink.api.common.functions.util.RuntimeUDFContext",
        "org.apache.flink.api.common.operators.CollectionExecutor.CompletedFuture",
        "org.apache.flink.api.common.operators.CollectionExecutor.DynamicPathCollector",
        "org.apache.flink.api.common.operators.base.BulkIterationBase",
        "org.apache.flink.api.common.operators.base.DeltaIterationBase",
        "org.apache.flink.api.common.operators.base.GroupReduceOperatorBase",
        "org.apache.flink.api.common.operators.util.ListKeyGroupedIterator",
        "org.apache.flink.api.common.operators.util.ListKeyGroupedIterator.ValuesIterator",
        "org.apache.flink.api.common.operators.util.TypeComparable",
        "org.apache.flink.api.common.operators.util.UserCodeWrapper",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.fs.FileSystem",
        "org.apache.flink.core.fs.Path",
        "org.apache.flink.core.fs.local.LocalFileSystem",
        "org.apache.flink.metrics.groups.OperatorMetricGroup",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.Collector",
        "org.apache.flink.util.OptionalFailure",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.Visitable",
        "org.apache.flink.util.Visitor"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators;\n\npublic class CollectionExecutor_GroupReduceOperatorBaseexecuteOnCollections_ArrayUtilsaddAllFikaTest {\n\n    @Test\n    public void testExecute() {\n    }\n}",
      "conditionCount": 18,
      "callCount": 2,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
      "thirdPartyMethod": "org.apache.commons.lang3.ArrayUtils.addAll(int[], int[])",
      "directCaller": "org.apache.flink.api.common.operators.base.GroupReduceOperatorBase.executeOnCollections(java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
      "path": [
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.executeUnaryOperator(org.apache.flink.api.common.operators.SingleInputOperator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.base.GroupReduceOperatorBase.executeOnCollections(java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
        "org.apache.commons.lang3.ArrayUtils.addAll(int[], int[])"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// General execution methods\n// --------------------------------------------------------------------------------------------\npublic JobExecutionResult execute(Plan program) throws Exception {\n    long startTime = System.currentTimeMillis();\n    JobID jobID = (program.getJobId() == null) ? new JobID() : program.getJobId();\n    JobInfo jobInfo = new JobInfoImpl(jobID, program.getJobName());\n    initCache(program.getCachedFiles());\n    Collection<? extends GenericDataSinkBase<?>> sinks = program.getDataSinks();\n    for (Operator<?> sink : sinks) // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    {\n        execute(sink, jobInfo);\n    }\n    long endTime = System.currentTimeMillis();\n    Map<String, OptionalFailure<Object>> accumulatorResults = AccumulatorHelper.toResultMap(accumulators);\n    return new JobExecutionResult(null, endTime - startTime, accumulatorResults);\n}",
        "private List<?> execute(Operator<?> operator, JobInfo jobInfo) throws Exception {\n    // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    return execute(operator, 0, jobInfo);\n}",
        "private List<?> execute(Operator<?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    List<?> result = this.intermediateResults.get(operator);\n    // if it has already been computed, use the cached variant\n    if (result != null) {\n        return result;\n    }\n    if (operator instanceof BulkIterationBase) {\n        result = executeBulkIteration(((BulkIterationBase<?>) (operator)), jobInfo);\n    } else if (operator instanceof DeltaIterationBase) {\n        result = executeDeltaIteration(((DeltaIterationBase<?, ?>) (operator)), jobInfo);\n    } else if (operator instanceof SingleInputOperator) {\n        // PATH: Test should invoke the next CollectionExecutor.executeUnaryOperator(...) [step in execution path]\n        result = executeUnaryOperator(((SingleInputOperator<?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof DualInputOperator) {\n        result = executeBinaryOperator(((DualInputOperator<?, ?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSourceBase) {\n        result = executeDataSource(((GenericDataSourceBase<?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSinkBase) {\n        executeDataSink(((GenericDataSinkBase<?>) (operator)), superStep, jobInfo);\n        result = Collections.emptyList();\n    } else {\n        throw new RuntimeException(\"Cannot execute operator \" + operator.getClass().getName());\n    }\n    this.intermediateResults.put(operator, result);\n    return result;\n}",
        "private <IN, OUT> List<OUT> executeUnaryOperator(SingleInputOperator<?, ?, ?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    Operator<?> inputOp = operator.getInput();\n    if (inputOp == null) {\n        throw new InvalidProgramException((\"The unary operation \" + operator.getName()) + \" has no input.\");\n    }\n    @SuppressWarnings(\"unchecked\")\n    List<IN> inputData = ((List<IN>) (execute(inputOp, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    SingleInputOperator<IN, OUT, ?> typedOp = ((SingleInputOperator<IN, OUT, ?>) (operator));\n    // build the runtime context and compute broadcast variables, if necessary\n    TaskInfo taskInfo = new TaskInfoImpl(typedOp.getName(), 1, 0, 1, 0);\n    RuntimeUDFContext ctx;\n    if (RichFunction.class.isAssignableFrom(typedOp.getUserCodeWrapper().getUserCodeClass())) {\n        ctx = createContext(superStep, taskInfo, jobInfo);\n        for (Map.Entry<String, Operator<?>> bcInputs : operator.getBroadcastInputs().entrySet()) {\n            List<?> bcData = execute(bcInputs.getValue(), jobInfo);\n            ctx.setBroadcastVariable(bcInputs.getKey(), bcData);\n        }\n    } else {\n        ctx = null;\n    }\n    // PATH: Test should invoke the next GroupReduceOperatorBase.executeOnCollections(...) [step in execution path]\n    return typedOp.executeOnCollections(inputData, ctx, executionConfig);\n}",
        "// --------------------------------------------------------------------------------------------\n@Override\nprotected List<OUT> executeOnCollections(List<IN> inputData, RuntimeContext ctx, ExecutionConfig executionConfig) throws Exception {\n    GroupReduceFunction<IN, OUT> function = this.userFunction.getUserCodeObject();\n    UnaryOperatorInformation<IN, OUT> operatorInfo = getOperatorInfo();\n    TypeInformation<IN> inputType = operatorInfo.getInputType();\n    int[] keyColumns = getKeyColumns(0);\n    int[] sortColumns = keyColumns;\n    boolean[] sortOrderings = new boolean[sortColumns.length];\n    if (groupOrder != null) {\n        // PATH: Test should invoke the next ArrayUtils.addAll(...) [step in execution path]\n        sortColumns = ArrayUtils.addAll(sortColumns, groupOrder.getFieldPositions());\n        sortOrderings = ArrayUtils.addAll(sortOrderings, groupOrder.getFieldSortDirections());\n    }\n    if (sortColumns.length == 0) {\n        // => all reduce. No comparator\n        checkArgument(sortOrderings.length == 0);\n    } else {\n        final TypeComparator<IN> sortComparator = getTypeComparator(inputType, sortColumns, sortOrderings, executionConfig);\n        Collections.sort(inputData, new Comparator<IN>() {\n            @Override\n            public int compare(IN o1, IN o2) {\n                return sortComparator.compare(o1, o2);\n            }\n        });\n    }\n    FunctionUtils.setFunctionRuntimeContext(function, ctx);\n    FunctionUtils.openFunction(function, DefaultOpenContext.INSTANCE);\n    ArrayList<OUT> result = new ArrayList<OUT>();\n    if (inputData.size() > 0) {\n        final TypeSerializer<IN> inputSerializer = inputType.createSerializer(executionConfig.getSerializerConfig());\n        if (keyColumns.length == 0) {\n            TypeSerializer<OUT> outSerializer = getOperatorInfo().getOutputType().createSerializer(executionConfig.getSerializerConfig());\n            List<IN> inputDataCopy = new ArrayList<IN>(inputData.size());\n            for (IN in : inputData) {\n                inputDataCopy.add(inputSerializer.copy(in));\n            }\n            CopyingListCollector<OUT> collector = new CopyingListCollector<OUT>(result, outSerializer);\n            function.reduce(inputDataCopy, collector);\n        } else {\n            boolean[] keyOrderings = new boolean[keyColumns.length];\n            final TypeComparator<IN> comparator = getTypeComparator(inputType, keyColumns, keyOrderings, executionConfig);\n            ListKeyGroupedIterator<IN> keyedIterator = new ListKeyGroupedIterator<IN>(inputData, inputSerializer, comparator);\n            TypeSerializer<OUT> outSerializer = getOperatorInfo().getOutputType().createSerializer(executionConfig.getSerializerConfig());\n            CopyingListCollector<OUT> collector = new CopyingListCollector<OUT>(result, outSerializer);\n            while (keyedIterator.nextKey()) {\n                function.reduce(keyedIterator.getValues(), collector);\n            } \n        }\n    }\n    FunctionUtils.closeFunction(function);\n    return result;\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic CollectionExecutor(ExecutionConfig executionConfig) {\n    this.executionConfig = executionConfig;\n    this.intermediateResults = new HashMap<Operator<?>, List<?>>();\n    this.accumulators = new HashMap<String, Accumulator<?, ?>>();\n    this.previousAggregates = new HashMap<String, Value>();\n    this.aggregators = new HashMap<String, Aggregator<?>>();\n    this.cachedFiles = new HashMap<String, Future<Path>>();\n    this.userCodeClassLoader = Thread.currentThread().getContextClassLoader();\n}",
        "public DynamicPathCollector(Set<Operator<?>> dynamicPathOperations) {\n    this.dynamicPathOperations = dynamicPathOperations;\n}",
        "public IterationRuntimeUDFContext(JobInfo jobInfo, TaskInfo taskInfo, ClassLoader classloader, ExecutionConfig executionConfig, Map<String, Future<Path>> cpTasks, Map<String, Accumulator<?, ?>> accumulators, OperatorMetricGroup metrics) {\n    super(jobInfo, taskInfo, classloader, executionConfig, cpTasks, accumulators, metrics);\n}",
        "public CompletedFuture(Path entry) {\n    try {\n        LocalFileSystem fs = ((LocalFileSystem) (FileSystem.getUnguardedFileSystem(entry.toUri())));\n        result = (entry.isAbsolute()) ? new Path(entry.toUri().getPath()) : new Path(fs.getWorkingDirectory(), entry);\n    } catch (Exception e) {\n        throw new RuntimeException(\"DistributedCache supports only local files for Collection Environments\");\n    }\n}"
      ],
      "fieldDeclarations": [
        "private final Map<Operator<?>, List<?>> intermediateResults;",
        "private final Map<String, Accumulator<?, ?>> accumulators;",
        "private final Map<String, Future<Path>> cachedFiles;",
        "private final Map<String, Value> previousAggregates;",
        "private final Map<String, Aggregator<?>> aggregators;",
        "private final ClassLoader userCodeClassLoader;",
        "private final ExecutionConfig executionConfig;",
        "private int iterationSuperstep;"
      ],
      "setters": [
        "private void initCache(Set<Map.Entry<String, DistributedCache.DistributedCacheEntry>> files) {\n    for (Map.Entry<String, DistributedCache.DistributedCacheEntry> file : files) {\n        Future<Path> doNothing = new CompletedFuture(new Path(file.getValue().filePath));\n        cachedFiles.put(file.getKey(), doNothing);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.lang3.ArrayUtils",
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.InvalidProgramException",
        "org.apache.flink.api.common.JobExecutionResult",
        "org.apache.flink.api.common.JobID",
        "org.apache.flink.api.common.JobInfo",
        "org.apache.flink.api.common.JobInfoImpl",
        "org.apache.flink.api.common.Plan",
        "org.apache.flink.api.common.TaskInfo",
        "org.apache.flink.api.common.TaskInfoImpl",
        "org.apache.flink.api.common.accumulators.Accumulator",
        "org.apache.flink.api.common.accumulators.AccumulatorHelper",
        "org.apache.flink.api.common.aggregators.Aggregator",
        "org.apache.flink.api.common.aggregators.AggregatorRegistry",
        "org.apache.flink.api.common.aggregators.AggregatorWithName",
        "org.apache.flink.api.common.aggregators.ConvergenceCriterion",
        "org.apache.flink.api.common.cache.DistributedCache",
        "org.apache.flink.api.common.cache.DistributedCache.DistributedCacheEntry",
        "org.apache.flink.api.common.functions.DefaultOpenContext",
        "org.apache.flink.api.common.functions.Function",
        "org.apache.flink.api.common.functions.GroupReduceFunction",
        "org.apache.flink.api.common.functions.OpenContext",
        "org.apache.flink.api.common.functions.RichFunction",
        "org.apache.flink.api.common.functions.RuntimeContext",
        "org.apache.flink.api.common.functions.util.CopyingListCollector",
        "org.apache.flink.api.common.functions.util.FunctionUtils",
        "org.apache.flink.api.common.functions.util.RuntimeUDFContext",
        "org.apache.flink.api.common.operators.CollectionExecutor.CompletedFuture",
        "org.apache.flink.api.common.operators.CollectionExecutor.DynamicPathCollector",
        "org.apache.flink.api.common.operators.base.BulkIterationBase",
        "org.apache.flink.api.common.operators.base.DeltaIterationBase",
        "org.apache.flink.api.common.operators.base.GroupReduceOperatorBase",
        "org.apache.flink.api.common.operators.util.ListKeyGroupedIterator",
        "org.apache.flink.api.common.operators.util.ListKeyGroupedIterator.ValuesIterator",
        "org.apache.flink.api.common.operators.util.TypeComparable",
        "org.apache.flink.api.common.operators.util.UserCodeWrapper",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.fs.FileSystem",
        "org.apache.flink.core.fs.Path",
        "org.apache.flink.core.fs.local.LocalFileSystem",
        "org.apache.flink.metrics.groups.OperatorMetricGroup",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.Collector",
        "org.apache.flink.util.OptionalFailure",
        "org.apache.flink.util.Preconditions",
        "org.apache.flink.util.Visitable",
        "org.apache.flink.util.Visitor"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators;\n\npublic class CollectionExecutor_GroupReduceOperatorBaseexecuteOnCollections_ArrayUtilsaddAllFikaTest {\n\n    @Test\n    public void testExecute() {\n    }\n}",
      "conditionCount": 18,
      "callCount": 2,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility(org.apache.flink.api.common.typeutils.TypeSerializerSnapshot)",
      "thirdPartyMethod": "com.esotericsoftware.minlog.Log.Logger.<init>()",
      "directCaller": "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
      "path": [
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility(org.apache.flink.api.common.typeutils.TypeSerializerSnapshot)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility(org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility(org.apache.flink.util.LinkedOptionalMap.MergeResult, org.apache.flink.util.LinkedOptionalMap.MergeResult, org.apache.flink.util.LinkedOptionalMap.MergeResult)",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging()",
        "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>(org.slf4j.Logger)",
        "com.esotericsoftware.minlog.Log.Logger.<init>()"
      ],
      "methodSources": [
        "@Override\npublic TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(TypeSerializerSnapshot<T> oldSerializerSnapshot) {\n    if (!(oldSerializerSnapshot instanceof KryoSerializerSnapshot)) {\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    KryoSerializerSnapshot<T> oldKryoSerializerSnapshot = ((KryoSerializerSnapshot<T>) (oldSerializerSnapshot));\n    if (snapshotData.getTypeClass() != oldKryoSerializerSnapshot.snapshotData.getTypeClass()) {\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // PATH: Test should invoke the next KryoSerializerSnapshot.resolveSchemaCompatibility(...) [step in execution path]\n    return resolveSchemaCompatibility(oldKryoSerializerSnapshot);\n}",
        "private TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(KryoSerializerSnapshot<T> oldKryoSerializerSnapshot) {\n    // merge the default serializers\n    final MergeResult<Class<?>, SerializableSerializer<?>> reconfiguredDefaultKryoSerializers = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getDefaultKryoSerializers(), snapshotData.getDefaultKryoSerializers());\n    if (reconfiguredDefaultKryoSerializers.hasMissingKeys()) {\n        logMissingKeys(reconfiguredDefaultKryoSerializers);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // merge default serializer classes\n    final MergeResult<Class<?>, Class<? extends Serializer<?>>> reconfiguredDefaultKryoSerializerClasses = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getDefaultKryoSerializerClasses(), snapshotData.getDefaultKryoSerializerClasses());\n    if (reconfiguredDefaultKryoSerializerClasses.hasMissingKeys()) {\n        logMissingKeys(reconfiguredDefaultKryoSerializerClasses);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // merge registration\n    final MergeResult<String, KryoRegistration> reconfiguredRegistrations = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getKryoRegistrations(), snapshotData.getKryoRegistrations());\n    if (reconfiguredRegistrations.hasMissingKeys()) {\n        logMissingKeys(reconfiguredRegistrations);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // there are no missing keys, now we have to decide whether we are compatible as-is or we\n    // require reconfiguration.\n    // PATH: Test should invoke the next KryoSerializerSnapshot.resolveSchemaCompatibility(...) [step in execution path]\n    return resolveSchemaCompatibility(reconfiguredDefaultKryoSerializers, reconfiguredDefaultKryoSerializerClasses, reconfiguredRegistrations);\n}",
        "private TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(MergeResult<Class<?>, SerializableSerializer<?>> reconfiguredDefaultKryoSerializers, MergeResult<Class<?>, Class<? extends Serializer<?>>> reconfiguredDefaultKryoSerializerClasses, MergeResult<String, KryoRegistration> reconfiguredRegistrations) {\n    if ((reconfiguredDefaultKryoSerializers.isOrderedSubset() && reconfiguredDefaultKryoSerializerClasses.isOrderedSubset()) && reconfiguredRegistrations.isOrderedSubset()) {\n        return TypeSerializerSchemaCompatibility.compatibleAsIs();\n    }\n    // reconfigure a new KryoSerializer\n    KryoSerializer<T> reconfiguredSerializer = new KryoSerializer<>(snapshotData.getTypeClass(), reconfiguredDefaultKryoSerializers.getMerged(), reconfiguredDefaultKryoSerializerClasses.getMerged(), reconfiguredRegistrations.getMerged());\n    return TypeSerializerSchemaCompatibility.compatibleWithReconfiguredSerializer(reconfiguredSerializer);\n}",
        "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer\n\n// Static field initializations\nprivate static final long serialVersionUID = 3L;\nprivate static final Logger LOG = LoggerFactory.getLogger(KryoSerializer.class);\n/**\n * Flag whether to check for concurrent thread access. Because this flag is static final, a\n * value of 'false' allows the JIT compiler to eliminate the guarded code sections.\n */\nprivate static final boolean CONCURRENT_ACCESS_CHECK = LOG.isDebugEnabled() || KryoSerializerDebugInitHelper.setToDebug;\n@Nullable\nprivate static final ChillSerializerRegistrar flinkChillPackageRegistrar = loadFlinkChillPackageRegistrar();\n\n// Static initializer blocks\nstatic static {\n    configureKryoLogging();\n}\n",
        "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}",
        "(Logger log) // PATH: Test should invoke the next new Log$Logger(...) [step in execution path]\n{\n    this.log = checkNotNull(log);\n}"
      ],
      "constructors": [
        "@SuppressWarnings(\"unused\")\npublic KryoSerializerSnapshot() {\n}",
        "KryoSerializerSnapshot(Class<T> typeClass, LinkedHashMap<Class<?>, SerializableSerializer<?>> defaultKryoSerializers, LinkedHashMap<Class<?>, Class<? extends Serializer<?>>> defaultKryoSerializerClasses, LinkedHashMap<String, KryoRegistration> kryoRegistrations) {\n    this.snapshotData = createFrom(typeClass, defaultKryoSerializers, defaultKryoSerializerClasses, kryoRegistrations);\n}"
      ],
      "fieldDeclarations": [
        "private static final Logger LOG = LoggerFactory.getLogger(KryoSerializerSnapshot.class);",
        "private static final int VERSION = 2;",
        "private KryoSerializerSnapshotData<T> snapshotData;"
      ],
      "setters": [
        "private void logMissingKeys(MergeResult<?, ?> mergeResult) {\n    mergeResult.missingKeys().forEach(key -> LOG.warn((\"The Kryo registration for a previously registered class {} does not have a \" + \"proper serializer, because its previous serializer cannot be loaded or is no \") + \"longer valid but a new serializer is not available\", key));\n}",
        "@Override\npublic void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException {\n    this.snapshotData = createFrom(in, userCodeClassLoader);\n}",
        "@Override\npublic void writeSnapshot(DataOutputView out) throws IOException {\n    snapshotData.writeSnapshotData(out);\n}"
      ],
      "imports": [
        "com.esotericsoftware.kryo.Serializer",
        "com.esotericsoftware.minlog.Log",
        "com.esotericsoftware.minlog.Log.Logger",
        "org.apache.flink.api.common.SerializableSerializer",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.api.common.typeutils.TypeSerializerSchemaCompatibility",
        "org.apache.flink.api.common.typeutils.TypeSerializerSnapshot",
        "org.apache.flink.api.java.typeutils.runtime.KryoRegistration",
        "org.apache.flink.core.memory.DataInputView",
        "org.apache.flink.core.memory.DataOutputView",
        "org.apache.flink.util.LinkedOptionalMap",
        "org.apache.flink.util.LinkedOptionalMap.MergeResult",
        "org.apache.flink.util.Preconditions",
        "org.slf4j.Logger",
        "org.slf4j.LoggerFactory"
      ],
      "testTemplate": "package org.apache.flink.api.java.typeutils.runtime.kryo;\n\npublic class KryoSerializerSnapshot_MinlogForwardermethod_LoggermethodFikaTest {\n\n    @Test\n    public void testResolveSchemaCompatibility() {\n    }\n}",
      "conditionCount": 7,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
      "thirdPartyMethod": "org.apache.commons.collections.iterators.ListIteratorWrapper.<init>(java.util.Iterator)",
      "directCaller": "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.next()",
      "path": [
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.executeBinaryOperator(org.apache.flink.api.common.operators.DualInputOperator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.executeOnCollections(java.util.List, java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.next()",
        "org.apache.commons.collections.iterators.ListIteratorWrapper.<init>(java.util.Iterator)"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// General execution methods\n// --------------------------------------------------------------------------------------------\npublic JobExecutionResult execute(Plan program) throws Exception {\n    long startTime = System.currentTimeMillis();\n    JobID jobID = (program.getJobId() == null) ? new JobID() : program.getJobId();\n    JobInfo jobInfo = new JobInfoImpl(jobID, program.getJobName());\n    initCache(program.getCachedFiles());\n    Collection<? extends GenericDataSinkBase<?>> sinks = program.getDataSinks();\n    for (Operator<?> sink : sinks) // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    {\n        execute(sink, jobInfo);\n    }\n    long endTime = System.currentTimeMillis();\n    Map<String, OptionalFailure<Object>> accumulatorResults = AccumulatorHelper.toResultMap(accumulators);\n    return new JobExecutionResult(null, endTime - startTime, accumulatorResults);\n}",
        "private List<?> execute(Operator<?> operator, JobInfo jobInfo) throws Exception {\n    // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    return execute(operator, 0, jobInfo);\n}",
        "private List<?> execute(Operator<?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    List<?> result = this.intermediateResults.get(operator);\n    // if it has already been computed, use the cached variant\n    if (result != null) {\n        return result;\n    }\n    if (operator instanceof BulkIterationBase) {\n        result = executeBulkIteration(((BulkIterationBase<?>) (operator)), jobInfo);\n    } else if (operator instanceof DeltaIterationBase) {\n        result = executeDeltaIteration(((DeltaIterationBase<?, ?>) (operator)), jobInfo);\n    } else if (operator instanceof SingleInputOperator) {\n        result = executeUnaryOperator(((SingleInputOperator<?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof DualInputOperator) {\n        // PATH: Test should invoke the next CollectionExecutor.executeBinaryOperator(...) [step in execution path]\n        result = executeBinaryOperator(((DualInputOperator<?, ?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSourceBase) {\n        result = executeDataSource(((GenericDataSourceBase<?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSinkBase) {\n        executeDataSink(((GenericDataSinkBase<?>) (operator)), superStep, jobInfo);\n        result = Collections.emptyList();\n    } else {\n        throw new RuntimeException(\"Cannot execute operator \" + operator.getClass().getName());\n    }\n    this.intermediateResults.put(operator, result);\n    return result;\n}",
        "private <IN1, IN2, OUT> List<OUT> executeBinaryOperator(DualInputOperator<?, ?, ?, ?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    Operator<?> inputOp1 = operator.getFirstInput();\n    Operator<?> inputOp2 = operator.getSecondInput();\n    if (inputOp1 == null) {\n        throw new InvalidProgramException((\"The binary operation \" + operator.getName()) + \" has no first input.\");\n    }\n    if (inputOp2 == null) {\n        throw new InvalidProgramException((\"The binary operation \" + operator.getName()) + \" has no second input.\");\n    }\n    // compute inputs\n    @SuppressWarnings(\"unchecked\")\n    List<IN1> inputData1 = ((List<IN1>) (execute(inputOp1, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    List<IN2> inputData2 = ((List<IN2>) (execute(inputOp2, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    DualInputOperator<IN1, IN2, OUT, ?> typedOp = ((DualInputOperator<IN1, IN2, OUT, ?>) (operator));\n    // build the runtime context and compute broadcast variables, if necessary\n    TaskInfo taskInfo = new TaskInfoImpl(typedOp.getName(), 1, 0, 1, 0);\n    RuntimeUDFContext ctx;\n    if (RichFunction.class.isAssignableFrom(typedOp.getUserCodeWrapper().getUserCodeClass())) {\n        ctx = createContext(superStep, taskInfo, jobInfo);\n        for (Map.Entry<String, Operator<?>> bcInputs : operator.getBroadcastInputs().entrySet()) {\n            List<?> bcData = execute(bcInputs.getValue(), jobInfo);\n            ctx.setBroadcastVariable(bcInputs.getKey(), bcData);\n        }\n    } else {\n        ctx = null;\n    }\n    // PATH: Test should invoke the next OuterJoinOperatorBase.executeOnCollections(...) [step in execution path]\n    return typedOp.executeOnCollections(inputData1, inputData2, ctx, executionConfig);\n}",
        "@Override\nprotected List<OUT> executeOnCollections(List<IN1> leftInput, List<IN2> rightInput, RuntimeContext runtimeContext, ExecutionConfig executionConfig) throws Exception {\n    TypeInformation<IN1> leftInformation = getOperatorInfo().getFirstInputType();\n    TypeInformation<IN2> rightInformation = getOperatorInfo().getSecondInputType();\n    TypeInformation<OUT> outInformation = getOperatorInfo().getOutputType();\n    TypeComparator<IN1> leftComparator = buildComparatorFor(0, executionConfig, leftInformation);\n    TypeComparator<IN2> rightComparator = buildComparatorFor(1, executionConfig, rightInformation);\n    TypeSerializer<IN1> leftSerializer = leftInformation.createSerializer(executionConfig.getSerializerConfig());\n    TypeSerializer<IN2> rightSerializer = rightInformation.createSerializer(executionConfig.getSerializerConfig());\n    OuterJoinListIterator<IN1, IN2> outerJoinIterator = new OuterJoinListIterator<>(leftInput, leftSerializer, leftComparator, rightInput, rightSerializer, rightComparator, outerJoinType);\n    // --------------------------------------------------------------------\n    // Run UDF\n    // --------------------------------------------------------------------\n    FlatJoinFunction<IN1, IN2, OUT> function = userFunction.getUserCodeObject();\n    FunctionUtils.setFunctionRuntimeContext(function, runtimeContext);\n    FunctionUtils.openFunction(function, DefaultOpenContext.INSTANCE);\n    List<OUT> result = new ArrayList<>();\n    Collector<OUT> collector = new CopyingListCollector<>(result, outInformation.createSerializer(executionConfig.getSerializerConfig()));\n    // PATH: Test should invoke the next OuterJoinOperatorBase$OuterJoinListIterator.next(...) [step in execution path]\n    while (outerJoinIterator.next()) {\n        IN1 left = outerJoinIterator.getLeft();\n        IN2 right = outerJoinIterator.getRight();\n        function.join(left == null ? null : leftSerializer.copy(left), right == null ? null : rightSerializer.copy(right), collector);\n    } \n    FunctionUtils.closeFunction(function);\n    return result;\n}",
        "@SuppressWarnings(\"unchecked\")\nprivate boolean next() throws IOException {\n    boolean hasMoreElements;\n    if (((currLeftIterator == null) || (!currLeftIterator.hasNext())) && ((currRightIterator == null) || (!currRightIterator.hasNext()))) {\n        hasMoreElements = nextGroups(outerJoinType);\n        if (hasMoreElements) {\n            if (outerJoinType != OuterJoinType.LEFT) {\n                currLeftIterator = new ListIteratorWrapper(currLeftSubset.iterator());\n            }\n            leftReturn = ((IN1) (currLeftIterator.next()));\n            if (outerJoinType != OuterJoinType.RIGHT) {\n                currRightIterator = new ListIteratorWrapper(currRightSubset.iterator());\n            }\n            rightReturn = ((IN2) (currRightIterator.next()));\n            return true;\n        } else {\n            // no more elements\n            return false;\n        }\n    } else if (currLeftIterator.hasNext() && (!currRightIterator.hasNext())) {\n        leftReturn = ((IN1) (currLeftIterator.next()));\n        currRightIterator.reset();\n        rightReturn = ((IN2) (currRightIterator.next()));\n        return true;\n    } else {\n        rightReturn = ((IN2) (currRightIterator.next()));\n        return true;\n    }\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic CollectionExecutor(ExecutionConfig executionConfig) {\n    this.executionConfig = executionConfig;\n    this.intermediateResults = new HashMap<Operator<?>, List<?>>();\n    this.accumulators = new HashMap<String, Accumulator<?, ?>>();\n    this.previousAggregates = new HashMap<String, Value>();\n    this.aggregators = new HashMap<String, Aggregator<?>>();\n    this.cachedFiles = new HashMap<String, Future<Path>>();\n    this.userCodeClassLoader = Thread.currentThread().getContextClassLoader();\n}",
        "public DynamicPathCollector(Set<Operator<?>> dynamicPathOperations) {\n    this.dynamicPathOperations = dynamicPathOperations;\n}",
        "public IterationRuntimeUDFContext(JobInfo jobInfo, TaskInfo taskInfo, ClassLoader classloader, ExecutionConfig executionConfig, Map<String, Future<Path>> cpTasks, Map<String, Accumulator<?, ?>> accumulators, OperatorMetricGroup metrics) {\n    super(jobInfo, taskInfo, classloader, executionConfig, cpTasks, accumulators, metrics);\n}",
        "public CompletedFuture(Path entry) {\n    try {\n        LocalFileSystem fs = ((LocalFileSystem) (FileSystem.getUnguardedFileSystem(entry.toUri())));\n        result = (entry.isAbsolute()) ? new Path(entry.toUri().getPath()) : new Path(fs.getWorkingDirectory(), entry);\n    } catch (Exception e) {\n        throw new RuntimeException(\"DistributedCache supports only local files for Collection Environments\");\n    }\n}"
      ],
      "fieldDeclarations": [
        "private final Map<Operator<?>, List<?>> intermediateResults;",
        "private final Map<String, Accumulator<?, ?>> accumulators;",
        "private final Map<String, Future<Path>> cachedFiles;",
        "private final Map<String, Value> previousAggregates;",
        "private final Map<String, Aggregator<?>> aggregators;",
        "private final ClassLoader userCodeClassLoader;",
        "private final ExecutionConfig executionConfig;",
        "private int iterationSuperstep;"
      ],
      "setters": [
        "private void initCache(Set<Map.Entry<String, DistributedCache.DistributedCacheEntry>> files) {\n    for (Map.Entry<String, DistributedCache.DistributedCacheEntry> file : files) {\n        Future<Path> doNothing = new CompletedFuture(new Path(file.getValue().filePath));\n        cachedFiles.put(file.getKey(), doNothing);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.collections.ResettableIterator",
        "org.apache.commons.collections.iterators.ListIteratorWrapper",
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.InvalidProgramException",
        "org.apache.flink.api.common.JobExecutionResult",
        "org.apache.flink.api.common.JobID",
        "org.apache.flink.api.common.JobInfo",
        "org.apache.flink.api.common.JobInfoImpl",
        "org.apache.flink.api.common.Plan",
        "org.apache.flink.api.common.TaskInfo",
        "org.apache.flink.api.common.TaskInfoImpl",
        "org.apache.flink.api.common.accumulators.Accumulator",
        "org.apache.flink.api.common.accumulators.AccumulatorHelper",
        "org.apache.flink.api.common.aggregators.Aggregator",
        "org.apache.flink.api.common.aggregators.AggregatorRegistry",
        "org.apache.flink.api.common.aggregators.AggregatorWithName",
        "org.apache.flink.api.common.aggregators.ConvergenceCriterion",
        "org.apache.flink.api.common.cache.DistributedCache",
        "org.apache.flink.api.common.cache.DistributedCache.DistributedCacheEntry",
        "org.apache.flink.api.common.functions.DefaultOpenContext",
        "org.apache.flink.api.common.functions.FlatJoinFunction",
        "org.apache.flink.api.common.functions.Function",
        "org.apache.flink.api.common.functions.OpenContext",
        "org.apache.flink.api.common.functions.RichFunction",
        "org.apache.flink.api.common.functions.RuntimeContext",
        "org.apache.flink.api.common.functions.util.CopyingListCollector",
        "org.apache.flink.api.common.functions.util.FunctionUtils",
        "org.apache.flink.api.common.functions.util.RuntimeUDFContext",
        "org.apache.flink.api.common.operators.CollectionExecutor.CompletedFuture",
        "org.apache.flink.api.common.operators.CollectionExecutor.DynamicPathCollector",
        "org.apache.flink.api.common.operators.base.BulkIterationBase",
        "org.apache.flink.api.common.operators.base.DeltaIterationBase",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinType",
        "org.apache.flink.api.common.operators.util.TypeComparable",
        "org.apache.flink.api.common.operators.util.UserCodeWrapper",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.fs.FileSystem",
        "org.apache.flink.core.fs.Path",
        "org.apache.flink.core.fs.local.LocalFileSystem",
        "org.apache.flink.metrics.groups.OperatorMetricGroup",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.Collector",
        "org.apache.flink.util.OptionalFailure",
        "org.apache.flink.util.Visitable",
        "org.apache.flink.util.Visitor"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators;\n\npublic class CollectionExecutor_OuterJoinListIteratornext_ListIteratorWrappermethodFikaTest {\n\n    @Test\n    public void testExecute() {\n    }\n}",
      "conditionCount": 21,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
      "thirdPartyMethod": "org.apache.commons.collections.ResettableIterator.reset()",
      "directCaller": "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.next()",
      "path": [
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.executeBinaryOperator(org.apache.flink.api.common.operators.DualInputOperator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.executeOnCollections(java.util.List, java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.next()",
        "org.apache.commons.collections.ResettableIterator.reset()"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// General execution methods\n// --------------------------------------------------------------------------------------------\npublic JobExecutionResult execute(Plan program) throws Exception {\n    long startTime = System.currentTimeMillis();\n    JobID jobID = (program.getJobId() == null) ? new JobID() : program.getJobId();\n    JobInfo jobInfo = new JobInfoImpl(jobID, program.getJobName());\n    initCache(program.getCachedFiles());\n    Collection<? extends GenericDataSinkBase<?>> sinks = program.getDataSinks();\n    for (Operator<?> sink : sinks) // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    {\n        execute(sink, jobInfo);\n    }\n    long endTime = System.currentTimeMillis();\n    Map<String, OptionalFailure<Object>> accumulatorResults = AccumulatorHelper.toResultMap(accumulators);\n    return new JobExecutionResult(null, endTime - startTime, accumulatorResults);\n}",
        "private List<?> execute(Operator<?> operator, JobInfo jobInfo) throws Exception {\n    // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    return execute(operator, 0, jobInfo);\n}",
        "private List<?> execute(Operator<?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    List<?> result = this.intermediateResults.get(operator);\n    // if it has already been computed, use the cached variant\n    if (result != null) {\n        return result;\n    }\n    if (operator instanceof BulkIterationBase) {\n        result = executeBulkIteration(((BulkIterationBase<?>) (operator)), jobInfo);\n    } else if (operator instanceof DeltaIterationBase) {\n        result = executeDeltaIteration(((DeltaIterationBase<?, ?>) (operator)), jobInfo);\n    } else if (operator instanceof SingleInputOperator) {\n        result = executeUnaryOperator(((SingleInputOperator<?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof DualInputOperator) {\n        // PATH: Test should invoke the next CollectionExecutor.executeBinaryOperator(...) [step in execution path]\n        result = executeBinaryOperator(((DualInputOperator<?, ?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSourceBase) {\n        result = executeDataSource(((GenericDataSourceBase<?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSinkBase) {\n        executeDataSink(((GenericDataSinkBase<?>) (operator)), superStep, jobInfo);\n        result = Collections.emptyList();\n    } else {\n        throw new RuntimeException(\"Cannot execute operator \" + operator.getClass().getName());\n    }\n    this.intermediateResults.put(operator, result);\n    return result;\n}",
        "private <IN1, IN2, OUT> List<OUT> executeBinaryOperator(DualInputOperator<?, ?, ?, ?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    Operator<?> inputOp1 = operator.getFirstInput();\n    Operator<?> inputOp2 = operator.getSecondInput();\n    if (inputOp1 == null) {\n        throw new InvalidProgramException((\"The binary operation \" + operator.getName()) + \" has no first input.\");\n    }\n    if (inputOp2 == null) {\n        throw new InvalidProgramException((\"The binary operation \" + operator.getName()) + \" has no second input.\");\n    }\n    // compute inputs\n    @SuppressWarnings(\"unchecked\")\n    List<IN1> inputData1 = ((List<IN1>) (execute(inputOp1, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    List<IN2> inputData2 = ((List<IN2>) (execute(inputOp2, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    DualInputOperator<IN1, IN2, OUT, ?> typedOp = ((DualInputOperator<IN1, IN2, OUT, ?>) (operator));\n    // build the runtime context and compute broadcast variables, if necessary\n    TaskInfo taskInfo = new TaskInfoImpl(typedOp.getName(), 1, 0, 1, 0);\n    RuntimeUDFContext ctx;\n    if (RichFunction.class.isAssignableFrom(typedOp.getUserCodeWrapper().getUserCodeClass())) {\n        ctx = createContext(superStep, taskInfo, jobInfo);\n        for (Map.Entry<String, Operator<?>> bcInputs : operator.getBroadcastInputs().entrySet()) {\n            List<?> bcData = execute(bcInputs.getValue(), jobInfo);\n            ctx.setBroadcastVariable(bcInputs.getKey(), bcData);\n        }\n    } else {\n        ctx = null;\n    }\n    // PATH: Test should invoke the next OuterJoinOperatorBase.executeOnCollections(...) [step in execution path]\n    return typedOp.executeOnCollections(inputData1, inputData2, ctx, executionConfig);\n}",
        "@Override\nprotected List<OUT> executeOnCollections(List<IN1> leftInput, List<IN2> rightInput, RuntimeContext runtimeContext, ExecutionConfig executionConfig) throws Exception {\n    TypeInformation<IN1> leftInformation = getOperatorInfo().getFirstInputType();\n    TypeInformation<IN2> rightInformation = getOperatorInfo().getSecondInputType();\n    TypeInformation<OUT> outInformation = getOperatorInfo().getOutputType();\n    TypeComparator<IN1> leftComparator = buildComparatorFor(0, executionConfig, leftInformation);\n    TypeComparator<IN2> rightComparator = buildComparatorFor(1, executionConfig, rightInformation);\n    TypeSerializer<IN1> leftSerializer = leftInformation.createSerializer(executionConfig.getSerializerConfig());\n    TypeSerializer<IN2> rightSerializer = rightInformation.createSerializer(executionConfig.getSerializerConfig());\n    OuterJoinListIterator<IN1, IN2> outerJoinIterator = new OuterJoinListIterator<>(leftInput, leftSerializer, leftComparator, rightInput, rightSerializer, rightComparator, outerJoinType);\n    // --------------------------------------------------------------------\n    // Run UDF\n    // --------------------------------------------------------------------\n    FlatJoinFunction<IN1, IN2, OUT> function = userFunction.getUserCodeObject();\n    FunctionUtils.setFunctionRuntimeContext(function, runtimeContext);\n    FunctionUtils.openFunction(function, DefaultOpenContext.INSTANCE);\n    List<OUT> result = new ArrayList<>();\n    Collector<OUT> collector = new CopyingListCollector<>(result, outInformation.createSerializer(executionConfig.getSerializerConfig()));\n    // PATH: Test should invoke the next OuterJoinOperatorBase$OuterJoinListIterator.next(...) [step in execution path]\n    while (outerJoinIterator.next()) {\n        IN1 left = outerJoinIterator.getLeft();\n        IN2 right = outerJoinIterator.getRight();\n        function.join(left == null ? null : leftSerializer.copy(left), right == null ? null : rightSerializer.copy(right), collector);\n    } \n    FunctionUtils.closeFunction(function);\n    return result;\n}",
        "@SuppressWarnings(\"unchecked\")\nprivate boolean next() throws IOException {\n    boolean hasMoreElements;\n    if (((currLeftIterator == null) || (!currLeftIterator.hasNext())) && ((currRightIterator == null) || (!currRightIterator.hasNext()))) {\n        hasMoreElements = nextGroups(outerJoinType);\n        if (hasMoreElements) {\n            if (outerJoinType != OuterJoinType.LEFT) {\n                currLeftIterator = new ListIteratorWrapper(currLeftSubset.iterator());\n            }\n            leftReturn = ((IN1) (currLeftIterator.next()));\n            if (outerJoinType != OuterJoinType.RIGHT) {\n                currRightIterator = new ListIteratorWrapper(currRightSubset.iterator());\n            }\n            rightReturn = ((IN2) (currRightIterator.next()));\n            return true;\n        } else {\n            // no more elements\n            return false;\n        }\n    } else if (currLeftIterator.hasNext() && (!currRightIterator.hasNext())) // PATH: Test should invoke the next ResettableIterator.reset(...) [step in execution path]\n    {\n        leftReturn = ((IN1) (currLeftIterator.next()));\n        currRightIterator.reset();\n        rightReturn = ((IN2) (currRightIterator.next()));\n        return true;\n    } else {\n        rightReturn = ((IN2) (currRightIterator.next()));\n        return true;\n    }\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic CollectionExecutor(ExecutionConfig executionConfig) {\n    this.executionConfig = executionConfig;\n    this.intermediateResults = new HashMap<Operator<?>, List<?>>();\n    this.accumulators = new HashMap<String, Accumulator<?, ?>>();\n    this.previousAggregates = new HashMap<String, Value>();\n    this.aggregators = new HashMap<String, Aggregator<?>>();\n    this.cachedFiles = new HashMap<String, Future<Path>>();\n    this.userCodeClassLoader = Thread.currentThread().getContextClassLoader();\n}",
        "public DynamicPathCollector(Set<Operator<?>> dynamicPathOperations) {\n    this.dynamicPathOperations = dynamicPathOperations;\n}",
        "public IterationRuntimeUDFContext(JobInfo jobInfo, TaskInfo taskInfo, ClassLoader classloader, ExecutionConfig executionConfig, Map<String, Future<Path>> cpTasks, Map<String, Accumulator<?, ?>> accumulators, OperatorMetricGroup metrics) {\n    super(jobInfo, taskInfo, classloader, executionConfig, cpTasks, accumulators, metrics);\n}",
        "public CompletedFuture(Path entry) {\n    try {\n        LocalFileSystem fs = ((LocalFileSystem) (FileSystem.getUnguardedFileSystem(entry.toUri())));\n        result = (entry.isAbsolute()) ? new Path(entry.toUri().getPath()) : new Path(fs.getWorkingDirectory(), entry);\n    } catch (Exception e) {\n        throw new RuntimeException(\"DistributedCache supports only local files for Collection Environments\");\n    }\n}"
      ],
      "fieldDeclarations": [
        "private final Map<Operator<?>, List<?>> intermediateResults;",
        "private final Map<String, Accumulator<?, ?>> accumulators;",
        "private final Map<String, Future<Path>> cachedFiles;",
        "private final Map<String, Value> previousAggregates;",
        "private final Map<String, Aggregator<?>> aggregators;",
        "private final ClassLoader userCodeClassLoader;",
        "private final ExecutionConfig executionConfig;",
        "private int iterationSuperstep;"
      ],
      "setters": [
        "private void initCache(Set<Map.Entry<String, DistributedCache.DistributedCacheEntry>> files) {\n    for (Map.Entry<String, DistributedCache.DistributedCacheEntry> file : files) {\n        Future<Path> doNothing = new CompletedFuture(new Path(file.getValue().filePath));\n        cachedFiles.put(file.getKey(), doNothing);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.collections.ResettableIterator",
        "org.apache.commons.collections.iterators.ListIteratorWrapper",
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.InvalidProgramException",
        "org.apache.flink.api.common.JobExecutionResult",
        "org.apache.flink.api.common.JobID",
        "org.apache.flink.api.common.JobInfo",
        "org.apache.flink.api.common.JobInfoImpl",
        "org.apache.flink.api.common.Plan",
        "org.apache.flink.api.common.TaskInfo",
        "org.apache.flink.api.common.TaskInfoImpl",
        "org.apache.flink.api.common.accumulators.Accumulator",
        "org.apache.flink.api.common.accumulators.AccumulatorHelper",
        "org.apache.flink.api.common.aggregators.Aggregator",
        "org.apache.flink.api.common.aggregators.AggregatorRegistry",
        "org.apache.flink.api.common.aggregators.AggregatorWithName",
        "org.apache.flink.api.common.aggregators.ConvergenceCriterion",
        "org.apache.flink.api.common.cache.DistributedCache",
        "org.apache.flink.api.common.cache.DistributedCache.DistributedCacheEntry",
        "org.apache.flink.api.common.functions.DefaultOpenContext",
        "org.apache.flink.api.common.functions.FlatJoinFunction",
        "org.apache.flink.api.common.functions.Function",
        "org.apache.flink.api.common.functions.OpenContext",
        "org.apache.flink.api.common.functions.RichFunction",
        "org.apache.flink.api.common.functions.RuntimeContext",
        "org.apache.flink.api.common.functions.util.CopyingListCollector",
        "org.apache.flink.api.common.functions.util.FunctionUtils",
        "org.apache.flink.api.common.functions.util.RuntimeUDFContext",
        "org.apache.flink.api.common.operators.CollectionExecutor.CompletedFuture",
        "org.apache.flink.api.common.operators.CollectionExecutor.DynamicPathCollector",
        "org.apache.flink.api.common.operators.base.BulkIterationBase",
        "org.apache.flink.api.common.operators.base.DeltaIterationBase",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinType",
        "org.apache.flink.api.common.operators.util.TypeComparable",
        "org.apache.flink.api.common.operators.util.UserCodeWrapper",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.fs.FileSystem",
        "org.apache.flink.core.fs.Path",
        "org.apache.flink.core.fs.local.LocalFileSystem",
        "org.apache.flink.metrics.groups.OperatorMetricGroup",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.Collector",
        "org.apache.flink.util.OptionalFailure",
        "org.apache.flink.util.Visitable",
        "org.apache.flink.util.Visitor"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators;\n\npublic class CollectionExecutor_OuterJoinListIteratornext_ResettableIteratorresetFikaTest {\n\n    @Test\n    public void testExecute() {\n    }\n}",
      "conditionCount": 21,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
      "thirdPartyMethod": "org.apache.commons.collections.ResettableIterator.hasNext()",
      "directCaller": "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.next()",
      "path": [
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.executeBinaryOperator(org.apache.flink.api.common.operators.DualInputOperator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.executeOnCollections(java.util.List, java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.next()",
        "org.apache.commons.collections.ResettableIterator.hasNext()"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// General execution methods\n// --------------------------------------------------------------------------------------------\npublic JobExecutionResult execute(Plan program) throws Exception {\n    long startTime = System.currentTimeMillis();\n    JobID jobID = (program.getJobId() == null) ? new JobID() : program.getJobId();\n    JobInfo jobInfo = new JobInfoImpl(jobID, program.getJobName());\n    initCache(program.getCachedFiles());\n    Collection<? extends GenericDataSinkBase<?>> sinks = program.getDataSinks();\n    for (Operator<?> sink : sinks) // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    {\n        execute(sink, jobInfo);\n    }\n    long endTime = System.currentTimeMillis();\n    Map<String, OptionalFailure<Object>> accumulatorResults = AccumulatorHelper.toResultMap(accumulators);\n    return new JobExecutionResult(null, endTime - startTime, accumulatorResults);\n}",
        "private List<?> execute(Operator<?> operator, JobInfo jobInfo) throws Exception {\n    // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    return execute(operator, 0, jobInfo);\n}",
        "private List<?> execute(Operator<?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    List<?> result = this.intermediateResults.get(operator);\n    // if it has already been computed, use the cached variant\n    if (result != null) {\n        return result;\n    }\n    if (operator instanceof BulkIterationBase) {\n        result = executeBulkIteration(((BulkIterationBase<?>) (operator)), jobInfo);\n    } else if (operator instanceof DeltaIterationBase) {\n        result = executeDeltaIteration(((DeltaIterationBase<?, ?>) (operator)), jobInfo);\n    } else if (operator instanceof SingleInputOperator) {\n        result = executeUnaryOperator(((SingleInputOperator<?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof DualInputOperator) {\n        // PATH: Test should invoke the next CollectionExecutor.executeBinaryOperator(...) [step in execution path]\n        result = executeBinaryOperator(((DualInputOperator<?, ?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSourceBase) {\n        result = executeDataSource(((GenericDataSourceBase<?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSinkBase) {\n        executeDataSink(((GenericDataSinkBase<?>) (operator)), superStep, jobInfo);\n        result = Collections.emptyList();\n    } else {\n        throw new RuntimeException(\"Cannot execute operator \" + operator.getClass().getName());\n    }\n    this.intermediateResults.put(operator, result);\n    return result;\n}",
        "private <IN1, IN2, OUT> List<OUT> executeBinaryOperator(DualInputOperator<?, ?, ?, ?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    Operator<?> inputOp1 = operator.getFirstInput();\n    Operator<?> inputOp2 = operator.getSecondInput();\n    if (inputOp1 == null) {\n        throw new InvalidProgramException((\"The binary operation \" + operator.getName()) + \" has no first input.\");\n    }\n    if (inputOp2 == null) {\n        throw new InvalidProgramException((\"The binary operation \" + operator.getName()) + \" has no second input.\");\n    }\n    // compute inputs\n    @SuppressWarnings(\"unchecked\")\n    List<IN1> inputData1 = ((List<IN1>) (execute(inputOp1, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    List<IN2> inputData2 = ((List<IN2>) (execute(inputOp2, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    DualInputOperator<IN1, IN2, OUT, ?> typedOp = ((DualInputOperator<IN1, IN2, OUT, ?>) (operator));\n    // build the runtime context and compute broadcast variables, if necessary\n    TaskInfo taskInfo = new TaskInfoImpl(typedOp.getName(), 1, 0, 1, 0);\n    RuntimeUDFContext ctx;\n    if (RichFunction.class.isAssignableFrom(typedOp.getUserCodeWrapper().getUserCodeClass())) {\n        ctx = createContext(superStep, taskInfo, jobInfo);\n        for (Map.Entry<String, Operator<?>> bcInputs : operator.getBroadcastInputs().entrySet()) {\n            List<?> bcData = execute(bcInputs.getValue(), jobInfo);\n            ctx.setBroadcastVariable(bcInputs.getKey(), bcData);\n        }\n    } else {\n        ctx = null;\n    }\n    // PATH: Test should invoke the next OuterJoinOperatorBase.executeOnCollections(...) [step in execution path]\n    return typedOp.executeOnCollections(inputData1, inputData2, ctx, executionConfig);\n}",
        "@Override\nprotected List<OUT> executeOnCollections(List<IN1> leftInput, List<IN2> rightInput, RuntimeContext runtimeContext, ExecutionConfig executionConfig) throws Exception {\n    TypeInformation<IN1> leftInformation = getOperatorInfo().getFirstInputType();\n    TypeInformation<IN2> rightInformation = getOperatorInfo().getSecondInputType();\n    TypeInformation<OUT> outInformation = getOperatorInfo().getOutputType();\n    TypeComparator<IN1> leftComparator = buildComparatorFor(0, executionConfig, leftInformation);\n    TypeComparator<IN2> rightComparator = buildComparatorFor(1, executionConfig, rightInformation);\n    TypeSerializer<IN1> leftSerializer = leftInformation.createSerializer(executionConfig.getSerializerConfig());\n    TypeSerializer<IN2> rightSerializer = rightInformation.createSerializer(executionConfig.getSerializerConfig());\n    OuterJoinListIterator<IN1, IN2> outerJoinIterator = new OuterJoinListIterator<>(leftInput, leftSerializer, leftComparator, rightInput, rightSerializer, rightComparator, outerJoinType);\n    // --------------------------------------------------------------------\n    // Run UDF\n    // --------------------------------------------------------------------\n    FlatJoinFunction<IN1, IN2, OUT> function = userFunction.getUserCodeObject();\n    FunctionUtils.setFunctionRuntimeContext(function, runtimeContext);\n    FunctionUtils.openFunction(function, DefaultOpenContext.INSTANCE);\n    List<OUT> result = new ArrayList<>();\n    Collector<OUT> collector = new CopyingListCollector<>(result, outInformation.createSerializer(executionConfig.getSerializerConfig()));\n    // PATH: Test should invoke the next OuterJoinOperatorBase$OuterJoinListIterator.next(...) [step in execution path]\n    while (outerJoinIterator.next()) {\n        IN1 left = outerJoinIterator.getLeft();\n        IN2 right = outerJoinIterator.getRight();\n        function.join(left == null ? null : leftSerializer.copy(left), right == null ? null : rightSerializer.copy(right), collector);\n    } \n    FunctionUtils.closeFunction(function);\n    return result;\n}",
        "@SuppressWarnings(\"unchecked\")\nprivate boolean next() throws IOException {\n    boolean hasMoreElements;\n    if (((currLeftIterator == null) || (!currLeftIterator.hasNext())) && ((currRightIterator == null) || (!currRightIterator.hasNext()))) {\n        hasMoreElements = nextGroups(outerJoinType);\n        if (hasMoreElements) {\n            if (outerJoinType != OuterJoinType.LEFT) {\n                currLeftIterator = new ListIteratorWrapper(currLeftSubset.iterator());\n            }\n            leftReturn = ((IN1) (currLeftIterator.next()));\n            if (outerJoinType != OuterJoinType.RIGHT) {\n                currRightIterator = new ListIteratorWrapper(currRightSubset.iterator());\n            }\n            rightReturn = ((IN2) (currRightIterator.next()));\n            return true;\n        } else {\n            // no more elements\n            return false;\n        }\n    } else if (currLeftIterator.hasNext() && (!currRightIterator.hasNext())) {\n        leftReturn = ((IN1) (currLeftIterator.next()));\n        currRightIterator.reset();\n        rightReturn = ((IN2) (currRightIterator.next()));\n        return true;\n    } else {\n        rightReturn = ((IN2) (currRightIterator.next()));\n        return true;\n    }\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic CollectionExecutor(ExecutionConfig executionConfig) {\n    this.executionConfig = executionConfig;\n    this.intermediateResults = new HashMap<Operator<?>, List<?>>();\n    this.accumulators = new HashMap<String, Accumulator<?, ?>>();\n    this.previousAggregates = new HashMap<String, Value>();\n    this.aggregators = new HashMap<String, Aggregator<?>>();\n    this.cachedFiles = new HashMap<String, Future<Path>>();\n    this.userCodeClassLoader = Thread.currentThread().getContextClassLoader();\n}",
        "public DynamicPathCollector(Set<Operator<?>> dynamicPathOperations) {\n    this.dynamicPathOperations = dynamicPathOperations;\n}",
        "public IterationRuntimeUDFContext(JobInfo jobInfo, TaskInfo taskInfo, ClassLoader classloader, ExecutionConfig executionConfig, Map<String, Future<Path>> cpTasks, Map<String, Accumulator<?, ?>> accumulators, OperatorMetricGroup metrics) {\n    super(jobInfo, taskInfo, classloader, executionConfig, cpTasks, accumulators, metrics);\n}",
        "public CompletedFuture(Path entry) {\n    try {\n        LocalFileSystem fs = ((LocalFileSystem) (FileSystem.getUnguardedFileSystem(entry.toUri())));\n        result = (entry.isAbsolute()) ? new Path(entry.toUri().getPath()) : new Path(fs.getWorkingDirectory(), entry);\n    } catch (Exception e) {\n        throw new RuntimeException(\"DistributedCache supports only local files for Collection Environments\");\n    }\n}"
      ],
      "fieldDeclarations": [
        "private final Map<Operator<?>, List<?>> intermediateResults;",
        "private final Map<String, Accumulator<?, ?>> accumulators;",
        "private final Map<String, Future<Path>> cachedFiles;",
        "private final Map<String, Value> previousAggregates;",
        "private final Map<String, Aggregator<?>> aggregators;",
        "private final ClassLoader userCodeClassLoader;",
        "private final ExecutionConfig executionConfig;",
        "private int iterationSuperstep;"
      ],
      "setters": [
        "private void initCache(Set<Map.Entry<String, DistributedCache.DistributedCacheEntry>> files) {\n    for (Map.Entry<String, DistributedCache.DistributedCacheEntry> file : files) {\n        Future<Path> doNothing = new CompletedFuture(new Path(file.getValue().filePath));\n        cachedFiles.put(file.getKey(), doNothing);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.collections.ResettableIterator",
        "org.apache.commons.collections.iterators.ListIteratorWrapper",
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.InvalidProgramException",
        "org.apache.flink.api.common.JobExecutionResult",
        "org.apache.flink.api.common.JobID",
        "org.apache.flink.api.common.JobInfo",
        "org.apache.flink.api.common.JobInfoImpl",
        "org.apache.flink.api.common.Plan",
        "org.apache.flink.api.common.TaskInfo",
        "org.apache.flink.api.common.TaskInfoImpl",
        "org.apache.flink.api.common.accumulators.Accumulator",
        "org.apache.flink.api.common.accumulators.AccumulatorHelper",
        "org.apache.flink.api.common.aggregators.Aggregator",
        "org.apache.flink.api.common.aggregators.AggregatorRegistry",
        "org.apache.flink.api.common.aggregators.AggregatorWithName",
        "org.apache.flink.api.common.aggregators.ConvergenceCriterion",
        "org.apache.flink.api.common.cache.DistributedCache",
        "org.apache.flink.api.common.cache.DistributedCache.DistributedCacheEntry",
        "org.apache.flink.api.common.functions.DefaultOpenContext",
        "org.apache.flink.api.common.functions.FlatJoinFunction",
        "org.apache.flink.api.common.functions.Function",
        "org.apache.flink.api.common.functions.OpenContext",
        "org.apache.flink.api.common.functions.RichFunction",
        "org.apache.flink.api.common.functions.RuntimeContext",
        "org.apache.flink.api.common.functions.util.CopyingListCollector",
        "org.apache.flink.api.common.functions.util.FunctionUtils",
        "org.apache.flink.api.common.functions.util.RuntimeUDFContext",
        "org.apache.flink.api.common.operators.CollectionExecutor.CompletedFuture",
        "org.apache.flink.api.common.operators.CollectionExecutor.DynamicPathCollector",
        "org.apache.flink.api.common.operators.base.BulkIterationBase",
        "org.apache.flink.api.common.operators.base.DeltaIterationBase",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinType",
        "org.apache.flink.api.common.operators.util.TypeComparable",
        "org.apache.flink.api.common.operators.util.UserCodeWrapper",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.fs.FileSystem",
        "org.apache.flink.core.fs.Path",
        "org.apache.flink.core.fs.local.LocalFileSystem",
        "org.apache.flink.metrics.groups.OperatorMetricGroup",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.Collector",
        "org.apache.flink.util.OptionalFailure",
        "org.apache.flink.util.Visitable",
        "org.apache.flink.util.Visitor"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators;\n\npublic class CollectionExecutor_OuterJoinListIteratornext_ResettableIteratorhasNextFikaTest {\n\n    @Test\n    public void testExecute() {\n    }\n}",
      "conditionCount": 21,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
      "thirdPartyMethod": "org.apache.commons.collections.ResettableIterator.next()",
      "directCaller": "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.next()",
      "path": [
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.executeBinaryOperator(org.apache.flink.api.common.operators.DualInputOperator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.executeOnCollections(java.util.List, java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.next()",
        "org.apache.commons.collections.ResettableIterator.next()"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// General execution methods\n// --------------------------------------------------------------------------------------------\npublic JobExecutionResult execute(Plan program) throws Exception {\n    long startTime = System.currentTimeMillis();\n    JobID jobID = (program.getJobId() == null) ? new JobID() : program.getJobId();\n    JobInfo jobInfo = new JobInfoImpl(jobID, program.getJobName());\n    initCache(program.getCachedFiles());\n    Collection<? extends GenericDataSinkBase<?>> sinks = program.getDataSinks();\n    for (Operator<?> sink : sinks) // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    {\n        execute(sink, jobInfo);\n    }\n    long endTime = System.currentTimeMillis();\n    Map<String, OptionalFailure<Object>> accumulatorResults = AccumulatorHelper.toResultMap(accumulators);\n    return new JobExecutionResult(null, endTime - startTime, accumulatorResults);\n}",
        "private List<?> execute(Operator<?> operator, JobInfo jobInfo) throws Exception {\n    // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    return execute(operator, 0, jobInfo);\n}",
        "private List<?> execute(Operator<?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    List<?> result = this.intermediateResults.get(operator);\n    // if it has already been computed, use the cached variant\n    if (result != null) {\n        return result;\n    }\n    if (operator instanceof BulkIterationBase) {\n        result = executeBulkIteration(((BulkIterationBase<?>) (operator)), jobInfo);\n    } else if (operator instanceof DeltaIterationBase) {\n        result = executeDeltaIteration(((DeltaIterationBase<?, ?>) (operator)), jobInfo);\n    } else if (operator instanceof SingleInputOperator) {\n        result = executeUnaryOperator(((SingleInputOperator<?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof DualInputOperator) {\n        // PATH: Test should invoke the next CollectionExecutor.executeBinaryOperator(...) [step in execution path]\n        result = executeBinaryOperator(((DualInputOperator<?, ?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSourceBase) {\n        result = executeDataSource(((GenericDataSourceBase<?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSinkBase) {\n        executeDataSink(((GenericDataSinkBase<?>) (operator)), superStep, jobInfo);\n        result = Collections.emptyList();\n    } else {\n        throw new RuntimeException(\"Cannot execute operator \" + operator.getClass().getName());\n    }\n    this.intermediateResults.put(operator, result);\n    return result;\n}",
        "private <IN1, IN2, OUT> List<OUT> executeBinaryOperator(DualInputOperator<?, ?, ?, ?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    Operator<?> inputOp1 = operator.getFirstInput();\n    Operator<?> inputOp2 = operator.getSecondInput();\n    if (inputOp1 == null) {\n        throw new InvalidProgramException((\"The binary operation \" + operator.getName()) + \" has no first input.\");\n    }\n    if (inputOp2 == null) {\n        throw new InvalidProgramException((\"The binary operation \" + operator.getName()) + \" has no second input.\");\n    }\n    // compute inputs\n    @SuppressWarnings(\"unchecked\")\n    List<IN1> inputData1 = ((List<IN1>) (execute(inputOp1, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    List<IN2> inputData2 = ((List<IN2>) (execute(inputOp2, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    DualInputOperator<IN1, IN2, OUT, ?> typedOp = ((DualInputOperator<IN1, IN2, OUT, ?>) (operator));\n    // build the runtime context and compute broadcast variables, if necessary\n    TaskInfo taskInfo = new TaskInfoImpl(typedOp.getName(), 1, 0, 1, 0);\n    RuntimeUDFContext ctx;\n    if (RichFunction.class.isAssignableFrom(typedOp.getUserCodeWrapper().getUserCodeClass())) {\n        ctx = createContext(superStep, taskInfo, jobInfo);\n        for (Map.Entry<String, Operator<?>> bcInputs : operator.getBroadcastInputs().entrySet()) {\n            List<?> bcData = execute(bcInputs.getValue(), jobInfo);\n            ctx.setBroadcastVariable(bcInputs.getKey(), bcData);\n        }\n    } else {\n        ctx = null;\n    }\n    // PATH: Test should invoke the next OuterJoinOperatorBase.executeOnCollections(...) [step in execution path]\n    return typedOp.executeOnCollections(inputData1, inputData2, ctx, executionConfig);\n}",
        "@Override\nprotected List<OUT> executeOnCollections(List<IN1> leftInput, List<IN2> rightInput, RuntimeContext runtimeContext, ExecutionConfig executionConfig) throws Exception {\n    TypeInformation<IN1> leftInformation = getOperatorInfo().getFirstInputType();\n    TypeInformation<IN2> rightInformation = getOperatorInfo().getSecondInputType();\n    TypeInformation<OUT> outInformation = getOperatorInfo().getOutputType();\n    TypeComparator<IN1> leftComparator = buildComparatorFor(0, executionConfig, leftInformation);\n    TypeComparator<IN2> rightComparator = buildComparatorFor(1, executionConfig, rightInformation);\n    TypeSerializer<IN1> leftSerializer = leftInformation.createSerializer(executionConfig.getSerializerConfig());\n    TypeSerializer<IN2> rightSerializer = rightInformation.createSerializer(executionConfig.getSerializerConfig());\n    OuterJoinListIterator<IN1, IN2> outerJoinIterator = new OuterJoinListIterator<>(leftInput, leftSerializer, leftComparator, rightInput, rightSerializer, rightComparator, outerJoinType);\n    // --------------------------------------------------------------------\n    // Run UDF\n    // --------------------------------------------------------------------\n    FlatJoinFunction<IN1, IN2, OUT> function = userFunction.getUserCodeObject();\n    FunctionUtils.setFunctionRuntimeContext(function, runtimeContext);\n    FunctionUtils.openFunction(function, DefaultOpenContext.INSTANCE);\n    List<OUT> result = new ArrayList<>();\n    Collector<OUT> collector = new CopyingListCollector<>(result, outInformation.createSerializer(executionConfig.getSerializerConfig()));\n    // PATH: Test should invoke the next OuterJoinOperatorBase$OuterJoinListIterator.next(...) [step in execution path]\n    while (outerJoinIterator.next()) {\n        IN1 left = outerJoinIterator.getLeft();\n        IN2 right = outerJoinIterator.getRight();\n        function.join(left == null ? null : leftSerializer.copy(left), right == null ? null : rightSerializer.copy(right), collector);\n    } \n    FunctionUtils.closeFunction(function);\n    return result;\n}",
        "@SuppressWarnings(\"unchecked\")\nprivate boolean next() throws IOException {\n    boolean hasMoreElements;\n    if (((currLeftIterator == null) || (!currLeftIterator.hasNext())) && ((currRightIterator == null) || (!currRightIterator.hasNext()))) {\n        hasMoreElements = nextGroups(outerJoinType);\n        if (hasMoreElements) {\n            if (outerJoinType != OuterJoinType.LEFT) {\n                currLeftIterator = new ListIteratorWrapper(currLeftSubset.iterator());\n            }\n            leftReturn = ((IN1) (currLeftIterator.next()));\n            if (outerJoinType != OuterJoinType.RIGHT) {\n                currRightIterator = new ListIteratorWrapper(currRightSubset.iterator());\n            }\n            rightReturn = ((IN2) (currRightIterator.next()));\n            return true;\n        } else {\n            // no more elements\n            return false;\n        }\n    } else if (currLeftIterator.hasNext() && (!currRightIterator.hasNext())) {\n        leftReturn = ((IN1) (currLeftIterator.next()));\n        currRightIterator.reset();\n        rightReturn = ((IN2) (currRightIterator.next()));\n        return true;\n    } else {\n        rightReturn = ((IN2) (currRightIterator.next()));\n        return true;\n    }\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic CollectionExecutor(ExecutionConfig executionConfig) {\n    this.executionConfig = executionConfig;\n    this.intermediateResults = new HashMap<Operator<?>, List<?>>();\n    this.accumulators = new HashMap<String, Accumulator<?, ?>>();\n    this.previousAggregates = new HashMap<String, Value>();\n    this.aggregators = new HashMap<String, Aggregator<?>>();\n    this.cachedFiles = new HashMap<String, Future<Path>>();\n    this.userCodeClassLoader = Thread.currentThread().getContextClassLoader();\n}",
        "public DynamicPathCollector(Set<Operator<?>> dynamicPathOperations) {\n    this.dynamicPathOperations = dynamicPathOperations;\n}",
        "public IterationRuntimeUDFContext(JobInfo jobInfo, TaskInfo taskInfo, ClassLoader classloader, ExecutionConfig executionConfig, Map<String, Future<Path>> cpTasks, Map<String, Accumulator<?, ?>> accumulators, OperatorMetricGroup metrics) {\n    super(jobInfo, taskInfo, classloader, executionConfig, cpTasks, accumulators, metrics);\n}",
        "public CompletedFuture(Path entry) {\n    try {\n        LocalFileSystem fs = ((LocalFileSystem) (FileSystem.getUnguardedFileSystem(entry.toUri())));\n        result = (entry.isAbsolute()) ? new Path(entry.toUri().getPath()) : new Path(fs.getWorkingDirectory(), entry);\n    } catch (Exception e) {\n        throw new RuntimeException(\"DistributedCache supports only local files for Collection Environments\");\n    }\n}"
      ],
      "fieldDeclarations": [
        "private final Map<Operator<?>, List<?>> intermediateResults;",
        "private final Map<String, Accumulator<?, ?>> accumulators;",
        "private final Map<String, Future<Path>> cachedFiles;",
        "private final Map<String, Value> previousAggregates;",
        "private final Map<String, Aggregator<?>> aggregators;",
        "private final ClassLoader userCodeClassLoader;",
        "private final ExecutionConfig executionConfig;",
        "private int iterationSuperstep;"
      ],
      "setters": [
        "private void initCache(Set<Map.Entry<String, DistributedCache.DistributedCacheEntry>> files) {\n    for (Map.Entry<String, DistributedCache.DistributedCacheEntry> file : files) {\n        Future<Path> doNothing = new CompletedFuture(new Path(file.getValue().filePath));\n        cachedFiles.put(file.getKey(), doNothing);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.collections.ResettableIterator",
        "org.apache.commons.collections.iterators.ListIteratorWrapper",
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.InvalidProgramException",
        "org.apache.flink.api.common.JobExecutionResult",
        "org.apache.flink.api.common.JobID",
        "org.apache.flink.api.common.JobInfo",
        "org.apache.flink.api.common.JobInfoImpl",
        "org.apache.flink.api.common.Plan",
        "org.apache.flink.api.common.TaskInfo",
        "org.apache.flink.api.common.TaskInfoImpl",
        "org.apache.flink.api.common.accumulators.Accumulator",
        "org.apache.flink.api.common.accumulators.AccumulatorHelper",
        "org.apache.flink.api.common.aggregators.Aggregator",
        "org.apache.flink.api.common.aggregators.AggregatorRegistry",
        "org.apache.flink.api.common.aggregators.AggregatorWithName",
        "org.apache.flink.api.common.aggregators.ConvergenceCriterion",
        "org.apache.flink.api.common.cache.DistributedCache",
        "org.apache.flink.api.common.cache.DistributedCache.DistributedCacheEntry",
        "org.apache.flink.api.common.functions.DefaultOpenContext",
        "org.apache.flink.api.common.functions.FlatJoinFunction",
        "org.apache.flink.api.common.functions.Function",
        "org.apache.flink.api.common.functions.OpenContext",
        "org.apache.flink.api.common.functions.RichFunction",
        "org.apache.flink.api.common.functions.RuntimeContext",
        "org.apache.flink.api.common.functions.util.CopyingListCollector",
        "org.apache.flink.api.common.functions.util.FunctionUtils",
        "org.apache.flink.api.common.functions.util.RuntimeUDFContext",
        "org.apache.flink.api.common.operators.CollectionExecutor.CompletedFuture",
        "org.apache.flink.api.common.operators.CollectionExecutor.DynamicPathCollector",
        "org.apache.flink.api.common.operators.base.BulkIterationBase",
        "org.apache.flink.api.common.operators.base.DeltaIterationBase",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinType",
        "org.apache.flink.api.common.operators.util.TypeComparable",
        "org.apache.flink.api.common.operators.util.UserCodeWrapper",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.fs.FileSystem",
        "org.apache.flink.core.fs.Path",
        "org.apache.flink.core.fs.local.LocalFileSystem",
        "org.apache.flink.metrics.groups.OperatorMetricGroup",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.Collector",
        "org.apache.flink.util.OptionalFailure",
        "org.apache.flink.util.Visitable",
        "org.apache.flink.util.Visitor"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators;\n\npublic class CollectionExecutor_OuterJoinListIteratornext_ResettableIteratornextFikaTest {\n\n    @Test\n    public void testExecute() {\n    }\n}",
      "conditionCount": 21,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
      "thirdPartyMethod": "org.apache.commons.collections.iterators.ListIteratorWrapper.<init>(java.util.Iterator)",
      "directCaller": "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.nextGroups(org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinType)",
      "path": [
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.executeBinaryOperator(org.apache.flink.api.common.operators.DualInputOperator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.executeOnCollections(java.util.List, java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.next()",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.nextGroups(org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinType)",
        "org.apache.commons.collections.iterators.ListIteratorWrapper.<init>(java.util.Iterator)"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// General execution methods\n// --------------------------------------------------------------------------------------------\npublic JobExecutionResult execute(Plan program) throws Exception {\n    long startTime = System.currentTimeMillis();\n    JobID jobID = (program.getJobId() == null) ? new JobID() : program.getJobId();\n    JobInfo jobInfo = new JobInfoImpl(jobID, program.getJobName());\n    initCache(program.getCachedFiles());\n    Collection<? extends GenericDataSinkBase<?>> sinks = program.getDataSinks();\n    for (Operator<?> sink : sinks) // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    {\n        execute(sink, jobInfo);\n    }\n    long endTime = System.currentTimeMillis();\n    Map<String, OptionalFailure<Object>> accumulatorResults = AccumulatorHelper.toResultMap(accumulators);\n    return new JobExecutionResult(null, endTime - startTime, accumulatorResults);\n}",
        "private List<?> execute(Operator<?> operator, JobInfo jobInfo) throws Exception {\n    // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    return execute(operator, 0, jobInfo);\n}",
        "private List<?> execute(Operator<?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    List<?> result = this.intermediateResults.get(operator);\n    // if it has already been computed, use the cached variant\n    if (result != null) {\n        return result;\n    }\n    if (operator instanceof BulkIterationBase) {\n        result = executeBulkIteration(((BulkIterationBase<?>) (operator)), jobInfo);\n    } else if (operator instanceof DeltaIterationBase) {\n        result = executeDeltaIteration(((DeltaIterationBase<?, ?>) (operator)), jobInfo);\n    } else if (operator instanceof SingleInputOperator) {\n        result = executeUnaryOperator(((SingleInputOperator<?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof DualInputOperator) {\n        // PATH: Test should invoke the next CollectionExecutor.executeBinaryOperator(...) [step in execution path]\n        result = executeBinaryOperator(((DualInputOperator<?, ?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSourceBase) {\n        result = executeDataSource(((GenericDataSourceBase<?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSinkBase) {\n        executeDataSink(((GenericDataSinkBase<?>) (operator)), superStep, jobInfo);\n        result = Collections.emptyList();\n    } else {\n        throw new RuntimeException(\"Cannot execute operator \" + operator.getClass().getName());\n    }\n    this.intermediateResults.put(operator, result);\n    return result;\n}",
        "private <IN1, IN2, OUT> List<OUT> executeBinaryOperator(DualInputOperator<?, ?, ?, ?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    Operator<?> inputOp1 = operator.getFirstInput();\n    Operator<?> inputOp2 = operator.getSecondInput();\n    if (inputOp1 == null) {\n        throw new InvalidProgramException((\"The binary operation \" + operator.getName()) + \" has no first input.\");\n    }\n    if (inputOp2 == null) {\n        throw new InvalidProgramException((\"The binary operation \" + operator.getName()) + \" has no second input.\");\n    }\n    // compute inputs\n    @SuppressWarnings(\"unchecked\")\n    List<IN1> inputData1 = ((List<IN1>) (execute(inputOp1, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    List<IN2> inputData2 = ((List<IN2>) (execute(inputOp2, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    DualInputOperator<IN1, IN2, OUT, ?> typedOp = ((DualInputOperator<IN1, IN2, OUT, ?>) (operator));\n    // build the runtime context and compute broadcast variables, if necessary\n    TaskInfo taskInfo = new TaskInfoImpl(typedOp.getName(), 1, 0, 1, 0);\n    RuntimeUDFContext ctx;\n    if (RichFunction.class.isAssignableFrom(typedOp.getUserCodeWrapper().getUserCodeClass())) {\n        ctx = createContext(superStep, taskInfo, jobInfo);\n        for (Map.Entry<String, Operator<?>> bcInputs : operator.getBroadcastInputs().entrySet()) {\n            List<?> bcData = execute(bcInputs.getValue(), jobInfo);\n            ctx.setBroadcastVariable(bcInputs.getKey(), bcData);\n        }\n    } else {\n        ctx = null;\n    }\n    // PATH: Test should invoke the next OuterJoinOperatorBase.executeOnCollections(...) [step in execution path]\n    return typedOp.executeOnCollections(inputData1, inputData2, ctx, executionConfig);\n}",
        "@Override\nprotected List<OUT> executeOnCollections(List<IN1> leftInput, List<IN2> rightInput, RuntimeContext runtimeContext, ExecutionConfig executionConfig) throws Exception {\n    TypeInformation<IN1> leftInformation = getOperatorInfo().getFirstInputType();\n    TypeInformation<IN2> rightInformation = getOperatorInfo().getSecondInputType();\n    TypeInformation<OUT> outInformation = getOperatorInfo().getOutputType();\n    TypeComparator<IN1> leftComparator = buildComparatorFor(0, executionConfig, leftInformation);\n    TypeComparator<IN2> rightComparator = buildComparatorFor(1, executionConfig, rightInformation);\n    TypeSerializer<IN1> leftSerializer = leftInformation.createSerializer(executionConfig.getSerializerConfig());\n    TypeSerializer<IN2> rightSerializer = rightInformation.createSerializer(executionConfig.getSerializerConfig());\n    OuterJoinListIterator<IN1, IN2> outerJoinIterator = new OuterJoinListIterator<>(leftInput, leftSerializer, leftComparator, rightInput, rightSerializer, rightComparator, outerJoinType);\n    // --------------------------------------------------------------------\n    // Run UDF\n    // --------------------------------------------------------------------\n    FlatJoinFunction<IN1, IN2, OUT> function = userFunction.getUserCodeObject();\n    FunctionUtils.setFunctionRuntimeContext(function, runtimeContext);\n    FunctionUtils.openFunction(function, DefaultOpenContext.INSTANCE);\n    List<OUT> result = new ArrayList<>();\n    Collector<OUT> collector = new CopyingListCollector<>(result, outInformation.createSerializer(executionConfig.getSerializerConfig()));\n    // PATH: Test should invoke the next OuterJoinOperatorBase$OuterJoinListIterator.next(...) [step in execution path]\n    while (outerJoinIterator.next()) {\n        IN1 left = outerJoinIterator.getLeft();\n        IN2 right = outerJoinIterator.getRight();\n        function.join(left == null ? null : leftSerializer.copy(left), right == null ? null : rightSerializer.copy(right), collector);\n    } \n    FunctionUtils.closeFunction(function);\n    return result;\n}",
        "@SuppressWarnings(\"unchecked\")\nprivate boolean next() throws IOException {\n    boolean hasMoreElements;\n    if (((currLeftIterator == null) || (!currLeftIterator.hasNext())) && ((currRightIterator == null) || (!currRightIterator.hasNext()))) {\n        // PATH: Test should invoke the next OuterJoinOperatorBase$OuterJoinListIterator.nextGroups(...) [step in execution path]\n        hasMoreElements = nextGroups(outerJoinType);\n        if (hasMoreElements) {\n            if (outerJoinType != OuterJoinType.LEFT) {\n                currLeftIterator = new ListIteratorWrapper(currLeftSubset.iterator());\n            }\n            leftReturn = ((IN1) (currLeftIterator.next()));\n            if (outerJoinType != OuterJoinType.RIGHT) {\n                currRightIterator = new ListIteratorWrapper(currRightSubset.iterator());\n            }\n            rightReturn = ((IN2) (currRightIterator.next()));\n            return true;\n        } else {\n            // no more elements\n            return false;\n        }\n    } else if (currLeftIterator.hasNext() && (!currRightIterator.hasNext())) {\n        leftReturn = ((IN1) (currLeftIterator.next()));\n        currRightIterator.reset();\n        rightReturn = ((IN2) (currRightIterator.next()));\n        return true;\n    } else {\n        rightReturn = ((IN2) (currRightIterator.next()));\n        return true;\n    }\n}",
        "private boolean nextGroups(OuterJoinType outerJoinType) throws IOException {\n    if (outerJoinType == OuterJoinType.FULL) {\n        return nextGroups();\n    } else if (outerJoinType == OuterJoinType.LEFT) {\n        boolean leftContainsElements = false;\n        while ((!leftContainsElements) && nextGroups()) {\n            currLeftIterator = new ListIteratorWrapper(currLeftSubset.iterator());\n            if (currLeftIterator.next() != null) {\n                leftContainsElements = true;\n            }\n            currLeftIterator.reset();\n        } \n        return leftContainsElements;\n    } else if (outerJoinType == OuterJoinType.RIGHT) {\n        boolean rightContainsElements = false;\n        while ((!rightContainsElements) && nextGroups()) {\n            currRightIterator = new ListIteratorWrapper(currRightSubset.iterator());\n            if (currRightIterator.next() != null) {\n                rightContainsElements = true;\n            }\n            currRightIterator.reset();\n        } \n        return rightContainsElements;\n    } else {\n        throw new IllegalArgumentException((\"Outer join of type '\" + outerJoinType) + \"' not supported.\");\n    }\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic CollectionExecutor(ExecutionConfig executionConfig) {\n    this.executionConfig = executionConfig;\n    this.intermediateResults = new HashMap<Operator<?>, List<?>>();\n    this.accumulators = new HashMap<String, Accumulator<?, ?>>();\n    this.previousAggregates = new HashMap<String, Value>();\n    this.aggregators = new HashMap<String, Aggregator<?>>();\n    this.cachedFiles = new HashMap<String, Future<Path>>();\n    this.userCodeClassLoader = Thread.currentThread().getContextClassLoader();\n}",
        "public DynamicPathCollector(Set<Operator<?>> dynamicPathOperations) {\n    this.dynamicPathOperations = dynamicPathOperations;\n}",
        "public IterationRuntimeUDFContext(JobInfo jobInfo, TaskInfo taskInfo, ClassLoader classloader, ExecutionConfig executionConfig, Map<String, Future<Path>> cpTasks, Map<String, Accumulator<?, ?>> accumulators, OperatorMetricGroup metrics) {\n    super(jobInfo, taskInfo, classloader, executionConfig, cpTasks, accumulators, metrics);\n}",
        "public CompletedFuture(Path entry) {\n    try {\n        LocalFileSystem fs = ((LocalFileSystem) (FileSystem.getUnguardedFileSystem(entry.toUri())));\n        result = (entry.isAbsolute()) ? new Path(entry.toUri().getPath()) : new Path(fs.getWorkingDirectory(), entry);\n    } catch (Exception e) {\n        throw new RuntimeException(\"DistributedCache supports only local files for Collection Environments\");\n    }\n}"
      ],
      "fieldDeclarations": [
        "private final Map<Operator<?>, List<?>> intermediateResults;",
        "private final Map<String, Accumulator<?, ?>> accumulators;",
        "private final Map<String, Future<Path>> cachedFiles;",
        "private final Map<String, Value> previousAggregates;",
        "private final Map<String, Aggregator<?>> aggregators;",
        "private final ClassLoader userCodeClassLoader;",
        "private final ExecutionConfig executionConfig;",
        "private int iterationSuperstep;"
      ],
      "setters": [
        "private void initCache(Set<Map.Entry<String, DistributedCache.DistributedCacheEntry>> files) {\n    for (Map.Entry<String, DistributedCache.DistributedCacheEntry> file : files) {\n        Future<Path> doNothing = new CompletedFuture(new Path(file.getValue().filePath));\n        cachedFiles.put(file.getKey(), doNothing);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.collections.ResettableIterator",
        "org.apache.commons.collections.iterators.ListIteratorWrapper",
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.InvalidProgramException",
        "org.apache.flink.api.common.JobExecutionResult",
        "org.apache.flink.api.common.JobID",
        "org.apache.flink.api.common.JobInfo",
        "org.apache.flink.api.common.JobInfoImpl",
        "org.apache.flink.api.common.Plan",
        "org.apache.flink.api.common.TaskInfo",
        "org.apache.flink.api.common.TaskInfoImpl",
        "org.apache.flink.api.common.accumulators.Accumulator",
        "org.apache.flink.api.common.accumulators.AccumulatorHelper",
        "org.apache.flink.api.common.aggregators.Aggregator",
        "org.apache.flink.api.common.aggregators.AggregatorRegistry",
        "org.apache.flink.api.common.aggregators.AggregatorWithName",
        "org.apache.flink.api.common.aggregators.ConvergenceCriterion",
        "org.apache.flink.api.common.cache.DistributedCache",
        "org.apache.flink.api.common.cache.DistributedCache.DistributedCacheEntry",
        "org.apache.flink.api.common.functions.DefaultOpenContext",
        "org.apache.flink.api.common.functions.FlatJoinFunction",
        "org.apache.flink.api.common.functions.Function",
        "org.apache.flink.api.common.functions.OpenContext",
        "org.apache.flink.api.common.functions.RichFunction",
        "org.apache.flink.api.common.functions.RuntimeContext",
        "org.apache.flink.api.common.functions.util.CopyingListCollector",
        "org.apache.flink.api.common.functions.util.FunctionUtils",
        "org.apache.flink.api.common.functions.util.RuntimeUDFContext",
        "org.apache.flink.api.common.operators.CollectionExecutor.CompletedFuture",
        "org.apache.flink.api.common.operators.CollectionExecutor.DynamicPathCollector",
        "org.apache.flink.api.common.operators.base.BulkIterationBase",
        "org.apache.flink.api.common.operators.base.DeltaIterationBase",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.MatchStatus",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinType",
        "org.apache.flink.api.common.operators.util.ListKeyGroupedIterator",
        "org.apache.flink.api.common.operators.util.ListKeyGroupedIterator.ValuesIterator",
        "org.apache.flink.api.common.operators.util.TypeComparable",
        "org.apache.flink.api.common.operators.util.UserCodeWrapper",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.GenericPairComparator",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.fs.FileSystem",
        "org.apache.flink.core.fs.Path",
        "org.apache.flink.core.fs.local.LocalFileSystem",
        "org.apache.flink.metrics.groups.OperatorMetricGroup",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.Collector",
        "org.apache.flink.util.OptionalFailure",
        "org.apache.flink.util.Visitable",
        "org.apache.flink.util.Visitor"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators;\n\npublic class CollectionExecutor_OuterJoinListIteratornextGroups_ListIteratorWrappermethodFikaTest {\n\n    @Test\n    public void testExecute() {\n    }\n}",
      "conditionCount": 28,
      "callCount": 1,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
      "thirdPartyMethod": "org.apache.commons.collections.ResettableIterator.reset()",
      "directCaller": "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.nextGroups(org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinType)",
      "path": [
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.executeBinaryOperator(org.apache.flink.api.common.operators.DualInputOperator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.executeOnCollections(java.util.List, java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.next()",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.nextGroups(org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinType)",
        "org.apache.commons.collections.ResettableIterator.reset()"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// General execution methods\n// --------------------------------------------------------------------------------------------\npublic JobExecutionResult execute(Plan program) throws Exception {\n    long startTime = System.currentTimeMillis();\n    JobID jobID = (program.getJobId() == null) ? new JobID() : program.getJobId();\n    JobInfo jobInfo = new JobInfoImpl(jobID, program.getJobName());\n    initCache(program.getCachedFiles());\n    Collection<? extends GenericDataSinkBase<?>> sinks = program.getDataSinks();\n    for (Operator<?> sink : sinks) // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    {\n        execute(sink, jobInfo);\n    }\n    long endTime = System.currentTimeMillis();\n    Map<String, OptionalFailure<Object>> accumulatorResults = AccumulatorHelper.toResultMap(accumulators);\n    return new JobExecutionResult(null, endTime - startTime, accumulatorResults);\n}",
        "private List<?> execute(Operator<?> operator, JobInfo jobInfo) throws Exception {\n    // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    return execute(operator, 0, jobInfo);\n}",
        "private List<?> execute(Operator<?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    List<?> result = this.intermediateResults.get(operator);\n    // if it has already been computed, use the cached variant\n    if (result != null) {\n        return result;\n    }\n    if (operator instanceof BulkIterationBase) {\n        result = executeBulkIteration(((BulkIterationBase<?>) (operator)), jobInfo);\n    } else if (operator instanceof DeltaIterationBase) {\n        result = executeDeltaIteration(((DeltaIterationBase<?, ?>) (operator)), jobInfo);\n    } else if (operator instanceof SingleInputOperator) {\n        result = executeUnaryOperator(((SingleInputOperator<?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof DualInputOperator) {\n        // PATH: Test should invoke the next CollectionExecutor.executeBinaryOperator(...) [step in execution path]\n        result = executeBinaryOperator(((DualInputOperator<?, ?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSourceBase) {\n        result = executeDataSource(((GenericDataSourceBase<?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSinkBase) {\n        executeDataSink(((GenericDataSinkBase<?>) (operator)), superStep, jobInfo);\n        result = Collections.emptyList();\n    } else {\n        throw new RuntimeException(\"Cannot execute operator \" + operator.getClass().getName());\n    }\n    this.intermediateResults.put(operator, result);\n    return result;\n}",
        "private <IN1, IN2, OUT> List<OUT> executeBinaryOperator(DualInputOperator<?, ?, ?, ?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    Operator<?> inputOp1 = operator.getFirstInput();\n    Operator<?> inputOp2 = operator.getSecondInput();\n    if (inputOp1 == null) {\n        throw new InvalidProgramException((\"The binary operation \" + operator.getName()) + \" has no first input.\");\n    }\n    if (inputOp2 == null) {\n        throw new InvalidProgramException((\"The binary operation \" + operator.getName()) + \" has no second input.\");\n    }\n    // compute inputs\n    @SuppressWarnings(\"unchecked\")\n    List<IN1> inputData1 = ((List<IN1>) (execute(inputOp1, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    List<IN2> inputData2 = ((List<IN2>) (execute(inputOp2, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    DualInputOperator<IN1, IN2, OUT, ?> typedOp = ((DualInputOperator<IN1, IN2, OUT, ?>) (operator));\n    // build the runtime context and compute broadcast variables, if necessary\n    TaskInfo taskInfo = new TaskInfoImpl(typedOp.getName(), 1, 0, 1, 0);\n    RuntimeUDFContext ctx;\n    if (RichFunction.class.isAssignableFrom(typedOp.getUserCodeWrapper().getUserCodeClass())) {\n        ctx = createContext(superStep, taskInfo, jobInfo);\n        for (Map.Entry<String, Operator<?>> bcInputs : operator.getBroadcastInputs().entrySet()) {\n            List<?> bcData = execute(bcInputs.getValue(), jobInfo);\n            ctx.setBroadcastVariable(bcInputs.getKey(), bcData);\n        }\n    } else {\n        ctx = null;\n    }\n    // PATH: Test should invoke the next OuterJoinOperatorBase.executeOnCollections(...) [step in execution path]\n    return typedOp.executeOnCollections(inputData1, inputData2, ctx, executionConfig);\n}",
        "@Override\nprotected List<OUT> executeOnCollections(List<IN1> leftInput, List<IN2> rightInput, RuntimeContext runtimeContext, ExecutionConfig executionConfig) throws Exception {\n    TypeInformation<IN1> leftInformation = getOperatorInfo().getFirstInputType();\n    TypeInformation<IN2> rightInformation = getOperatorInfo().getSecondInputType();\n    TypeInformation<OUT> outInformation = getOperatorInfo().getOutputType();\n    TypeComparator<IN1> leftComparator = buildComparatorFor(0, executionConfig, leftInformation);\n    TypeComparator<IN2> rightComparator = buildComparatorFor(1, executionConfig, rightInformation);\n    TypeSerializer<IN1> leftSerializer = leftInformation.createSerializer(executionConfig.getSerializerConfig());\n    TypeSerializer<IN2> rightSerializer = rightInformation.createSerializer(executionConfig.getSerializerConfig());\n    OuterJoinListIterator<IN1, IN2> outerJoinIterator = new OuterJoinListIterator<>(leftInput, leftSerializer, leftComparator, rightInput, rightSerializer, rightComparator, outerJoinType);\n    // --------------------------------------------------------------------\n    // Run UDF\n    // --------------------------------------------------------------------\n    FlatJoinFunction<IN1, IN2, OUT> function = userFunction.getUserCodeObject();\n    FunctionUtils.setFunctionRuntimeContext(function, runtimeContext);\n    FunctionUtils.openFunction(function, DefaultOpenContext.INSTANCE);\n    List<OUT> result = new ArrayList<>();\n    Collector<OUT> collector = new CopyingListCollector<>(result, outInformation.createSerializer(executionConfig.getSerializerConfig()));\n    // PATH: Test should invoke the next OuterJoinOperatorBase$OuterJoinListIterator.next(...) [step in execution path]\n    while (outerJoinIterator.next()) {\n        IN1 left = outerJoinIterator.getLeft();\n        IN2 right = outerJoinIterator.getRight();\n        function.join(left == null ? null : leftSerializer.copy(left), right == null ? null : rightSerializer.copy(right), collector);\n    } \n    FunctionUtils.closeFunction(function);\n    return result;\n}",
        "@SuppressWarnings(\"unchecked\")\nprivate boolean next() throws IOException {\n    boolean hasMoreElements;\n    if (((currLeftIterator == null) || (!currLeftIterator.hasNext())) && ((currRightIterator == null) || (!currRightIterator.hasNext()))) {\n        // PATH: Test should invoke the next OuterJoinOperatorBase$OuterJoinListIterator.nextGroups(...) [step in execution path]\n        hasMoreElements = nextGroups(outerJoinType);\n        if (hasMoreElements) {\n            if (outerJoinType != OuterJoinType.LEFT) {\n                currLeftIterator = new ListIteratorWrapper(currLeftSubset.iterator());\n            }\n            leftReturn = ((IN1) (currLeftIterator.next()));\n            if (outerJoinType != OuterJoinType.RIGHT) {\n                currRightIterator = new ListIteratorWrapper(currRightSubset.iterator());\n            }\n            rightReturn = ((IN2) (currRightIterator.next()));\n            return true;\n        } else {\n            // no more elements\n            return false;\n        }\n    } else if (currLeftIterator.hasNext() && (!currRightIterator.hasNext())) {\n        leftReturn = ((IN1) (currLeftIterator.next()));\n        currRightIterator.reset();\n        rightReturn = ((IN2) (currRightIterator.next()));\n        return true;\n    } else {\n        rightReturn = ((IN2) (currRightIterator.next()));\n        return true;\n    }\n}",
        "private boolean nextGroups(OuterJoinType outerJoinType) throws IOException {\n    if (outerJoinType == OuterJoinType.FULL) {\n        return nextGroups();\n    } else if (outerJoinType == OuterJoinType.LEFT) {\n        boolean leftContainsElements = false;\n        while ((!leftContainsElements) && nextGroups()) // PATH: Test should invoke the next ResettableIterator.reset(...) [step in execution path]\n        {\n            currLeftIterator = new ListIteratorWrapper(currLeftSubset.iterator());\n            if (currLeftIterator.next() != null) {\n                leftContainsElements = true;\n            }\n            currLeftIterator.reset();\n        } \n        return leftContainsElements;\n    } else if (outerJoinType == OuterJoinType.RIGHT) {\n        boolean rightContainsElements = false;\n        while ((!rightContainsElements) && nextGroups()) {\n            currRightIterator = new ListIteratorWrapper(currRightSubset.iterator());\n            if (currRightIterator.next() != null) {\n                rightContainsElements = true;\n            }\n            currRightIterator.reset();\n        } \n        return rightContainsElements;\n    } else {\n        throw new IllegalArgumentException((\"Outer join of type '\" + outerJoinType) + \"' not supported.\");\n    }\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic CollectionExecutor(ExecutionConfig executionConfig) {\n    this.executionConfig = executionConfig;\n    this.intermediateResults = new HashMap<Operator<?>, List<?>>();\n    this.accumulators = new HashMap<String, Accumulator<?, ?>>();\n    this.previousAggregates = new HashMap<String, Value>();\n    this.aggregators = new HashMap<String, Aggregator<?>>();\n    this.cachedFiles = new HashMap<String, Future<Path>>();\n    this.userCodeClassLoader = Thread.currentThread().getContextClassLoader();\n}",
        "public DynamicPathCollector(Set<Operator<?>> dynamicPathOperations) {\n    this.dynamicPathOperations = dynamicPathOperations;\n}",
        "public IterationRuntimeUDFContext(JobInfo jobInfo, TaskInfo taskInfo, ClassLoader classloader, ExecutionConfig executionConfig, Map<String, Future<Path>> cpTasks, Map<String, Accumulator<?, ?>> accumulators, OperatorMetricGroup metrics) {\n    super(jobInfo, taskInfo, classloader, executionConfig, cpTasks, accumulators, metrics);\n}",
        "public CompletedFuture(Path entry) {\n    try {\n        LocalFileSystem fs = ((LocalFileSystem) (FileSystem.getUnguardedFileSystem(entry.toUri())));\n        result = (entry.isAbsolute()) ? new Path(entry.toUri().getPath()) : new Path(fs.getWorkingDirectory(), entry);\n    } catch (Exception e) {\n        throw new RuntimeException(\"DistributedCache supports only local files for Collection Environments\");\n    }\n}"
      ],
      "fieldDeclarations": [
        "private final Map<Operator<?>, List<?>> intermediateResults;",
        "private final Map<String, Accumulator<?, ?>> accumulators;",
        "private final Map<String, Future<Path>> cachedFiles;",
        "private final Map<String, Value> previousAggregates;",
        "private final Map<String, Aggregator<?>> aggregators;",
        "private final ClassLoader userCodeClassLoader;",
        "private final ExecutionConfig executionConfig;",
        "private int iterationSuperstep;"
      ],
      "setters": [
        "private void initCache(Set<Map.Entry<String, DistributedCache.DistributedCacheEntry>> files) {\n    for (Map.Entry<String, DistributedCache.DistributedCacheEntry> file : files) {\n        Future<Path> doNothing = new CompletedFuture(new Path(file.getValue().filePath));\n        cachedFiles.put(file.getKey(), doNothing);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.collections.ResettableIterator",
        "org.apache.commons.collections.iterators.ListIteratorWrapper",
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.InvalidProgramException",
        "org.apache.flink.api.common.JobExecutionResult",
        "org.apache.flink.api.common.JobID",
        "org.apache.flink.api.common.JobInfo",
        "org.apache.flink.api.common.JobInfoImpl",
        "org.apache.flink.api.common.Plan",
        "org.apache.flink.api.common.TaskInfo",
        "org.apache.flink.api.common.TaskInfoImpl",
        "org.apache.flink.api.common.accumulators.Accumulator",
        "org.apache.flink.api.common.accumulators.AccumulatorHelper",
        "org.apache.flink.api.common.aggregators.Aggregator",
        "org.apache.flink.api.common.aggregators.AggregatorRegistry",
        "org.apache.flink.api.common.aggregators.AggregatorWithName",
        "org.apache.flink.api.common.aggregators.ConvergenceCriterion",
        "org.apache.flink.api.common.cache.DistributedCache",
        "org.apache.flink.api.common.cache.DistributedCache.DistributedCacheEntry",
        "org.apache.flink.api.common.functions.DefaultOpenContext",
        "org.apache.flink.api.common.functions.FlatJoinFunction",
        "org.apache.flink.api.common.functions.Function",
        "org.apache.flink.api.common.functions.OpenContext",
        "org.apache.flink.api.common.functions.RichFunction",
        "org.apache.flink.api.common.functions.RuntimeContext",
        "org.apache.flink.api.common.functions.util.CopyingListCollector",
        "org.apache.flink.api.common.functions.util.FunctionUtils",
        "org.apache.flink.api.common.functions.util.RuntimeUDFContext",
        "org.apache.flink.api.common.operators.CollectionExecutor.CompletedFuture",
        "org.apache.flink.api.common.operators.CollectionExecutor.DynamicPathCollector",
        "org.apache.flink.api.common.operators.base.BulkIterationBase",
        "org.apache.flink.api.common.operators.base.DeltaIterationBase",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.MatchStatus",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinType",
        "org.apache.flink.api.common.operators.util.ListKeyGroupedIterator",
        "org.apache.flink.api.common.operators.util.ListKeyGroupedIterator.ValuesIterator",
        "org.apache.flink.api.common.operators.util.TypeComparable",
        "org.apache.flink.api.common.operators.util.UserCodeWrapper",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.GenericPairComparator",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.fs.FileSystem",
        "org.apache.flink.core.fs.Path",
        "org.apache.flink.core.fs.local.LocalFileSystem",
        "org.apache.flink.metrics.groups.OperatorMetricGroup",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.Collector",
        "org.apache.flink.util.OptionalFailure",
        "org.apache.flink.util.Visitable",
        "org.apache.flink.util.Visitor"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators;\n\npublic class CollectionExecutor_OuterJoinListIteratornextGroups_ResettableIteratorresetFikaTest {\n\n    @Test\n    public void testExecute() {\n    }\n}",
      "conditionCount": 28,
      "callCount": 2,
      "covered": false
    },
    {
      "entryPoint": "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
      "thirdPartyMethod": "org.apache.commons.collections.ResettableIterator.next()",
      "directCaller": "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.nextGroups(org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinType)",
      "path": [
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.Plan)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.execute(org.apache.flink.api.common.operators.Operator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.CollectionExecutor.executeBinaryOperator(org.apache.flink.api.common.operators.DualInputOperator, int, org.apache.flink.api.common.JobInfo)",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.executeOnCollections(java.util.List, java.util.List, org.apache.flink.api.common.functions.RuntimeContext, org.apache.flink.api.common.ExecutionConfig)",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.next()",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.nextGroups(org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinType)",
        "org.apache.commons.collections.ResettableIterator.next()"
      ],
      "methodSources": [
        "// --------------------------------------------------------------------------------------------\n// General execution methods\n// --------------------------------------------------------------------------------------------\npublic JobExecutionResult execute(Plan program) throws Exception {\n    long startTime = System.currentTimeMillis();\n    JobID jobID = (program.getJobId() == null) ? new JobID() : program.getJobId();\n    JobInfo jobInfo = new JobInfoImpl(jobID, program.getJobName());\n    initCache(program.getCachedFiles());\n    Collection<? extends GenericDataSinkBase<?>> sinks = program.getDataSinks();\n    for (Operator<?> sink : sinks) // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    {\n        execute(sink, jobInfo);\n    }\n    long endTime = System.currentTimeMillis();\n    Map<String, OptionalFailure<Object>> accumulatorResults = AccumulatorHelper.toResultMap(accumulators);\n    return new JobExecutionResult(null, endTime - startTime, accumulatorResults);\n}",
        "private List<?> execute(Operator<?> operator, JobInfo jobInfo) throws Exception {\n    // PATH: Test should invoke the next CollectionExecutor.execute(...) [step in execution path]\n    return execute(operator, 0, jobInfo);\n}",
        "private List<?> execute(Operator<?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    List<?> result = this.intermediateResults.get(operator);\n    // if it has already been computed, use the cached variant\n    if (result != null) {\n        return result;\n    }\n    if (operator instanceof BulkIterationBase) {\n        result = executeBulkIteration(((BulkIterationBase<?>) (operator)), jobInfo);\n    } else if (operator instanceof DeltaIterationBase) {\n        result = executeDeltaIteration(((DeltaIterationBase<?, ?>) (operator)), jobInfo);\n    } else if (operator instanceof SingleInputOperator) {\n        result = executeUnaryOperator(((SingleInputOperator<?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof DualInputOperator) {\n        // PATH: Test should invoke the next CollectionExecutor.executeBinaryOperator(...) [step in execution path]\n        result = executeBinaryOperator(((DualInputOperator<?, ?, ?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSourceBase) {\n        result = executeDataSource(((GenericDataSourceBase<?, ?>) (operator)), superStep, jobInfo);\n    } else if (operator instanceof GenericDataSinkBase) {\n        executeDataSink(((GenericDataSinkBase<?>) (operator)), superStep, jobInfo);\n        result = Collections.emptyList();\n    } else {\n        throw new RuntimeException(\"Cannot execute operator \" + operator.getClass().getName());\n    }\n    this.intermediateResults.put(operator, result);\n    return result;\n}",
        "private <IN1, IN2, OUT> List<OUT> executeBinaryOperator(DualInputOperator<?, ?, ?, ?> operator, int superStep, JobInfo jobInfo) throws Exception {\n    Operator<?> inputOp1 = operator.getFirstInput();\n    Operator<?> inputOp2 = operator.getSecondInput();\n    if (inputOp1 == null) {\n        throw new InvalidProgramException((\"The binary operation \" + operator.getName()) + \" has no first input.\");\n    }\n    if (inputOp2 == null) {\n        throw new InvalidProgramException((\"The binary operation \" + operator.getName()) + \" has no second input.\");\n    }\n    // compute inputs\n    @SuppressWarnings(\"unchecked\")\n    List<IN1> inputData1 = ((List<IN1>) (execute(inputOp1, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    List<IN2> inputData2 = ((List<IN2>) (execute(inputOp2, superStep, jobInfo)));\n    @SuppressWarnings(\"unchecked\")\n    DualInputOperator<IN1, IN2, OUT, ?> typedOp = ((DualInputOperator<IN1, IN2, OUT, ?>) (operator));\n    // build the runtime context and compute broadcast variables, if necessary\n    TaskInfo taskInfo = new TaskInfoImpl(typedOp.getName(), 1, 0, 1, 0);\n    RuntimeUDFContext ctx;\n    if (RichFunction.class.isAssignableFrom(typedOp.getUserCodeWrapper().getUserCodeClass())) {\n        ctx = createContext(superStep, taskInfo, jobInfo);\n        for (Map.Entry<String, Operator<?>> bcInputs : operator.getBroadcastInputs().entrySet()) {\n            List<?> bcData = execute(bcInputs.getValue(), jobInfo);\n            ctx.setBroadcastVariable(bcInputs.getKey(), bcData);\n        }\n    } else {\n        ctx = null;\n    }\n    // PATH: Test should invoke the next OuterJoinOperatorBase.executeOnCollections(...) [step in execution path]\n    return typedOp.executeOnCollections(inputData1, inputData2, ctx, executionConfig);\n}",
        "@Override\nprotected List<OUT> executeOnCollections(List<IN1> leftInput, List<IN2> rightInput, RuntimeContext runtimeContext, ExecutionConfig executionConfig) throws Exception {\n    TypeInformation<IN1> leftInformation = getOperatorInfo().getFirstInputType();\n    TypeInformation<IN2> rightInformation = getOperatorInfo().getSecondInputType();\n    TypeInformation<OUT> outInformation = getOperatorInfo().getOutputType();\n    TypeComparator<IN1> leftComparator = buildComparatorFor(0, executionConfig, leftInformation);\n    TypeComparator<IN2> rightComparator = buildComparatorFor(1, executionConfig, rightInformation);\n    TypeSerializer<IN1> leftSerializer = leftInformation.createSerializer(executionConfig.getSerializerConfig());\n    TypeSerializer<IN2> rightSerializer = rightInformation.createSerializer(executionConfig.getSerializerConfig());\n    OuterJoinListIterator<IN1, IN2> outerJoinIterator = new OuterJoinListIterator<>(leftInput, leftSerializer, leftComparator, rightInput, rightSerializer, rightComparator, outerJoinType);\n    // --------------------------------------------------------------------\n    // Run UDF\n    // --------------------------------------------------------------------\n    FlatJoinFunction<IN1, IN2, OUT> function = userFunction.getUserCodeObject();\n    FunctionUtils.setFunctionRuntimeContext(function, runtimeContext);\n    FunctionUtils.openFunction(function, DefaultOpenContext.INSTANCE);\n    List<OUT> result = new ArrayList<>();\n    Collector<OUT> collector = new CopyingListCollector<>(result, outInformation.createSerializer(executionConfig.getSerializerConfig()));\n    // PATH: Test should invoke the next OuterJoinOperatorBase$OuterJoinListIterator.next(...) [step in execution path]\n    while (outerJoinIterator.next()) {\n        IN1 left = outerJoinIterator.getLeft();\n        IN2 right = outerJoinIterator.getRight();\n        function.join(left == null ? null : leftSerializer.copy(left), right == null ? null : rightSerializer.copy(right), collector);\n    } \n    FunctionUtils.closeFunction(function);\n    return result;\n}",
        "@SuppressWarnings(\"unchecked\")\nprivate boolean next() throws IOException {\n    boolean hasMoreElements;\n    if (((currLeftIterator == null) || (!currLeftIterator.hasNext())) && ((currRightIterator == null) || (!currRightIterator.hasNext()))) {\n        // PATH: Test should invoke the next OuterJoinOperatorBase$OuterJoinListIterator.nextGroups(...) [step in execution path]\n        hasMoreElements = nextGroups(outerJoinType);\n        if (hasMoreElements) {\n            if (outerJoinType != OuterJoinType.LEFT) {\n                currLeftIterator = new ListIteratorWrapper(currLeftSubset.iterator());\n            }\n            leftReturn = ((IN1) (currLeftIterator.next()));\n            if (outerJoinType != OuterJoinType.RIGHT) {\n                currRightIterator = new ListIteratorWrapper(currRightSubset.iterator());\n            }\n            rightReturn = ((IN2) (currRightIterator.next()));\n            return true;\n        } else {\n            // no more elements\n            return false;\n        }\n    } else if (currLeftIterator.hasNext() && (!currRightIterator.hasNext())) {\n        leftReturn = ((IN1) (currLeftIterator.next()));\n        currRightIterator.reset();\n        rightReturn = ((IN2) (currRightIterator.next()));\n        return true;\n    } else {\n        rightReturn = ((IN2) (currRightIterator.next()));\n        return true;\n    }\n}",
        "private boolean nextGroups(OuterJoinType outerJoinType) throws IOException {\n    if (outerJoinType == OuterJoinType.FULL) {\n        return nextGroups();\n    } else if (outerJoinType == OuterJoinType.LEFT) {\n        boolean leftContainsElements = false;\n        while ((!leftContainsElements) && nextGroups()) {\n            currLeftIterator = new ListIteratorWrapper(currLeftSubset.iterator());\n            if (currLeftIterator.next() != null) {\n                leftContainsElements = true;\n            }\n            currLeftIterator.reset();\n        } \n        return leftContainsElements;\n    } else if (outerJoinType == OuterJoinType.RIGHT) {\n        boolean rightContainsElements = false;\n        while ((!rightContainsElements) && nextGroups()) {\n            currRightIterator = new ListIteratorWrapper(currRightSubset.iterator());\n            if (currRightIterator.next() != null) {\n                rightContainsElements = true;\n            }\n            currRightIterator.reset();\n        } \n        return rightContainsElements;\n    } else {\n        throw new IllegalArgumentException((\"Outer join of type '\" + outerJoinType) + \"' not supported.\");\n    }\n}"
      ],
      "constructors": [
        "// --------------------------------------------------------------------------------------------\npublic CollectionExecutor(ExecutionConfig executionConfig) {\n    this.executionConfig = executionConfig;\n    this.intermediateResults = new HashMap<Operator<?>, List<?>>();\n    this.accumulators = new HashMap<String, Accumulator<?, ?>>();\n    this.previousAggregates = new HashMap<String, Value>();\n    this.aggregators = new HashMap<String, Aggregator<?>>();\n    this.cachedFiles = new HashMap<String, Future<Path>>();\n    this.userCodeClassLoader = Thread.currentThread().getContextClassLoader();\n}",
        "public DynamicPathCollector(Set<Operator<?>> dynamicPathOperations) {\n    this.dynamicPathOperations = dynamicPathOperations;\n}",
        "public IterationRuntimeUDFContext(JobInfo jobInfo, TaskInfo taskInfo, ClassLoader classloader, ExecutionConfig executionConfig, Map<String, Future<Path>> cpTasks, Map<String, Accumulator<?, ?>> accumulators, OperatorMetricGroup metrics) {\n    super(jobInfo, taskInfo, classloader, executionConfig, cpTasks, accumulators, metrics);\n}",
        "public CompletedFuture(Path entry) {\n    try {\n        LocalFileSystem fs = ((LocalFileSystem) (FileSystem.getUnguardedFileSystem(entry.toUri())));\n        result = (entry.isAbsolute()) ? new Path(entry.toUri().getPath()) : new Path(fs.getWorkingDirectory(), entry);\n    } catch (Exception e) {\n        throw new RuntimeException(\"DistributedCache supports only local files for Collection Environments\");\n    }\n}"
      ],
      "fieldDeclarations": [
        "private final Map<Operator<?>, List<?>> intermediateResults;",
        "private final Map<String, Accumulator<?, ?>> accumulators;",
        "private final Map<String, Future<Path>> cachedFiles;",
        "private final Map<String, Value> previousAggregates;",
        "private final Map<String, Aggregator<?>> aggregators;",
        "private final ClassLoader userCodeClassLoader;",
        "private final ExecutionConfig executionConfig;",
        "private int iterationSuperstep;"
      ],
      "setters": [
        "private void initCache(Set<Map.Entry<String, DistributedCache.DistributedCacheEntry>> files) {\n    for (Map.Entry<String, DistributedCache.DistributedCacheEntry> file : files) {\n        Future<Path> doNothing = new CompletedFuture(new Path(file.getValue().filePath));\n        cachedFiles.put(file.getKey(), doNothing);\n    }\n}"
      ],
      "imports": [
        "org.apache.commons.collections.ResettableIterator",
        "org.apache.commons.collections.iterators.ListIteratorWrapper",
        "org.apache.flink.api.common.ExecutionConfig",
        "org.apache.flink.api.common.InvalidProgramException",
        "org.apache.flink.api.common.JobExecutionResult",
        "org.apache.flink.api.common.JobID",
        "org.apache.flink.api.common.JobInfo",
        "org.apache.flink.api.common.JobInfoImpl",
        "org.apache.flink.api.common.Plan",
        "org.apache.flink.api.common.TaskInfo",
        "org.apache.flink.api.common.TaskInfoImpl",
        "org.apache.flink.api.common.accumulators.Accumulator",
        "org.apache.flink.api.common.accumulators.AccumulatorHelper",
        "org.apache.flink.api.common.aggregators.Aggregator",
        "org.apache.flink.api.common.aggregators.AggregatorRegistry",
        "org.apache.flink.api.common.aggregators.AggregatorWithName",
        "org.apache.flink.api.common.aggregators.ConvergenceCriterion",
        "org.apache.flink.api.common.cache.DistributedCache",
        "org.apache.flink.api.common.cache.DistributedCache.DistributedCacheEntry",
        "org.apache.flink.api.common.functions.DefaultOpenContext",
        "org.apache.flink.api.common.functions.FlatJoinFunction",
        "org.apache.flink.api.common.functions.Function",
        "org.apache.flink.api.common.functions.OpenContext",
        "org.apache.flink.api.common.functions.RichFunction",
        "org.apache.flink.api.common.functions.RuntimeContext",
        "org.apache.flink.api.common.functions.util.CopyingListCollector",
        "org.apache.flink.api.common.functions.util.FunctionUtils",
        "org.apache.flink.api.common.functions.util.RuntimeUDFContext",
        "org.apache.flink.api.common.operators.CollectionExecutor.CompletedFuture",
        "org.apache.flink.api.common.operators.CollectionExecutor.DynamicPathCollector",
        "org.apache.flink.api.common.operators.base.BulkIterationBase",
        "org.apache.flink.api.common.operators.base.DeltaIterationBase",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinListIterator.MatchStatus",
        "org.apache.flink.api.common.operators.base.OuterJoinOperatorBase.OuterJoinType",
        "org.apache.flink.api.common.operators.util.ListKeyGroupedIterator",
        "org.apache.flink.api.common.operators.util.ListKeyGroupedIterator.ValuesIterator",
        "org.apache.flink.api.common.operators.util.TypeComparable",
        "org.apache.flink.api.common.operators.util.UserCodeWrapper",
        "org.apache.flink.api.common.serialization.SerializerConfig",
        "org.apache.flink.api.common.typeinfo.TypeInformation",
        "org.apache.flink.api.common.typeutils.CompositeType",
        "org.apache.flink.api.common.typeutils.GenericPairComparator",
        "org.apache.flink.api.common.typeutils.TypeComparator",
        "org.apache.flink.api.common.typeutils.TypeSerializer",
        "org.apache.flink.core.fs.FileSystem",
        "org.apache.flink.core.fs.Path",
        "org.apache.flink.core.fs.local.LocalFileSystem",
        "org.apache.flink.metrics.groups.OperatorMetricGroup",
        "org.apache.flink.types.Value",
        "org.apache.flink.util.Collector",
        "org.apache.flink.util.OptionalFailure",
        "org.apache.flink.util.Visitable",
        "org.apache.flink.util.Visitor"
      ],
      "testTemplate": "package org.apache.flink.api.common.operators;\n\npublic class CollectionExecutor_OuterJoinListIteratornextGroups_ResettableIteratornextFikaTest {\n\n    @Test\n    public void testExecute() {\n    }\n}",
      "conditionCount": 28,
      "callCount": 1,
      "covered": false
    }
  ]
}